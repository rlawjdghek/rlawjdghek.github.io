---
title:  "wandb"
excerpt: "wandb"
categories:
  - Pytorch & Tensorflow & Coding
  
tags:
  - Pytorch & Tensorflow & Coding
  
toc: true
toc_sticky: true
toc_label: "On this page"
    
last_modified_at: 2021-10-25T12:48:00-05:00
---
무조건 사용해야 하는 wandb. 사용법은 매우 간단해서 아래 CIFAR100 코드 참고해서 사용하자.

### 사용법 요약
1. wandb.init(project, name, notes)
2. wandb.config.update(args)
3. wandb.log

### 간단 설명
1. wandb.init에서 project가 같으면 이 코드가 wandb 웹페이지에 들어가면 하나의 탭에 들어가도록 한다.
2. wandb.init에서 name이 같으면 project안에서 그래프 등이 나뉜다. 하나의 project에서 여러 name에 대한 그래프를 한방에 볼 수 있다. 
3. wandb.init에서 notes는 하나의 name에 대한 추가 설명
4. wandb.config.update는 항상 argparse가 들어가야 한다. class는 들어갈 수 없다. json기반이므로 dictiondary가 들어가면 될듯하다.
5. wandb.log는 tensorboard와 같이 dictionary가 들어가면 된다. 

```python
import argparse
import os
from tqdm import tqdm

import torch
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import GradScaler, autocast
import torchvision
import torchvision.transforms as T

import timm
import wandb

class AverageMeter (object):
    def __init__(self):
        self.reset ()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def Accuracy(output, target, topk=(1,)):
    with torch.no_grad():
        maxk = max(topk)
        batch_size = target.size(0)

        _, pred = output.topk(maxk, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))

        res = []
        for k in topk:
            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)
            res.append(correct_k.mul_(100.0 / batch_size))
        return res

class args:
    #### dataset ####
    n_workers = 8

    #### training ####
    batch_size = 512
    n_epochs = 100
    lr = 1e-3

    #### config ####
    CUDA_VISIBLE_DEVICES = "1"
    is_wandb = True

parser = argparse.ArgumentParser()
#### dataset ####
parser.add_argument("--n_workers", type=int, default=8)
parser.add_argument("--batch_size", type=int, default=128)
parser.add_argument("--n_epochs", type=int, default=100)
parser.add_argument("--lr", type=float, default=1e-3)
parser.add_argument("--CUDA_VISIBLE_DEVICES", type=str, default="1")
parser.add_argument("--is_wandb", type=bool, default=True)
args = parser.parse_args()

os.environ["CUDA_VISIBLE_DEVICES"] = args.CUDA_VISIBLE_DEVICES
if args.is_wandb:
    wandb.init(project="project MNIST test", name="name MNIST test", notes="notes MNIST test")
    wandb.config.update(args)

transform = T.Compose([
    T.RandomHorizontalFlip(),
    T.ToTensor(),
    T.Normalize(0.5, 0.5)
])
train_ds = torchvision.datasets.CIFAR10(root="/home/jeonghokim/deprecated", transform=transform, download=True, train=True)
test_ds = torchvision.datasets.CIFAR10(root="/home/jeonghokim/deprecated", transform=transform, download=True, train=False)
train_dl = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers = args.n_workers,
                      pin_memory=True)
test_dl = DataLoader(train_ds, batch_size=args.batch_size, shuffle=False, num_workers=args.n_workers,
                     pin_memory=True)

model = timm.create_model("resnet50").cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
criterion = torch.nn.CrossEntropyLoss()

scaler = GradScaler()
for epoch in range(args.n_epochs):
    train_loss = AverageMeter()
    train_acc = AverageMeter()
    test_loss = AverageMeter()
    test_acc = AverageMeter()

    model.train()
    with tqdm(train_dl, total=len(train_dl), leave=False) as train_loop:
        for img, label in train_loop:
            img = img.cuda()
            label = label.cuda()

            optimizer.zero_grad()
            with autocast():
                output = model(img)
                loss_ = criterion(output, label)

            scaler.scale(loss_).backward()
            scaler.step(optimizer)
            scaler.update()

            acc1 = Accuracy(output, label)[0]
            train_loss.update(loss_, args.batch_size)
            train_acc.update(acc1, args.batch_size)

    model.eval()
    with torch.no_grad():
        with tqdm(test_dl, total=len(test_dl), leave=False) as test_loop:
            for img, label in test_loop:
                img = img.cuda()
                label = label.cuda()

            with autocast():
                output = model(img)
                loss_ = criterion(output, label)

            acc1 = Accuracy(output, label)[0]
            test_loss.update(loss_, args.batch_size)
            test_acc.update(acc1, args.batch_size)

    wandb.log({"train loss": train_loss.avg, "train acc": train_acc.avg, "test loss": test_loss.avg,
               "test acc": test_acc.avg})
```









