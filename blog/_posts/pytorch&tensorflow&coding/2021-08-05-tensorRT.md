---
title:  "TensorRT로 추론 속도를 단축하자."
excerpt: "도커를 활용하여 구현해보자"
categories:
  - Pytorch & Tensorflow & Coding
  
tags:
  - pytorch
  
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2021-08-05T23:19:00-05:00
---

2021년 7월에 열린 LG competition에서 private score2등을 하여 2차 평가를 받게 되었다. 그 과정에서 추론 속도가 평가 항목에 있어 tensorRT로 
파이토치 모델의 추론 속도를 단축하는 것을 맡았다. 

Task: 도커로 TensorRT 환경을 구축하고 모델변환까지 해보자.

### 1. TensorRT 환경 구축
도커를 활용하기 때문에 먼저 도커에 대한 선행지식이 필요하다. 도커 개념은 [거모장-docker](https://rlawjdghek.github.io/docker/docker/)를 참고하자.
#### 1-1. 현재 서버의 우분투, cuda 환경 고려
```bash
cat /etc/issue
```
로 우분투 버전확인, nvidia-smi로 nvidia-driver 버전을 확인하고, nvcc -V로 cuda환경을 확인한다. 나는 dash6을 기준으로 하기 때문에 

nvidia-driver는 460.39, cuda버전은 11.2이다. (파이토치는 cudatoolkit 11.1).

#### 1-2. tensorRT 버전 확인
다음 [링크](https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/running.html)에 들어가서 자신과 맡는 버전을 확인한다.
내 서버는 1-1과 같으므로 21.02를 설치하였다. 도커 설치 명령어는 아래와 같다.
```bash
docker pull nvcr.io/nvidia/tensorrt:21.02-py3
```
버전만 바꾸어주면 다른 버전도 가능하다.

이제 도커를 실행해 보자. 주의할 점은 텐서 rt정도 사용하는 코드는 상당한 자원을 잡아먹는데 이를 방지하기 위해 --ipc=host라는 명령어를 사용하여 이미지를 실행한다.
```bash
docker run -it --gpus all --ipc=host --name tensorrt_test nvcr.io/nvidia/tensorrt:21.02-py3 
```

#### 1-3. pytorch, opencv 등 필요한 라이브러리 설치
pytorch는 늘 설치하듯 파이토치 홈페이지가서 **pip로 설치하면 된다.** 

opencv아래와 같다. 먼저 sudo가 없으니 sudo부터 깔아준다.
```bash
apt-get update
```
로 업데이트 먼저 해줘야 한다. 안하면 sudo를 깔 수가 없음.

```bash
apt-get install -y sudo
```

```bash
wget https://gist.githubusercontent.com/eungbean/0880de7604472219c7e3f6ddb7cebde5/raw/443d629bf83b65bb59e34564626f872ab1124b3f/opencv-3.4.0-install-script.sh
```
로 쉘스크립트를 다운받은 후

```bash
bash opencv-install.sh
```
로 설치한다. 중간에 많이 Y를 눌러줘야 한다.


(**추천**)아니면 headless 버전으로 깔면 쉽게 해결된다. 
```bash
pip install opencv-python-headless
```


### 2. 코드 작성
기본 코드가 파이토치이기 때문에 torch2trt를 사용한다. 먼저 예를 들어 아래와 같은 코드가 있다고 하자.

```python
import torch
import torch.nn as nn

x = torch.ones((1,3,224,224)).cuda()
model = nn.Sequential(
    nn.Conv2d(3, 16, 3, 1, 1),
    nn.Conv2d(16,32, 3, 1, 1)
).eval().cuda()


with torch.no_grad():
    for i in range(100):
        output = model(x)
```

그냥 의미없는 추론코드 이다. 이제 이 코드를 torch2trt로 바꾸어보자. 


```python
import torch
import torch.nn as nn
from torch2trt import torch2trt  # 모듈 임포트

x = torch.ones((1,3,224,224)).cuda()
model = nn.Sequential(
    nn.Conv2d(3,16,3,1,1),
    nn.Conv2d(16,32,3,1,1)
).eval().cuda()  # 이부분을 주목해야 한다. 모델은 반드시 eval모드여야한다.

rt_model = torch2trt(model, [x], use_onnx=True)  # use_onnx는 쓰자. efficientnet-b3은 안돌아감.

with torch.no_grad():
    for i in range(100):
        output = rt_model(x)

```

위의 두 코드는 tensorrt가  추론속도만 따지면 10/13정도 차이난다. 저장은 그냥 파이토치와 같다.

```python
import torch
import torch.nn as nn
from torch2trt import torch2trt  # 모듈 임포트

x = torch.ones((1,3,224,224)).cuda()
model = nn.Sequential(
    nn.Conv2d(3,16,3,1,1),
    nn.Conv2d(16,32,3,1,1)
).eval().cuda() 

rt_model = torch2trt(model, [x], use_onnx=True)

with torch.no_grad():
    for i in range(100):
        output = rt_model(x)

torch.save(model.state_dict(), "./model.pth")
```

불러오는 건 아래와 같다. TRT모듈을 만들고 load
```python
import torch
import torch.nn as nn
from torch2trt import torch2trt, TRTModule  # 모듈 임포트

x = torch.ones((1,3,224,224)).cuda()
rt_model = TRTModule()

rt_model.load_state_dict(torch.load("./model.pth"))

with torch.no_grad():
    for i in range(100):
        output = rt_model(x)


```


결론
1. batch는 1밖에안된다.
2. use_onnx=True 쓰자. efficient-b3등에서는 in_place연산에서등 버그가 생길수 있다.
3. LG 추론에는 사용 못했다. 애초에 추론시간이 3초밖에안걸려서 모델을 torch->onnx->tensorRT로 가는 시간이 더걸림. 
