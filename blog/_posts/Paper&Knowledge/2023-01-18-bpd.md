---
title: "Bits per dim 정리"
excerpt: "Bits per dim 정리"
categories:
  - Paper & Knowledge
  
tags:
  - Generative models
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2023-01-18T15:04:00-05:00
---



![](/assets/images/2023-01-18-bpd/1.PNG)
![](/assets/images/2023-01-18-bpd/2.PNG)

생성된 이미지의 픽셀을 생각해 볼 때, 하나의 픽셀이 나올 확률의 관점으로 볼 수 있다. bpd를 구글링 하였을 때 나오는 $p(y \| x)$는 logit x가 들어왔을 때, RGB값 $y\in [0, 255]$가 될 확률을 나타낸다. diffusion 논문에서 보이는 bpd 값은 모든 픽셀의 nll 평균을 밑이 2인 로그로 변환한 것이다. 위의 이미지 27번식이 정확한 bpd이지만, 뒤의 항을 생략하여 가장 첫번째 항과 8만 주목하면 된다. 람다는 0이라 생각.

![](/assets/images/2023-01-18-bpd/4.jpg)
이제 예시로 diffusion에서 bpd를 구해보자. **즉, nll을 구하고, 스케일링만 해주면 된다.**
위의 필기가 코드의 전체적인 흐름을 나타낸다. diffusion에서는 가장 초기의 reverse process의 nll인 prior bpd와 나머지 process의 bpd의 총합을 더하여 최종 bpd를 구한다. bpd와 nll은 단순 스케일링 차이이므로, 코드는 bpd 스케일에서 진행됨을 유의하자. 

먼저 가장 처음의 calc_bpd_loop이다. 이 함수에서는 1000번의 reverse process에서 나오는 bpd의 합을 구한다. diffusion은 추론이 많은만큼 각 스텝의 bpd는 작다.
```python
def calc_bpd_loop(self, model, x_start, clip_denoised=True, model_kwargs=None):
    """
    Compute the entire variational lower-bound, measured in bits-per-dim,
    as well as other related quantities.
    :param model: the model to evaluate loss on.
    :param x_start: the [N x C x ...] tensor of inputs.
    :param clip_denoised: if True, clip denoised samples.
    :param model_kwargs: if not None, a dict of extra keyword arguments to
        pass to the model. This can be used for conditioning.
    :return: a dict containing the following keys:
                - total_bpd: the total variational lower-bound, per batch element.
                - prior_bpd: the prior term in the lower-bound.
                - vb: an [N x T] tensor of terms in the lower-bound.
                - xstart_mse: an [N x T] tensor of x_0 MSEs for each timestep.
                - mse: an [N x T] tensor of epsilon MSEs for each timestep.
    """
    device = x_start.device
    batch_size = x_start.shape[0]

    vb = []
    xstart_mse = []
    mse = []
    for t in list(range(self.num_timesteps))[::-1]:
        t_batch = th.tensor([t] * batch_size, device=device)
        noise = th.randn_like(x_start)
        x_t = self.q_sample(x_start=x_start, t=t_batch, noise=noise)
        # Calculate VLB term at the current timestep
        with th.no_grad():
            out = self._vb_terms_bpd(
                model,
                x_start=x_start,
                x_t=x_t,
                t=t_batch,
                clip_denoised=clip_denoised,
                model_kwargs=model_kwargs,
            )
        vb.append(out["output"])
        xstart_mse.append(mean_flat((out["pred_xstart"] - x_start) ** 2))
        eps = self._predict_eps_from_xstart(x_t, t_batch, out["pred_xstart"])
        mse.append(mean_flat((eps - noise) ** 2))

    vb = th.stack(vb, dim=1)
    xstart_mse = th.stack(xstart_mse, dim=1)
    mse = th.stack(mse, dim=1)

    prior_bpd = self._prior_bpd(x_start)
    total_bpd = vb.sum(dim=1) + prior_bpd
    return {
        "total_bpd": total_bpd,
        "prior_bpd": prior_bpd,
        "vb": vb,
        "xstart_mse": xstart_mse,
        "mse": mse,
    }
```
이 함수 안의 vb_terms_bpd는 각 스텝마다의 bpd를 구한다. 


```python
def _vb_terms_bpd(
    self, model, x_start, x_t, t, clip_denoised=True, model_kwargs=None
):
    """
    Get a term for the variational lower-bound.
    The resulting units are bits (rather than nats, as one might expect).
    This allows for comparison to other papers.
    :return: a dict with the following keys:
                - 'output': a shape [N] tensor of NLLs or KLs.
                - 'pred_xstart': the x_0 predictions.
    """
    true_mean, _, true_log_variance_clipped = self.q_posterior_mean_variance(
        x_start=x_start, x_t=x_t, t=t
    )
    out = self.p_mean_variance(
        model, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs
    )
    kl = normal_kl(
        true_mean, true_log_variance_clipped, out["mean"], out["log_variance"]
    )
    kl = mean_flat(kl) / np.log(2.0)

    decoder_nll = -discretized_gaussian_log_likelihood(
        x_start, means=out["mean"], log_scales=0.5 * out["log_variance"]
    )
    assert decoder_nll.shape == x_start.shape
    decoder_nll = mean_flat(decoder_nll) / np.log(2.0)

    # At the first timestep return the decoder NLL,
    # otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))
    output = th.where((t == 0), decoder_nll, kl)
    return {"output": output, "pred_xstart": out["pred_xstart"]}
```
가장 아래 where을 보면 t != 0일때 kl로 구하는 것을 볼 수 있다. 

먼저 t=1~998일때부터 보자. 이 경우에는, 우리는 실제 목표 분포 q를 구할수 있으므로, 추론한 분포 p와 q의 KL을 통하여 nll을 계산한다. 두 가우시안의 kl은 VAE에서 봤듯이, 아래 함수로 간소화되어 계산할 수 있다.
![](/assets/images/2023-01-18-bpd/5.PNG)


코드는 아래와 같다.
```python
def normal_kl(mean1, logvar1, mean2, logvar2):
    """
    Compute the KL divergence between two gaussians.
    Shapes are automatically broadcasted, so batches can be compared to
    scalars, among other use cases.
    """
    tensor = None
    for obj in (mean1, logvar1, mean2, logvar2):
        if isinstance(obj, th.Tensor):
            tensor = obj
            break
    assert tensor is not None, "at least one argument must be a Tensor"

    # Force variances to be Tensors. Broadcasting helps convert scalars to
    # Tensors, but it does not work for th.exp().
    logvar1, logvar2 = [
        x if isinstance(x, th.Tensor) else th.tensor(x).to(tensor)
        for x in (logvar1, logvar2)
    ]

    return 0.5 * (
        -1.0
        + logvar2
        - logvar1
        + th.exp(logvar1 - logvar2)
        + ((mean1 - mean2) ** 2) * th.exp(-logvar2)
    )
```

다음으로 t=0를 보면, 실제 분포와의 거리를 구해야 하므로 p(x)를 구한다. 필기에서 볼 수 있듯이, vb_terms_bpd에서는 우리는 현재 이산분포에서 계산하기 때문에 p(x)를 구하기 위해서 gaussian의 cdf를 사용한다. 연산의 이점을 활용하기 위해서 gaussian의 cdf는 아래 식과같이 근사된다.
![](/assets/images/2023-01-18-bpd/3.PNG)
이를 코드로 나타내면 함수 discretized_gaussian_log_likelihood이다.
```python
def discretized_gaussian_log_likelihood(x, *, means, log_scales):
    """
    Compute the log-likelihood of a Gaussian distribution discretizing to a
    given image.
    :param x: the target images. It is assumed that this was uint8 values,
              rescaled to the range [-1, 1].
    :param means: the Gaussian mean Tensor.
    :param log_scales: the Gaussian log stddev Tensor.
    :return: a tensor like x of log probabilities (in nats).
    """
    assert x.shape == means.shape == log_scales.shape
    centered_x = x - means
    inv_stdv = th.exp(-log_scales)
    plus_in = inv_stdv * (centered_x + 1.0 / 255.0)
    cdf_plus = approx_standard_normal_cdf(plus_in)
    min_in = inv_stdv * (centered_x - 1.0 / 255.0)
    cdf_min = approx_standard_normal_cdf(min_in)
    log_cdf_plus = th.log(cdf_plus.clamp(min=1e-12))
    log_one_minus_cdf_min = th.log((1.0 - cdf_min).clamp(min=1e-12))
    cdf_delta = cdf_plus - cdf_min
    log_probs = th.where(
        x < -0.999,
        log_cdf_plus,
        th.where(x > 0.999, log_one_minus_cdf_min, th.log(cdf_delta.clamp(min=1e-12))),
    )
    assert log_probs.shape == x.shape
    return log_probs
```

cdf 근사 코드는 아래와 같다.
```python
def approx_standard_normal_cdf(x):
    """
    A fast approximation of the cumulative distribution function of the
    standard normal.
    """
    return 0.5 * (1.0 + th.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * th.pow(x, 3))))
```

마지막으로 prior_bpd를 구한다. t=999를 생각하면 된다. 필기를 참고하여 그 이전의 standard gaussian임을 주목하자.
```python
def _prior_bpd(self, x_start):
    """
    Get the prior KL term for the variational lower-bound, measured in
    bits-per-dim.
    This term can't be optimized, as it only depends on the encoder.
    :param x_start: the [N x C x ...] tensor of inputs.
    :return: a batch of [N] KL values (in bits), one per batch element.
    """
    batch_size = x_start.shape[0]
    t = th.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)
    qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)
    kl_prior = normal_kl(
        mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0
    )
    return mean_flat(kl_prior) / np.log(2.0)
```