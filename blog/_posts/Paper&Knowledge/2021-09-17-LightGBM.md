---
title:  "[NIPS2017]A Highly Efficient Gradient Boosting Decision Tree"
excerpt: "[NIPS2017]A Highly Efficient Gradient Boosting Decision Tree"
categories:
  - Paper & Knowledge
  
tags:
  - Paper & Knowledge
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2021-08-14T15:33:00-05:00
---

논문 스터디에서 발표용으로 논문을 고르던 중 ML논문을 한번도 안 읽어봐서 대표적으로 많이 사용하는 LightGBM을 골랐다.
슬라이드는 다음 [링크](https://docs.google.com/presentation/d/1jxBdTbfOvv6wQkYMNRjUXRTjkhmOIGw5/edit?usp=drive_web&ouid=115578388967484997653&rtpof=true) 참조

### Abstract & Introduction
Decision Tree는 머신러닝의 대표적인 알고리즘으로 좋은 성능을 보여준다. 하지만 데이터가 많아지는 만큼 연산량과 쓸모없는 데이터가 증가하면서 알고리즘이 개선되어야 한다. LightGBM은 
Data instance, Feature의 두가지 측면에서 효율적으로 연산량을 줄인다.

### Preliminaries
먼저 이 논문을 잘 이해하기 위해서는 Gradient와 정보량의 관계를 집고 넘어가는 것이 좋다. 다음 예제를 보면,

@@@@ 이진 분류 문제에서 어떤 모델이 A를 [1/2, 1/2] 로 예측하였고, B를 [1, 0]으로 예측 하였을 때, 어느 데이터에서 더 많은 정보를 얻을 수 있을까? @@@@

대충 생각해보면 B를 잘 예측 하였으므로 B라고 할 수 있겠다. 하지만 각각의 엔트로피를 계산해보자.
![](/assets/images/2021-09-17-LightGBM/1.PNG)
A의 엔트로피가 더 크다는 것을 알 수 있다. 즉, A의 정보량이 더 많다. 이는 모델이 A에서 더 많은 정보를 배울 수 있고, 모델 입장에서 A가 더 가치있다고 할 수 있다. 
정보를 많이 배울 수 있다는 말은, 업데이트가 더 많이 되고, **gradient가 더 크다는 말이다.** 

기존의 연구들은 데이터 갯수를 줄이기 위해 Pre-sorted Algorithm, Histogram-based Algorithm을 사용했다.
Pre-sorted Algorithm은 가장 기본적으로 threshold를 나누기 위해 우선 data별로 정렬한 뒤 적당한 값으로 기준 노드의 threshold로 설정한다. 실직적으로 줄어드는 데이터는 없다. 

Histogram-based Algorithm는 data의 갯수를 줄이기 위해서 어떤 한 feature에 대하여 data instance들이 갖는 값을 bin으로 나눈다. 예를 들어 0~10에는 무수히 많은 실수가 들어있는데 
이를 1, ..., 10으로 나눔으로써 데이터 갯수만큼의 threshold가 나오는 것을 bin의 갯수 (여기서는 10)로 잡을 수 있다.

### Methods
#### Gradient-based One-side Sampling (GOSS)
위에서 gradient를 미리 설명한 것은 GOSS알고리즘이 gradient로 중요하고 안중요한 데이터로 나누었기 때문이다. gradient가 크면 정보량이 많다, 즉 좋은 데이터라 할 수 있다는 사실을 이용한다. 
알고리즘은 아래 사진과 같다.
![](/assets/images/2021-09-17-LightGBM/2.PNG) 
그림에 적힌 숫자대로 설명을 하면 아래와 같다. 쉬움.

1. 초기 모델로 예측
2. 로스 계산, weight는 나중에 들어갈 중요도를 추가하는 계수 초기화.
3. G를 절댓값의 크기에 맞게 계산. 앞의 예제에서도 볼 수 있듯이 gradient가 큰 값일 수록 더 중요한 데이터임을 알 수 있음.
4. 중요한 데이터의 앞 topN개를 살림
5 G가 작은 순대로 topN개를 고르고, 그 중에서 randN개를 안중요한 데이터로 사용
6 .사용할 데이터는 topN + randN개
7 .안중요한 데이터에 가중치 부여
8. 새로운 모델을 이전 모델로부터 사용할 데이터만 가지고 업데이트한다. 
9. 모델에 만들어진 새로운 모델 append. 

중요한 것은 오른쪽 그림과 같이 업데이트에 사용하는 데이터는 topN + randN개 뿐이고, 논문에서 제시한 파라미터 (a, b)대로라면 전체 데이터의 30%를 사용한다.

직관적으로 시간적인 측면에서는 줄이는 것을 알 수 있지만, 이 방법이 과연 성능을 유지할까 하는 의문이 든다. 따라서 저자들은 GOSS알고리즘으로 생성된 Decision Tree가 일반적으로 사용하는
Decision Tree와 큰 차이가 없다는 것을 증명하였다.

먼저 기존의 Decision Tree가 가진 정보량을 보자.
![](/assets/images/2021-09-17-LightGBM/3.PNG)
첫번째 항은 threshold d에 대하여 왼쪽노드, 즉 d보다 작은 값을 가지는 데이터들의 gradient를 뜻하고, 두번째항은 d보다 큰값을 가지는 데이터들의 gradient이다. 따라서 V는 전체데이터의 정보량.

다음으로 GOSS로 만든 Decision Tree의 정보량을 보자.
![](/assets/images/2021-09-17-LightGBM/4.PNG) 
항이 4개로 증가한 것을 볼 수 있는데, 우리는 데이터를 topN, randN개로 나누었으므로 그 안에서 또 2개로 나누어 표기 할 수 있다. 식을 보면 집합 A(topN), B(randN)에 포함된 원소별로 
시그마가 4개인 것을 볼 수 있다. 따라서 V는 GOSS에서 사용하는 데이터들의 정보량.


![](/assets/images/2021-09-17-LightGBM/5.PNG)
세번째 수식은 GOSS에서 사용하는 데이터 정보량이 전체데이터를 사용했을 때와 얼마나 차이나는지 보여주는 수식이다. 결론부터 말하자면 차이가 없다. $O(\frac{1}{n})$만큼 차이가 나는데 애초에 우리가 데이터를 
줄이려는 이유는 데이터가 많기 때문이므로 n은 충분히 크다고 가정할 수 있다.

#### Exclusive Feature Bundling
GOSS로 data instance의 갯수를 줄였으므로 이제 feature를 줄여보자. 여기서는 conflict의 개념만 알고 넘어가면 될 것같다.
![](/assets/images/2021-09-17-LightGBM/6.PNG)
위의 알고리즘에서 conflict를 카운팅하는 것을 볼 수 있다. conflict가 일정 횟수 이상 일어나면 두 feature는 안 맞는 것으로 판단하여 각각 다른 bundle로 나뉜다.
논문에서 제시한 conflict의 예는 0값에 대해서 말한다. 오른쪽의 예제를 보면, 위에서는 각각 data instance가 0을 교차로 갖고 있어 총 6개의 feature에서 모두 conflict가 일어난다.

밑에서는 위의 기준대로라면 한 번도 일어나지 않지만, 저자들이 도입한 $\gamma$를 일정값 주게된다면 conflict가 일어나는 규제를 완화하여 0.2값에서 conflict를 일어나게 할 수 있다. 
이전에도 언급하였듯, conflict가 많으면 두 feature가 안 맞는 것이므로 $\gamma$가 클수록 더 많은 feature가 줄어든다는 것을 유추할 수 있다.

### Experiments
실험에 사용한 데이터는 총 5가지이다. 그림 참고.
![](/assets/images/2021-09-17-LightGBM/7.PNG)

베이스라인은 그림처럼 5가지 사용하였다. 이해하는데 큰 어려움이 없음.
![](/assets/images/2021-09-17-LightGBM/8.PNG)

결과는 LightGBM이 시간, 성능 측면에서 모두 가장 좋다. 한가지 주목할 점은, 시간적 측면에서는 data instance를 줄이는 것보다 feature의 갯수를 줄이는 것이 더 효과적이다.
성능 측면에서는, lgb_baseline 즉, GOSS알고리즘만 사용한 것과, GOSS, EFB 둘다 사용한 것의 성능이 거의 같은데, feature가 쓸모없는 것이 많고, EFB알고리즘이 이러한 feature들을 잘 잡아준다는 것을
실험적으로 증명한다고 할 수 있다. 
![](/assets/images/2021-09-17-LightGBM/9.PNG)



