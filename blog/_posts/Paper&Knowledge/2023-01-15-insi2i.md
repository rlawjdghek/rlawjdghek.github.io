---
title: "Instance aware image to image translation 정리"
excerpt: "Instance aware image to image translation 정리"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2023-01-13T15:04:00-05:00
---


INIT 데이터셋을 기반으로 한 instance aware i2i 논문들을 정리한다. 시간순으로 총 4개, 모두 CVPR논문이다. 
1. [CVPR2019]Towards_Instance-Level_Image-To-Image_Translation
2. [CVPR2020]DUNIT-Detection-Based_Unsupervised_Image-to-Image_Translation
3. [CVPR2021]MGUIT-Memory-Guided_Unsupervised_Image-to-Image_Translation
4. [CVPR2022]InstaFormer - Instance-Aware Image-to-Image Translation with Transformer


![](/assets/images/2023-01-15-insi2i/1.PNG)
# 1. [CVPR2019]Towards_Instance-Level_Image-To-Image_Translation
본 논문에서는 기존의 i2i (MUNIT, DRIT, CycleGAN)가 이미지 내에 있는 객체들을 잘 고려하지 못한다는 한계점을 지적한다. 따라서 본 논문에서는 INIT dataset을 처음 제시하고, 이미지 내에있는 객체들의 정보를 활용하여 더욱 현실적인 이미지를 생성하는 것을 목표로 하는 새로운 task를 제시하였다. 기존의 MUNIT과 DRIT의 프레임워크에 더하여 객체의 스타일과 content를 반영하였다.

위의 그림에서 볼수 있듯이, source를 sunny, target을 night이라고 할 때, object들과 global이미지의 스타일을 뽑고, 이를 각 translation에 adain으로 적용하여 변환한다. 이때의 intuition은 global한 스타일이 object에 영향을 미칠 수는 있으나, local의 스타일이 global에 영향을 미치기는 어려울 수 있다는 것이다. 이를 논문에서는 coarse-to-fine content-style pair association이라 표현한다. 따라서 
1. object의 스타일은 object를 reconstruction할때만 사용.
2. global의 스타일은 object를 변환할떄 사용, global을 reconstruction할 때 사용. 

하지만 추후 발전된 논문에서 주장하는 것은 본 논문에서는 object와 global이미지를 별개로 학습한다는 단점이 있다고 말한다. 즉, 위의 그림에서도 residual block이 따로 존재하는 것을 볼 수 있고, 추가적으로 테스트 떄에는 global의 정보만 사용할 수 있기 때문에, local의 정보를 전혀 사용하지 않는다는 단점이 있다. 따라서 DUNIT이라는 논문에서는 미리 학습된 pre-trained object detection모델을 사용하여 본 생성 문제의 성능을 향상시킨다.

![](/assets/images/2023-01-15-insi2i/2.PNG)
# 2. [CVPR2020]DUNIT-Detection-Based_Unsupervised_Image-to-Image_Translation
"Towards_Instance-Level_Image-To-Image_Translation" 논문에서 보이는 2가지의 주요 문제점은 아래와 같다.
1. global 이미지에서의 feature map에서 instance를 직접적으로 사용하지 않았다. 즉, global 이미지가 encoder를 거친 뒤 나온 feature map에서 roi를 뽑아 reconstruction을 하였다.
2. 테스트 때에는 gt bounding box가 없으므로 global 스타일 정보만을 사용해야 하는데, 여기서 instnace 정보를 사용할 방법이 있다. 
본 논문에서는 미리 학습된 object detection 모델을 사용하여 위와 같은 기존 논문의 2가지 한계점을 아래와 같이 해결하였다.
1. instance부분을 직접적으로 encoding하는 모듈로 encoding한 local feature map을 직접적으로 global 이미지의 feature map에 대입하였다.
2. pre-trained OD 모델로 teat 때에도 local instance의 정보를 사용하였다. 

본 논문에서는 DRIT을 기본 프레임워크로 사용하였고, DRIT내용은 생락한다. 훈련과정을 나열하여 정리하면 아래와 같다.
1. target domain에서는 instance정보를 사용하지 않는다. 즉, INIT과 같이 style정보는 global image에서만 뽑아서 사용한다. (단, INIT에서는 instance들을 reconstruction할 때 instance의 style정보를 사용하였다.)
2. source domain에서 뽑은 global feature map과 instance feature map에 target domain의 global style정보를 섞는다.
3. 그림 중간에 주황색 박스와 같이 global feature map에 instance feature map을 대입한다. merged feature map에서는 global 정보와 instance 정보가 각각 유지되고, 이들은 모두 target global style을 받았다.
4. merge된 feature map을 decoder에 통과하여 target domain의 이미지를 생성한다.
5. 본 논문에서의 가정 중 하나는, 변환 전 이미지와 변환 후 이미지에 속하는 object의 bounding box가 동일하다는 것이다. 따라서 변환 후 이미지에도 변환 전 이미지의 gt를 적용하여 OD모델이 추론한 bounding box를 regression하는 instnace consistency loss를 주가하였다. 이 로스는 변환 전후의 이미지에서 OD가 예측한 bounding box의 왼쪽 위(top left)와 오른쪽 아래(bottom right) 좌표를 L1 loss로 맞춰주었다. **개인적인 생각으로, 이 가정은 틀렸다고 생각한다. 왜냐하면 낮에서 밤으로 변환 되었을때, 차량이 보이지 않는 부분이 존재할 수 있다.**

![](/assets/images/2023-01-15-insi2i/3.PNG)
![](/assets/images/2023-01-15-insi2i/4.PNG)
# 3. [CVPR2021]MGUIT-Memory-Guided_Unsupervised_Image-to-Image_Translation
DUNIT에서의 문제점은 아래와 같이 2가지가 존재한다.
1. pretrained된 OD 모델을 사용해야 한다는 것이다.
2. DUNIT의 그림에서 볼 수 있듯이, DUNIT에서는 target domain의 스타일을 global stlye 정보만 사용하여 source domain의 global, instance feature map에 적용하였다. 
본 논문에서는 memory network를 통하여 위의 한계점을 아래와 같이 해결한다.
1. memory network는 OD network만큼 연산량이 무겁지 않다.
2. memory network에 source domain의 global, instance content와 가장 가까운 target domain의 global, instance style, content를 저장하는 memory network를 만들어 DUNIT에서 target domain의 global style만 사용하였던 한계점을 instance style까지 활용하게 되었다.

본 논문에서는 DRIT을 기본 프레임워크로 사용하였고, DRIT내용은 생략한다. 제시하는 memory network 훈련은 아래와 같다. 
1. 총 K개의 클래스와, N개의 memory item이 있다고 설정하자. 각 클래스마다 $N_k$개의 item이 있고, 모두 합치면 N이 된다.
2. 각 아이템은 매칭을 하기 위한 key, 매칭된 key에 대하여 각 도메인의 style vector인 $v^x$, $v^y$가 있다. shape은 모두 $[1 \times 1 \times c]$. 이는 K개의 클래스가 있을 때, 각각이 독립적으로 존재한다. 
3. 두번째 그림에서 볼 수 있듯이, source image feature map이 주어졌을 때, K개의 클래스 (INIT dataset은 배경, 간판, 차량, 사람으로 K=4)로 feature map을 나눈다. 
4. 각 클래스에 대하여 content 벡터를 활용하여 (각 클래스에 해당하는 픽셀 갯수는 P개 있다고 하자), memory network의 k클래스에 해당하는 item들의 key에 대하여 거리를 구한다. 거리는 cosine similarity를 사용. 
5. $N_k$개의 item의 key 대하여 모두 거리를 구한다음 소프트맥스로 확률값으로 바꾸어준다. 이 확률값들을 가중치로 사용할 것이다.
6. 각 value에 대하여 5에서 구한 가중치를 곱해주고 더한다. value가 각 도메인(X, Y)에 대해서 2개이므로 X,Y에 해당하는 2개의 스타일벡터를 얻을 수 있다. 이는 adain에서 사용할 스타일벡터이다.
7. X의 content, Y의 style을 decoder에 넣어 변환된 이미지를 생성한다. 
헷갈리지 말아야 할 점은, p의 개념인데, 논문에서도 notation에 p가 아래첨자로 있다. p는 패치의 단위로, feature map에서 instance에 해당하는 부분의 패치갯수가 $2 \times 4$ 라면 p는 1~8에 해당한다. 즉, k 클래스에 해당하는 item과 k클래스에 해당하는 패치끼리를 모두 계산하여 P개에 해당하는 style vector가 나온다. 따라서 style feature map이 등장한다. 

memory network의 N개의 item들은 업데이트도 된다. 과정은 아래와 같다.
1. 클래스 K개의 각 content vector와 N개의 item들의 distance를 구한다. 이때에도 cosine similarity로 계산. 그러면 [P x N]의 행렬이 된다. 여기에 P를 기준으로 softmax를 한다. 왜냐하면 우리느 지금 item들을 업데이트해야 하기 때문에 item(N)을 기준으로 확률분포를 구해야한다. 
2. k, $v^x$, $v^y$를 업데이트한다. 자세한 사항은 논문의 식 6을 참조하고, 알아둬야 할 것은 content를 비교할때 사용한 key는 도메인에 상관없이 content는 공유된다는 사실 떄문에 X,Y도메인에 대하여 모두 content 벡터를 사용하여 업데이트되고, style에 해당하는 v들은 각 도메인에 해당하는 style vector에 의해 업데이트 된다.

마지막으로 item들끼지의 밀집되는 현상(representative and discriminative)을 없애기 위하여 feature contrastive loss를 추가하였다. 간단히 말하자면, 각 key와 value에 대하여 자신 빼고는 모두 negative로 설정하였다. feature separateness loss라는 기존방식도 마찬가지로 저장된 feature들끼지의 discriminative를 높이는 목적인데, 이는 triplet loss를 통해서 가장 비슷한 key를 positive, 두번째로 비슷한 key를 negative로 두었다. 이는 더 효율적인 연산을 보이지만, 실험을 통해 성능은 contrastive loss가 더 좋다는 것을 보였다.


![](/assets/images/2023-01-15-insi2i/5.PNG)
# 4. [CVPR2022]InstaFormer - Instance-Aware Image-to-Image Translation with Transformer
본 논문은 instance의 스타일 및 content를 잘 유지하면서 translation 과정에서 어떻게 고려해야 할지를 연구한 위의 논문들과는 별개로, 트랜스포머를 사용하여 global-instace 관계를 잘 이해하는 논문이다. 사실상 이 논문은 트랜스포머에 instance들을 token으로 넣고, INIT 논문에서처럼 instance들을 reconstruction하는 방식을 채택하기 위하여 instance-aware i2i task를 적용한 것이다. 본 논문에서는 기존 방법들이 global 이미지와 instance들의 관계를 잘 이해하지 못한다는 문제를 제기하고, 이를 트랜스포머를 활용하여 해결한 것을 메인 contribution으로 제시한다. 본 논문의 contribution은 아래와 같다.
1. transformer를 self-attention을 통하여 통합하였다.
2. transformer에 layernorm을 adain으로 변경하여 style을 적용하였다.
3. CUT에서 사용한 global image의 patch기반 NCE loss를 확장하여 instance-level의 NCE loss를 제시한다. 

논문에서 제시한 훈련 프로세스를 정리하면 아래와 같다. 위의 대표도는 본문에 설명한 것이 많이 빠져있으므로 참고만 하자. 
1. 입력 이미지 x를 encoder를 통과하여 다운샘플링된 feature map c ($[h \times w \times l]$) 를 얻는다. 
2. c를 convoltuion을 한번 통과하여 트랜스포머에 들어갈 input p $[h/k \times w/k \times l^{\prime}_c]$를 얻는다.
3. c에서 ROIalign을 통하여 instance들을 뽑고, 이를 convolution을 통하여 instance patch $p^{ins$$ [1 \times 1 \times l^\{\prime}_c]$를 얻는다. 이를 2에서 얻은 global patch와 concat한다. 
4. Normal distribution에서 mapping network를 통과하여 style vector를 만든다. 또한 target domain의 이미지를 style encoder에 통과하여 style vector를 얻을 수 있다.
5. transformer에 3에서 얻은 concat된 patch embedding과 4에서 얻은 style vector를 입력으로 한다. 이 때 transformer는 layernorm이 아닌 adain으로 대체되었다.
6. decoder를 통과하여 생성 이미지를 만든다. 

loss는 아래와 같다.
1. 기존의 CUT의 global NCE loss (=입력 이미지와 생성 이미지를 encoder에 통과시키면서 중간 feature map을 NCE loss.)
2. 위의 3번 과정에서 뽑은 c에서 instance를 뽑아 만든 instance-level NCE loss
3. 실제 이미지와 생성 이미지를 분류하는 adv loss
4. 4에서 사용한 target domain 이미지의 style content를 imagereconstruction loss. 이는 content와 style의 disentangle을 돕는다.
5. target domain의 생성 이미지를 style encoder를 통과하여 만든 이미지의 style이 4에서 사용한 normal distribution 벡터가 mapping network를 통과하여 얻은 style vector 또는 target domain이미지가 style encoder를 통과하여 만든 style vector와 같다는 style reconstruction loss. 이는 content와 style의 disentangle을 돕는다.