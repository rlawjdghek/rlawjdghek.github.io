---
title:  "GAN 학습할때 팁들"
excerpt: "GAN 학습할때 팁들"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
  
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2021-07-05T12:48:00-05:00
---

[](https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/)와 
[](https://arxiv.org/pdf/1511.06434.pdf) 의 내용을 정리. 

하지만 Task에 따라 다르므로 그냥 구형하다가 생각 안나면 보도록 하자.

1. D에는 Pooling 사용 x. convolution에서 stride로 해결해라.

2. G에는 Upsample 써라.

3. fc layer는 D의 마지막, G의 처음에 써라.

4. BN써라. 단, G의 마지막, D의 처음에는 사용 x. Batchnrom을 모든 layer에 적용하는 것은 sample에 변동을 주며  모델을 불안정하게 한다. 
그렇기 때문에 discriminator의 input과 generator output에 batch norm을 적용하는 것은 피해야 한다.

5. G에는 Relu써라. 단, 마지막에는 빼고. 마지막은 Tanh써라.

6. D에는 LeakyRelu써라. 기울기는 0.2

7. Tanh를 사용하였기 때문에 입력을 [-1, 1]로 맞춰라. Normalize([0.5,], [0.5,])

8. G, D 네트워크 초기화 mean = 0, variance = 0.02

9. Adam을 lr = 0.0002, beta=(0.5, 0.999)로 써라. 

10. generator의 트레이닝시에 label과 loss function을 flip 한다.

11. generator에 대한 input으로서 사양하는 노이즈는 gasussian random number를 샘플로 한다.

12. batch norm statistics 계산을 위해서 real또는 fake끼리만 mini batch로 사용 한다.

13. discriminator에서 작은 random noise로 label smoothing을 사용 한다.

14. discriminator의 label에 random noise를 추가 한다.

15. 그러지 말아야할 이유가 있지 않은 한 DCGAN architecture를 사용한다.

16. discriminator에서 loss가 0.0인 것은 실패다.

17. generator의 loss가 일정하게 감소할 경우, 아마도 discriminator가 garbage image로 인해 잘못되고 있는 것일 수 있다.

18. label을 가지고 있다면 사용하는 것이 좋다.

19. discriminator의 input에 noise를 넣고 시간에 따라서 noise를 감쇄 시킨다.

20. training과 generation에서 50% dropout을 사용 한다.

21. generator와 discriminator는 stochastic gradient descent를 사용해서 128의 batch size로 훈련된다.
모든 모델은 mini-batch stochastic gradient descent(SGD)에서 batch size 128로 트레이닝 된다.
