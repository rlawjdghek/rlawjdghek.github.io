---
title:  "[ICLR2017]Wasserstein GAN"
excerpt: "[ICLR2017]Wasserstein GAN"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2021-11-02T15:04:00-05:00
---

정리 한하고 있다가 심층생성모델에서 발표해야 해서 읽는 논문.

* Notation
TV: Total Variation,
KL: KL divergence, 
JS: Jensen-Shannon divergence

# Introduction
널리 알려져 있듯이, generative모델이 학습하는 것은 real data의 확률분포($P_r$)로부터, learnable parameter를 활용하여 비슷한 분포($P_{\theta})의 확률값이 최대가 되는 
확률 변수를 생성하는 것이다. 즉, 우리가 가진 훈련 데이터에서 likelihood를 최대화 하는 샘플 하나를 찾는 것이다. 이는 $Pr$과 $P_{\theta}$의 KL을 최소화 하는 것과 같다. \
$P_r$을 추정하는 것은 매우 어렵기 때문에 기존에는 비슷한 확률 분포를 얻는 방법 2가지를 활용하였다. 
1. low dimensional manifold를 활용하여 얻는다.
2. 비교적 쉬운 조건부를 활용하여 SR이나 super resolution에 쓰이는 변형된 input image를 활용한다. 
하지만 이러한 방법들은 다양성이 없으므로 GAN등의 방법을 사용하여 여러 어플리케이션에 적용해왔다. 하지만 GAN은 매우 unstable하고 학습 과정을 해석하기 힘들다는 단점이 있다.

이 논문에서는 model의 분포가 real distribution과 얼마나 다른지를 측정하는 측도에 집중한다. 또한 이를 통해 언급하엿던 GAN의 문제점을 완화하였다.


# Different Distances
우선 [링크](https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i)를 참조하여 notation의 해석을 기록해 둔다. 논문의 말을 먼저 적으면, \
"Let $\mathcal{X}$ be a compact metric set (such as the space of images $[0, 1]^d$) and let $\Sigma$ denote the set of all the Borel subsets of $\mathcal{X}$".\
compact metric set은 닫힌 공간이라고 이해하자. 우리가 다루는 아무리 큰 데이터라도, 예를 들어 이미지라도, 각 사진의 해상도는 유한하고, 각 픽셀은 0~1사이의 값을 가지므로 닫힌 공간에서의 분포라고 할 수 있다. 
또한 이 가정이 성립하지 않는다면 앞으로의 내용이 거짓이 된다. Borel subset은 측정 가능한 집합이고, 측정가능의 확률적인 의미는 $\mathbb{P}_r, \mathbb{P}_g$같은 확률분포로 확률값이 계산될 수 있는 집합을 
의미한다. 또한 연속함수의 기대값을 계산하기 위한 수학적인 최소조건이 된다. Wasserstein distance에는 기댓값을 사용하기 때문에 이것이 필수조건이 된다. 

"Let Prob(\mathcal{X}) denote the space of probability measures defined on \mathcal{X}."\
probability measure는 probability distribution이라 생각하자. 참조에서는 단지 확률값의 측정을 어떤 공간에서 하는지, 그리고 확률변수가 묵시적으로 정의되 있다는 점의 차이만 존재한다고 한다. 
예를 들어 동전을 던질 때 앞면이 나오는 상황은 H, 뒷면은 T라고 하면 확률측도는 $\mathbb{P}(H)=p, \mathbb{P}(T)=1-p$라고 된다. 하지만 확률 분포에서는 확률 변수 X를 도입하여
$P(X=0)=p, P(X=1)=1-p$라고 한다.

간단한 예제로 Earth-Mover(Wasserstein distance)가 확률분포의 거리를 측정할 때에 안정적 (수렴 가능함)을 보였다.  

1. TV
TV 거리를 문장으로 말하자면, 두 확률 분포가 있을 때 모든 확률 변수의 범위들에 대하여 두 분포 확률값의 차이의 최댓값이다. 예를 들어
![](/assets/images/2021-11-02-WGAN/1.JPG)
그림에서 A가 초록색 그래프의 범위라면 초록색 분포의 확률값은 1이고, 파란색 분포의 확률값은 0이 되어 최대로 1이된다. 즉, 겹치지 않는 두 확률분포의 TV는 무조건 1이다. 
에제는 위의 그림처럼 두 확률분포가 겹치지 않을 경우 ($\theta!=0$)일떈 1, 겹칠 경우 ($\theta=0$)일땐 0이 된다. 

2. KL, JS
이 두 거리함수 또한 풀면 아래 그림과 같다.
![](/assets/images/2021-11-02-WGAN/2.jpg)
결론적으로는 무한대 또는 0으로 불연속

3. Earth-Mover(Wasserstein)
![](/assets/images/2021-11-02-WGAN/3.jpg)
그림과 같이 풀 수 있고, $Z_1=Z=2$일 경우에는 결국 $|\theta|$가 된다. 

예제가 의미하는 것은 real분포 (0, z)가 있을 때, 우리는 변수 $\theta$를 0으로 가게 해야 한다. 하지만 Gradient Descent로는 불연속이기 때문에 Wasserstein을 
제외한 나머지 distance로는 불가능하므로 실제 학습이 되지 않는다. 이를 볼 떄, TV, KL, JS는 두 확률 분포가 다를 때는 완전히 다르다고 판단하는 경향이 있다. 이는 D가 잘 죽게 하는 원인이 된다. 
정리하자면, 
1. TV, KL, JS는 두 확률 분포가 겹치지 않은 부분이 있으면 불연속이 된다.
2. EM은 TV, KL, JS보다 weak하다. $\biconditional$ JS, TV, KL은 수렴하지 않지만, EM은 수렴 할 수도 있다.
3. EM은 분포수렴이다. (분포 수렴은 개별 특징보다 전체적인 분포를 중시한다. 예를 들어, KL은 한 개의 샘플의 확률값에 대하여 무한대가 되면 전체 값이 무한대가 된다.) 

# Wasserstein GAN & Experiment
우선 WGAN의 단점부터 나열하자.
1. 느린 훈련속도
$WGAN에서 소개한 손실함수 $W(\mathbb{P}_r, \mathbb{P}\_g)=sup\_{||f|| \ leq 1} \mathbb{E}\_{x \sim \mathbb{P}_r} \[f(x)\] - \mathbb{E}\_{x \sim \mathbb{P}\_{\theta}}\[f(x)\]$
이다. Lipschitz condition을 맞추기 위해 weight clpping을 사용한다.
weight clipping은 WGAN에서 반드시 필요한 제한조건이지만 **훈련시간이 느려진다는 단점**이 존재한다.\

2. Optimizer와 lr
![](/assets/images/2021-11-02-WGAN/4.JPG)
알고리즘을 보면 D를 5번 훈련하고 G를 1번 훈련한다. 또한 D를 훈련할 떄 momentum-base optimizer (e.g Adam)은 학습이 안된다. 또한 learning rate도 낮아야한다. 

WGAN이 GAN에서 흔히 발생하는 4가지 문제를 완화하는 방법에 대해 기록한다. 
1. Mode Collapse
mode collapse가 발생하는 원인은 D가 gradient를 생성하지 못하는 것에서 시작한다. gradient를 받지 못하여 fix되면, G도 지금까지 D를 속이기 위해 만든 이상한 결과물만 만들고, 이로 인해 D는 유의미한 output을 
도출해내지 못한다. 즉, D가 먼저 수렴해버려서 vanishing gradient가 발생한다. G와 D 둘다 업데이트 되지 않는다. 하지만 WGAN은 D의 gradient가 죽을 일이 없기 때문에 (아래 그림 참고) D가 fix되지 않는다.
![](/assets/images/2021-11-02-WGAN/5.JPG)
WGAN논문에서는 실험 동안 한번도 mode collapse가 발생하지 않았다고 한다. 


2. 훈련과 손실함수의 관계파악 불가능 
기존 GAN의 큰 문제점으로는 훈련 도중 얼만큼 훈련이 남았는지 알기 힘들다는 것이다. 출력을 계속 생산하면서 눈으로 직접 확인하는 수밖에 없는데 이는 굉장히 비효율적. 
![](/assets/images/2021-11-02-WGAN/6.JPG)
하지만 WGAN은 두 확률분포의 거리를 나타내기 때문에 손실함수의 값이 감소하는 것이 성능에 비례하게 된다.

3. 안정성
WGAN의 논문에서는 DCGAN을 주로 활용하여 실험하였다. 기존의 DCGAN은 LSUN데이터에 대하여 안정적으로 학습을 하지만, BN이 빠지면 학습을 하지 못한다. 반면, 같은 BN이 없는 DCGAN을 
WGAN loss로 훈련하면 안정적인 학습이 가능하다. 

4. 균형
보통은 D가 수렴하는 문제가 강하기 떄문에(D의 힘이 보통 더 세다), mode collapse 또는 vanishing gradient 문제가 발생한다. 하지만 WGAN은 D의 gradient가 계속 살아있기 떄문에 
G와 D의 균형을 맞추기가 비교적 쉽다. 










