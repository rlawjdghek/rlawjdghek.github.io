---
title: "[NeurIPS2018]Vid2Vid : Video-to-Video Synthesis"
excerpt: "[NeurIPS2018]Vid2Vid : Video-to-Video Synthesis"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2022-05-19T15:04:00-05:00
---

리팩토링 코드 : [](https://github.com/rlawjdghek/Generative_Models/tree/main/GANs/Vid2Vid)
비디오 GAN을 시작하는 논문. 엔비디아에서 나온 논문인데, 논문은 쉽게 썼는데, 실제로 코드는 굉장히 어렵다. 코드 리팩토링 하면서 중요하게 느꼈던 것은
1. 원래에는 network와 model 2개의 클래스를 사용해서 model을 지정하고 그 안에 nn.Module을 넣은 식으로 코딩을 했다면 비디오에서는 5차원이기 때문에 4차원 입력이 들어가는 nn.Module 상위에 클래스가 한개 더 있어야 한다. 
따라서 내 코드에서는 basenetwork -> basemodule -> basemodel로 된다. 단, D는 그냥 4차원만 다루므로 이미지 때와 같다. 
2. 비디오는 데이터로더 개념이 다르다. 먼저 G가 한번에 몇 프레임을 볼지가 중요한데, 데이터 로더는 return할 때 비디오 한개의 전체 프레임을 리턴한다. 그 다음 for문을 한번 더 사용해서 Generator용으로 프레임을 부분적으로
추출해서 입력으로 보낸다. 이 코드에서 기본값은 3장의 프레임을 사용했으므로 G의 입력은 [BS x 9 x 512 x 512]이다. 
3. pretrained된 flownet을 사용하기 때문에 손실함수가 굉장히 복잡하다. Generator Module은 9개의 텐서를 리턴하고 모두 훈련과정에 사용된다.
4. 아래 그림은 오피셜 코드의 구조이다. 오피셜 코드는 D를 forward하는 과정에서 모든 모델을 업데이트 하므로 아래 그림의 D forward에서 손실함수가 있는것을 볼 수 있다. 
5. 코드기 굉장히 유동적이라 처음에 읽기가 힘들었다. 이 코드는 자주 보면 좋을듯 하다. 

![](/assets/images/2022-05-19-vid2vid/1.jpg)
![](/assets/images/2022-05-19-vid2vid/7.jpg)
![](/assets/images/2022-05-19-vid2vid/3.jpg)
![](/assets/images/2022-05-19-vid2vid/4.jpg)
![](/assets/images/2022-05-19-vid2vid/5.jpg)
![](/assets/images/2022-05-19-vid2vid/6.jpg)

# Abstract & Introduction
abstract는 논문 제목에서 알 수 있듯이 img2img에서 한 차원을 높여 vid2vid로 간 것이다. 단순 img2img으로 합성한 이미지를 모두 이어붙이면 굉장히 어색한 부분들이 (얼룩덜룩) 생기기 때문에 시간적 정보를 주어 
학습에 활용한다.

# Method 
기본적인 문제 정의는 img2img에서 차원만 증가하였기 때문에 notation과 최종 목표 함수를 간단히 적으면 아래와 같다. 
$\mathbf{s}_1^T = {s_1, ..., s_T}$, $\mathbf{x}_1^T = {x_1, ..., x_T}$를 각각 source domain (A)와 target domain (B)의 비디오 변수라 하자. 즉, 한 개의 비디오에는 시간 $T$만큼의 프레임이 존재한다. 
도메인 B에 속하는 생성 비디오를 $\mathbf{\hat{x}}_1^T$라 할 때, 만약 생성데이터가 B 분포를 그대로 따른다면 조건부 확률은 아래와 같이 정의될 것이다. 

\begin{equation}
p(\mathbf{\hat{x}}_1^T | \mathbf{s}_1^T) = p(\mathbf{x}_1^T | \mathbf{s}_1^T)
\end{equation}

### Sequential Generator
이 소단원에서 처음 나오는 문장은 다음과 같다. "To simplify the video-to-video synthesis problem, we make a markov assumption where we factorize the conditional distribution $p(\mathbf{\hat{x}}_1^T | \mathbf{s}_1^T)$
to a product form given by"

\begin{equation}
p(\hat{x}_1^T | s_1^T) = \prod\_{t=1}^T p(\hat{x}_1^T | x\_{t-L}^{t-1}, s\_{t-L}^t)
\end{equation}

즉, markov chain에 따라 이전의 모든 프레임이 아닌 일부 $L$개만 보는 것이라 할 수 있다. 완벽히 같은 식이 되기 위해서는 이전 $L$개가 아니라 전체가 다 되어야함. markov assumption이 적용된 위의 식으로부터, 현재
프레임을 생성하기 위해 1) 현재 도메인 A프레임, 2)과거 L개의 도메인 A프레임, 3) <u>과거의</u> L개의 생성 B이미지들이 필요하다. 또한 L의 수치에 따라 발생하는 장단점도 중요하다. 만약 L이 작을 경우, 즉 Generator에 들어가는 
과거 생성이미지가 많아질수록 더 영상의 질은 좋아지겠지만 (prior가 높아지므로), cost와 한번에 들어가는 프레임이 많아지므로 VRAM이 증가한다. 
반면 L이 너무 작으면, (극한의 경우 0일때에는 img2img와 같음) 과거의 상황을 전혀 알 수 없다. 또한 비디오의 특성상 전 프레임과 현 프레임상의 같은 부분이 굉장히 많다. 과거 프레임의 optical flow가 주어진다면, 과거 프레임으로부터
현재 프레임을 예측할 수 있다. optical flow를 활용한 F의 생성이미지를 수식으로 나타내면 아래와 같다.

\begin{equation}
F(x_{t-L}^{t-1}, s_{t-L}^t) = (1-\tilde{m}\_t) \odot \tilde{w}\_{t-1}(\tilde{x}_{t-1}) + \tilde{m}_t \odot \tilde{h}_t
\end{equation}

위의 식을 자세히 살펴보면, Generator F에 t-L부터 t-1까지의 도메인 B 생성 프레임과 t-L부터 현재 프레임 t까지의 도메인 A프레임이 들어온다면, 현재 프레임을 생성한다. 현재 프레임은 optical flow를 활용해서 두개의 항으로 나누어진다.
위의 식 첫번째 항에서 $\tilde{w}_{t-1} = W(\tilde{x}\_{t-L}^{t-1}, s\_{t-L}^t)$ 는 이전 t-L에서 t-1까지의 생성이미지와 현재까지의 도메인 A가 warp되어 optical flow를 예측한다. 즉, 생성된 B프레임을 기반으로 optical flow를 에측하고,
이 정보와 더불어 두번째항은 제일 처음의 그림의 Generator Pipeline에서 오른쪽으로 나가는 branch에 해당하는 부분이다. 코드에서는 img_raw로 표시된다. img_raw와 weight sum을 하기 위해서, 왼쪽의 flow tensor를 생성하고, 과거에 생성된
B이미지들, prev_B를 warp하여 현재 이미지를 만들어낸다. img_warp로 표시되는데 이것과 가중합이 되는것을 알 수 있다. 논문 식 (4)번 아래에 w와 h와 m에 대한 설명이 나오는데, 이것들은 각각 Generator Pipeline 그림의 왼쪽, 오른쪽, 가운데에 해당한다.

### Conditional Image Discriminator
이 소단원은 일반적인 Discriminator에 해당한다. 스케치에서는 D forward에 해당하는데, network architecture는 pix2pixHD의 MultiScaleD를 사용한다. 주의할 점은 이 D는 4차원만 다룬다. 코드에서도 BaseModule을 지정하지 않았다.
또한 pix2pixHD에서와 같이, 입력이 real_B와 gene_B만 들어가는 것이 아니라, (real_B, real_A)와 (gene_B, real_A)로 쌍을 지어서 들어간다.

### Conditional Video Discriminator
이 Discriminator는 스케치의 D forward T에 해당하는 것이다. 코드에서는 T는 temporal이라 해서 몇몇 프레임을 스킵하여 고려한다. 디폴트는 2. 스케치를 보면, 논문에서 말한 것과 같이
$(x_{t-K}^{t-1}, w_{t-K}^{t-2})$, $(\tilde{x}\_{t-K}^{t-1}, w_{t-K}^{t-2})$가 쌍을 지어서 가는 것을 알 수 있다. 이 Discriminator는 실제 optical flow이미지인 flowref가 real또는 gene이미지에 잘 반영이 되었는지
구분하는 역할을 한다. 따라서 optical flow정답을 주고 real또는 gene이미지를 쌍으로 구분하도록 한다. 

### Learning Objective function
최종 손실함수는 3개로 볼 수 있다.
1. Conditional Image Discriminator Loss
위 소단윈에서 설명한 손실함수이다. 일반적인 GAN loss에서 (real_B, real_A), (gene_B, real_A)가 들어간다. 

2. Conditional Video Discriminator Loss
위 소단원에서 설명한 손실함수이다. 일반적인 GAN loss에서 (real_B, flow_ref), (gene_B, flow_ref)가 들어간다. 

3. Flow Loss
Generator Pipeline 스케치를 보면, flow를 생성하고 생성된 flow를 활용하여 이전에 생성된 이미지를 warp하는 것을 볼 수 있다. 따라서 flow loss는 생성된 flow와 실제 flow를 맞춰주고, warp된 이미지와 실제 이미지를 
맞춰준다. 

