---
title:  "[CVPR2020]Revisiting Knowledge Distillation via Label Smoothing Regularization"
excerpt: "[CVPR2020]Revisiting Knowledge Distillation via Label Smoothing Regularization"
categories:
  - Paper & Knowledge
  
tags:
  - KD
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2021-11-22T15:04:00-05:00
---

Knowledge Distillation가 왜 효과가 있는지 근본적인 원인에 대한 분석을 다룬다. 제시한 메소드 자체는 굉장히 직관적으로 떠오를 수 있기 때문에 큰 기여를 보이지 않고 성능 또한 크게 증가할 수 있어 보이지만, 
KD에 대한 고찰이 명확하기 때문에 기존의 teacher를 사용한 KD 뿐만 아니라 self KD를 이해하고 새로운 방법을 제시하는 데에 많은 도움이 되는 논문이다. **핵심을 말하자면, KD는 기존의 방법론이 주장하는 비슷한 클래스
의 관점으로도 해석 할 수 있지만, soft label을 활용한 label smooting과 같은 regularization 방법에 더 가깝다는 것이다. ** \
결과적으로 저지들이 품고있는 의구심에 대한 해답을 제시하였고, 이를 기반으로 가장 먼저 떠오를 수 있는 방법으로 성능을 증가시킴으로써 논문의 완성도는 굉장히 높다고 본다. 하지만 최적은 아님.

# Abstract & Introduction
대표적인 2가지 질문을 가지고 시작한다. 논문의 흐름은 이 2가지 질문의 해답과 해답을 이용한 직관적인 방법론을 제시함으로써 성능 향상을 증명하였다. 질문은 아래와 같다.
1. 지금까지는 teacher가 student보다 강했다. 즉, 성능이 더 좋았다. 그렇다면 teacher가 student보다 약한 경우에는 어떻게 되는가?
2. teacher가 더 약하다는 것은 둘째치고, teacher가 학습이 거의 안된 정도라면 어떻게 되는가?
두 질문 모두 teacher가 student보다 약하다는 것을 가정하고 있지만, 두번째 질문은 더 나아가 teacher가 거의 학습이 안된 모델일 경우에는 어떻게 되는가이다.
저자들은 이를 해결하기 위해 먼저 dark knowledge를 재해석하였다. Hinton의 KD 논문에서 dark knowledge는 teacher가 student에게 주는 의미있는 정보라 하였다. 이러한 기존의 KD논문들에서는 Hinton이 들었던
예시와 같이 비슷한 클래스에 약간의 confidence를 줌으로써 비슷한 클래스에 더욱 정보를 준다고 주장한다. \
하지만 이 논문에서는 비슷한 클래스 뿐만 아니라 다른 클래스에서도 우리가 알지 못하는 도움이 되는 정보가 있다고 말한다. 

# Exploratory Experiments and Counterintuitive Observations
먼저 기존의 비슷한 클래스에 정보를 주는 soft label을 활용하기 때문에 KD가 의미가 있다는 주장에 반하는 증거를 댄다. 저자들은 이를 증명하기 위해 위의 두 가지 질문에 대응하는 두 가지의 실험을 준비하였다.

1. ReKD: 보통의 경우 teacher의 성능이 student보다 좋았다면 ReKD에서는 그 반대이다. 
2. DeKD: 보통의 경우 teacher의 성능이 어느정도 나오지만, DeKD에서는 teacher의 성능이 거의 나오지 않는다.
논문에서의 결과에 따르면 두 실험의 결과는 아래와 같이 요약할 수 있다.
1. ReKD에서 일반적인 KD (teacher -> student)의 성능이 ReKD (student -> teacher)보다는 낮다. 이유는 당연히 teacher의 본래적인 capacity가 높기 때문인데, 주목할 점은 
ReKD에서도 teacher만 사용하는 것보다 성능이 증가하였다는 것이다. 즉, 성능이 더 안좋은 모델이 teacher로 가더라도 성능이 더 좋은 student를 가르칠 수 있다.
2. DeKD에서 또한 성능이 거의 안나오는 teacher로 student의 성능을 올렸다.
이렇듯, 극단 적인 2번 상황에서도 student의 성능이 오른 것을 보면, teacher의 성능은 아주 중요하지 않고 (그래도 성능 향상에는 좋은 teacher가 좋다.) 기존의 방법론들이 주장하는 비슷한 클래스의 관점에서는 틀렸다는 것이다.
**따라서 저자들은 KD를 regularization으로 간주하였다.**

# Method
KD와 같이 soft label을 통한 regularization으로 가장 먼저 떠오르는 것은 label smoothing이다. 위의 2번과 같은 상황에서는 teacher의 성능이 아주 조금 있으므로 약간이라도 클래스의 정보가 반영되었지만, 
teacher가 전혀 학습이 안된 상황을 가정하면, 우리는 teacher로부터 uniform distribution을 갖을 수 있다. 즉, 모든 logit이 같다고 가정할 수 있고, 이 때의 teacher의 정확도는 클래스 수로 나눈값과 같다. 예를 들어
CIFAR100같은 경우는 1%이다. 모든 logit값이 같다는 말은 label smoothing과 완전히 동치이다. label smoothing은 정답 레이블만 confidence가 높고 나머지는 사용자가 정한 값으로 일정하지만, hard label을 적용한다면 
정답 레이블 값에 같은 logit을 갖는 벡터를 더한것과 같다. \
따라서 이를 비추어 볼 때, 저자들은 KD의 soft label을 학습된 label smoothing이라고 주장한다. 따라서 학습이 거의 안된 모델을 teacher로 사용하였을때에도 성능이 증가하였다는 것은 결국 보편적으로 알려진 label
smoothing을 사용하였기 때문이다.  

저자들은 이 사실을 이용하여 teacher가 필요없는 2가지 메소드를 제시한다.
1. $TF-KD_{self}$: 어떠한 모델을 먼저 훈련 한다음 이 모델을 활용하여 똑같은 모델을 학습하기 위한 soft label로 사용한다. 손실 함수는 아래와 같다. \
$\mathcal{L}_{self} = (1 - \alpha) H(q, p) + \alpha D\_{KL}(p\_{\tau}^t, p\_{\tau})$ \
2. $TF-KD_{reg}$: soft label로 label smoothing을 사용한다. computational cost가 거의 없다.
$\mathcal{L}_{reg} = (1 - \alpha) H(q, p) + \alpha D\_{KL}(p\_{\tau}^d, p\_{\tau})$ \ 

실험은 생략한다. CIFAR100, TinyImagenet, Imagenet에서 실험하였고 위의 방법들이 기본 student 성능보다 높게 나온다. 