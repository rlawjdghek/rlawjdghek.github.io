---
title:  "[ICLR2020]Contrastive Representation Distillation"
excerpt: "[ICLR2020]Contrastive Representation Distillation"
categories:
  - Paper & Knowledge
  
tags:
  - Knowledge Distillation
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2021-09-30T14:33:00-05:00
---

KD에서 반드시 알아야 할 최신 논문. 2번째 읽을때 이 메모를 적는다. 

# Abstract & Introduction
기존 Hinton이 제시한 KL divergence는 teacher와 student의 두 확률 분포를 같게 함으로써 teacher의 **마지막 representation 정보**만을 따라간다. 하지만, 저자들은 
이와 같은 distillation은 teacher의 구조적인 지식을 전달하지 못하므로 더 많은 정보를 전달 하기 위한 방법을 고안하였다.

한가지 증거로는 KL divergence에서는 input(보통 penultimate layer의 output)의 각 dimension의 값들을 독립적으로 본다. 따라서 각 값의 위치나 정답 클래스를 (supervised 에서만) 알면서도
추가적인 정보를 주지 못하고 있다. 다른 예로는 이미지 생성에서 L2 loss만 사용하면 blurred 되는 현상도 볼 수 있다. 왜냐하면, L2 loss는 인접 픽셀 (차원)간의 관계를 무시한다는 것을 전제로 하기 때문.
즉, 고차원에서 correlation을 잡아야 한다.

# Method
Contrastive Learning의 핵심은 metric space에서 비슷한 것은 (positive pair)더 가까이, 다른것은 (negative pair)은 더 멀게 하는 것.
이해하기 쉽도록 먼저 supervision이고, 자기 자신만 positive pair라고 하자. 다시 말해, 어떤 input $x_i$가 들어왔을 때, teacher를 이용하여 $f^s(x_i), f^t(x_i)$를 가까이 하고, $f_s(x_i), f^t(x_j)$는
멀게 하는 것이 목표이다. \
또한 mutual information: 두 정보의 상호 의존성을 측정하는 지표, 두 분포가 독립이면 0이 되고, dependancy가 강할 수록 값이 커진다고 이해. 높을수록 좋다. mutual information의 식은 아래와 같다. \
$I(X;Y) = \mathbb{E}\_{P\_{XY}} log{\frac{P_{XY}}{P_X P_Y}}$ \
로그에 있는 식이 KL divergence와 거의 유사함을 알 수 있다. 위의 식으로 부터, 우리는 joint distribution, product of marginal distribution을 두어야 한다. \

Student의 penultimate layer의 output을 $S = f^S (x)$, teacher를 $T = f^T (x)$라고 할 때, 우리는 teacher와 student의 output이 관계가 있다는 것을 가정하므로, 
joint distribution $p(S, T)$와 product of marginal distribution $p(S)p(T)$를 생각할 수 있다. 두 output의 dependency가 강해야 하므로 mutual information값도 높을 수록 좋고, 
이는 KL divergence를 최대화 하는 것과 같다. 또한 $C=1$은 teacher와 student에 같은 input이 들어가는 경우, $C=0$을 다른 이미지가 주어진 경우로 볼 때, 새로운 확률 분포 q를 아래와 같이 정의할 수 있다.\
$q(T, S | C = 1) = p(T,S), q(T, S | C = 0) = p(T)p(S)$. \
이 때 $q(C = 1) = \frac{1}{N+1}, q(C = 0) = \frac{N}{N+1}$. \ 

다음으로 $C=1$, teacher와 student의 output이 있을 때, 이것이 같은 확률은 아래와 같이 베이지안 룰로 설명이 된다. \
$q(C=1 | T, S) = \frac{q(T, S | C = 1)q(C=1)}{q(T,S|C=0)q(C=0) + q(x,y|C=1)q(C=1)} = \frac{p(T,S)}{p(T, S) + Np(T)p(S)}$ \ 
mutual information과 연결시켜 생각한다면, \ 
$log(q(C=1|T,S)) = log{\frac{p(T,S)}{p(T,S) + Np(T)p(S)}} = -log(1+ N \cdot \frac{p(T)p(S)}{p(T,S)} \leq -log(N) + log{\frac{p(T, S)}{p(T)p(S)}}$\
양변에 기댓값을 씌운다면, \
$I(T;S) \geq log(N) + \mathbb{E}\_{q(T,S|C=1)}{log(q(C=1|T,S))}$ \
즉, mutual information의 lower bound가 증가하므로 전체적인 teacher와 student의 의존도도 상승한다고 할 수 있다. 논문에 쓰여진 저자의 말을 빌리면,\
"I(T;S) is the mutual information between the distributions of the teacher and student embeddings. Thus maximizing $\mathbb{E}_{q(T,S|C=1)log(q(C=1|T,S)}$ w.r.t the parameters of the student network
S increases a lower bound on mutual information." \ 

우리는 새로운 확률 분포 q를 p에 대한 조건부 확률로써 정의하였기 때문에 정확한 분포에 접근 할 수 없다. 여기서 저자의 main idea가 나온다. \
새로운 모델 $h: \{\mathcal{T}, \mathcal{S} \} \rightarrow \[0, 1\]$을 정의하고, 이 모델을 $q(C=1 | T, S))$에서부터 뽑힌 sample을 입력으로 받아 log likelihood를 최대화 하도록 한다. \ 
$\mathcal{L}\_{critic}(h) = \mathbb{E}_{q(T,S|C=1)}\[\log h(T, S)\] + N\mathbb{E}\_{q(T,S | C=0)}\[1 - \log(h(T, S)))\]$\
위의 손실 함수는 같은 pair가 왔을때 (C=1) $h(T,S)=1$, 다른 pair (C=0)일 때에는 $h(T,S)=0$이 되어야 최대화 된다.\
지금 목표는 $q(C=1|T,S)$를 구하는 것이고, 이를 직접적으로 구하기 힘드므로 새로운 모델 h를 정의하여 간접적으로 가려는 것.\ 
그 과정에서 h에 대한 손실함수를 위의 식으로 $\mathcal{L}\_{critic}$으로 둠으로써 이 손실함수를 최대화 하는 $h$는 $q(C = 1 | T, S)$와 같아진다고 증명하였다. 

이제 $h$를 구체적으로 살펴보자. 우선 위에서 $h: \{T, S\} \rightarrow \[0, 1\]$임을 상기하자. 0과 1사이의 값을 맞추기 위하여 $h$를 아래와 같이 정의하였다. \
$h(T,S)=\frac{e^{g^T(T)' g^S(S) / \tau}}{e^{g^T(T)' g^S(S) / \tau} + \frac{N}{M}}$. \
$M$은 cardinality (training data instance의 갯수), $\tau$는 concentration level이다. (나중에 ablation으로 다룬다). CRD 손실함수는 이 $h(T,S)$를 $\mathcal{L}_{critic}$에 넣은 것이다. 

정리하자면, \
1. teacher와 student의 output의 mutual information을 최대화 해야한다.
2. 직접적으로 최대화 하기는 어려우므로 lower bound를 높인다.
3. 이를 위해 새로운 확률분포 $q$를 정의하였고, 베이지안 등으로 정리 후에 $q(C=1 \| T, S)$의 분포를 알아야 한다는 결론을 얻었다.  
4. $q(C=1\|T,S)$분포를 직접적으로는 알 수 없으므로 모델 $h$과 그에 대응하는 손실함수 $\mathcal{L}_{critic}$을 정의하여 최대화 하는 h를 찾는것이 $q(C=1 \| T, S)$를 찾는 것과 동일하다.
5. $h(T,S)=\frac{e^{g^T(T)' g^S(S) / \tau}}{e^{g^T(T)' g^S(S) / \tau} + \frac{N}{M}}$. 로 h를 표현한다. (이것이 실제로 코드에 들어가는 부분이다.)
6. 결과적으로 teacher의 output과의 mutual information을 최대화 하는 student를 찾는다. 식으로 나타내면 $f^\{S^ * \} = argmax_\{f^S\}\mathbb{E}_\{q(T,S\|C=1)\}\[\log(h^*(T,S))\]$ 

**이 논문에서의 메인 아이디어는 mutual information을 teacher와 student에 적용하였다는 것, mutual information의 lower bound를 높이는 테크닉은 InfoNCE와 같지만, 이 논문에서는 objective를 다르게 설정하였다는 것에서
novelty를 보였다. 또한 성능이 좋아서 억셉을 받을 수 있었다.**

나머지는 KD loss와 Cross entropy를 사용하여 최종 손실함수는 아래와 같다. 
$\mathcal{L} = \mathcal{L}\_{critic} + \mathcal{L}\_{KD} + \mathcal{L}\_{CE}$

# Experiments & Ablation Study
CRD의 대표적인 한계점으로는 classification에만 적용할 수 있다는 것이다. 따라서 내 생각엔 이 논문을 마지막으로 classification에만 적용되는 방법론을 소개하는 것은 마무리가 된 것 같다. 실험에 사용된 데이터셋은 모두 분류 데이터셋이다.
따라서 분류 데이터셋에서의 다양한 ablation study를 진행하였다. 
### InfoNCE vs CRD 
CRD는 InfoNCE와 경쟁적인 구도를 그리기 때문에 이에 대한 ablation study를 진행하였다. (당연히 CRD가 높게 나온다.)

### Hyperparameters and computation overhead
##### Number of negaties
우선 적절한 negative sample의 수를 분석하였다. CIFAR-100와 같은 작은 데이터셋에서라도 한개의 input에 대한 negative sample은 49500개 이므로 대략적인 최적의 negative sample의 수를 제시하는 것은 상당히 좋다.
당연하게도, negative sample의 갯수가 많으면 많을수록 좋지만, 논문에서 보여준 것과 같이 4096개와 16384는 거의 차이가 없지만, 연산량에서 많은 차이를 보인다. 

##### Temperature
Method에서 잠깐 나온 temperature에 대한 ablation도 진행하였다. 0.1이 최적이다. 

##### Computational cost
가장 중요한 것인데, CRD는 병렬 연산이 가능하므로 그렇게 오래 걸리지 않는다고 한다. 또한 VRAM은 600MB밖에 먹지 않아 그렇게 크지도 않다. 기존보다 12%정도 느리다고 한다.

