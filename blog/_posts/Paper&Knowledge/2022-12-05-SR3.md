---
title: "[Arxiv2021]Image Super-resolution via iterative refinement"
excerpt: "[Arxiv2021]Image Super-resolution via iterative refinement"
categories:
  - Paper & Knowledge
  
tags:
  - Diffusion Models
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2022-12-05T15:04:00-05:00
---

DDPM의 저자 Jonathan Ho가 DDPM을 발표한 이후 super resolution에 적용한 논문. DDPM의 개념을 알고 있다면 아주 쉽고, 오피셜 코드도 아주 잘 되어있어서 아래 프레임워크만 보면 이해가 된다. 본 포스팅에서도 논문을 읽다가 메모할 점만 적고 깊게는 적지 않는다.
![](/assets/images/2022-12-05-SR3/1.jpg)
위의 그림에서 주목할 점은 notation과 각 변수가 어떻게 종속 되어있는 지이다. x는 condition, y는 diffusion과정에서 상태에 해당하는데, unconditional DDPM에서와 마찬가지로 $y_T$는 순수 정규분포 노이즈, $y_0$는 훈련 데이터이다. 따라서 unconditional과 마찬가지로 $y_t$를 생성할 떄 x가 개입되지 않는 것이 중요하다. 이외에도 논문에서 시사하는 바와 중요한 점을 나열하고, 나중에 까먹엇을 경우에만 참고하는 용으로 읽어보자.

1. SR3에서 condition $x$는 forward markovian diffusion process $q$에 개입하지 않는다. 따라서 논문의 eq. (1) ~ (5)에서는 $x$가 없는 것을 볼 수 있다. 
2. 본 논문에서는 section2의 마지막에 $x$가 forward pass에도 condition을 줄 수 있다고 한다. 하지만 이를 future work로 남겨두었다.
3. 64의 해상도를 1024로 올리는 작업을 한다고 하자 (16배). 이 떄 **cascade generation**, 즉 4배씩 2번 작업하는 것이 더 좋다. 시간적 측면으로는, 정보가 아주 부족한 저해상도에서 올리는 것이 많은 sample step을 요하는데, 한번에 16배 확대하는 것은 4배씩 2번 하는것보다 더 많은 스텝을 요구한다. 
4. 본 논문에서는 DDPM의 Unet구조에서 1) BigGAN residual blk, 2) skip connection $\frac{1}{\sqrt{2}}$, 3) blk의 숫자 증가, 4) 저해상도 일수록 channel multiplier 증가.
5. Blur augmentation을 사용하였고, DDPM의 objective function을 MSE에서 L1으로 변경하였다. 
6. 256x256의 이미지넷 super-resolution을 수행하기 위해서 64x64로 훈련시킨 Improved DDPM이 생성한 이미지에 SR3를 적용하여 256으로 만들어 내었다. 이것의 성능은 256을 직접적으로 생성한 BigGAN과 거의 유사하다. 즉, 이런식으로 고해상도를 직접 생성하는 diffusion model에 적용될 수 있다. 이 개념은 추후 Dall-E2같은 모델에 사용되었다. 



