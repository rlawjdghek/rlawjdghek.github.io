---
title:  "[CVPR2018]CosFace: Large Margin Cosine Loss for Deep Face Recognition"
excerpt: "[CVPR2018]CosFace: Large Margin Cosine Loss for Deep Face Recognition"
categories:
  - Paper & Knowledge
  
tags:
  - Representation Learning
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2021-09-18T22:33:00-05:00
---
KD에 이어서 새로운 훈련 방법을 생각해내기 위하여 읽은 논문. SphereFace에 이어서 새로운 Loss와 해석을 소개한 논문. 

### Abstract & Introduction
목적은 다 똑같다. Intra-class variance를 최소화 시키고, Inter-class variance를 최대화 시킨다. 기존에 널리 알려진 softmax loss는 분류를 수행하기에 약하다. 
또한 좀 더 발전된 형태의 Angular-softmax (A softmax) 또한 두 가지 문제점을 가진다. * A-softmax = $cos(m \theta_1) = cos(\theta_2)$
 
1. cos함수는 단조성이 없다. 즉, 사이클 성질이 있으므로 수렴하기가 어렵다. 
2. inter-class feature들이 큰 margin을 가질 수도 있고, 작은 margin을 가질 수도 있다. 즉, 세타에 비례하여 커지고 작아지므로 어떤 상수 m이 세타에 dependent하다. 따라서 조절이 불명확.

### Methods
위의 세 논문은 모두 softmax loss를 베이스로 시작하므로 softmax를 해석하는 데에 중복된 내용이 많다. 요약해서 말하면, softmax에서 Wx는 결국 cos으로 나타낼 수 있고,
W의 각 column ($W_j$)은 x와 곱해지고 각 클래스의 logit을 결정한다. 즉, 어떤 feature x가 들어오면 마지막 final logit을 만드는데에 핵심이 된다고 할 수 있고, 수식적으로 보면
$W_j x$가 하나의 로짓값이고, 이 식은 $|W_j||x|cos(\theta)$라고 할 수 있으므로 하나의 클래스에 대한 센터라고 할 수 있다. 
**위의 논문들에서 내가 배운 가장 중요한 점은 마지막 FC 가중치 W의 의미이다. W는 feature extractor에서 뽑은 x의 feature를 최종적으로 분류하는데 어떤 input이 비슷하다면 비슷한 feature로 
뽑힐 것이고, W를 지남으로서 같은 클래스에 위치한 로짓값이 가장 크게 도출된다. 결국 각 클래스에 대한 로짓값은 W의 column들과 feature의 내적값이므로 이는 cos함수와 같다. 따러서 가중치 W는 
각 클래스의 center라고 볼 수 있다.** 

softmax를 설명하고 나면 결국 이 논문에서 사용하는 loss는 
![](/assets/images/2021-09-18-CosFace/1.PNG)
와 같다. 

코사인 값에 margin m을 주었다는 것에 주목하자.

여기서 소개하는 scale factor s와 margin factor m는 중요한 하이퍼 파라미터이므로 저자들은 섹션을 따로만들어 두 하이퍼파라미터에 대한 고찰과 범위를 알려준다. 
#### Scale factor s
우선 가중치 W와 feature x를 normalization하는 것은 매우 중요함. 만약 normalization을 안하면 loss를 줄이기 위해 두개의 절댓값을 줄여버리게 되므로 목적이 달라지게 된다. 따라서 consistancy를 
유지하기 위해 절댓값을 1로 맞춰주어야한다. 그에 따라 클래스들을 hypersphere에 올려좋았을 때 중심부터의 거리를 정해주기 위해 scale factor s를 사용한다. s는 반지름이라 생각 할 수 있다.
결론적으로 s는 아래의 lower bound를 갖는다.
![](/assets/images/2021-09-18-CosFace/2.PNG)


#### Margin factor m
m은 아래와 같은 범위를 갖는다. K는 feature의 dimension.
![](/assets/images/2021-09-18-CosFace/3.PNG)
첫번째 식인 $K=2$일때, 즉 2차원일 때를 보자.
예를 들어보자. 8개의 클래스가 있을 떄, 2D평면에 원을 그려놓고 각 클래스들이 최대한 벌어지게 하려면 원에서 8개의 점을 45도 각도로 나누면 된다. 이렇게 생각한다면,
$cos\theta_1 - m > cos\theta_2$에서 $\theta_1$

을 기준으로 한다면 각 각도에 $\theta-1$을 빼면 된다. 즉, 식은

$1 - m > cos(\theta_2 - \theta_1)$이고, $\theta_2 > \theta_1$이라 가정해도 일반성을 잃지 않는다. $\theta_2 - \theta_1$이 가질수 있는 최대값 (각도)는 $\frac{2\pi}{C}$라 할 수있다. 
이 때 C는 클래스의 갯수. 위의 예제에서는 8개였으므로 45도가 나오는 것을 확인 할 수 있다. 따라서 $m < 1 - cos(\frac{2\pi}{C})$

하지만 2차원인 경우는 거의 없으므로 아래 두 식이 많이 사용되는 것을 주목하자. 첫번째 식에서 C가 커지므로 C를 $cos(\frac{2\pi}{C})$에 근사시킨것 같다.  

### Experiments
![](/assets/images/2021-09-18-CosFace/4.PNG)
위의 그래프는 다양한 m에 대해서 정확도를 구한 것이다. m이 너무 커지면 오히려 성능이 떨어지는 것에 주목. 


