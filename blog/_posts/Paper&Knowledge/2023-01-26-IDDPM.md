---
title: "[PMLR2021]Improved Denoising Diffusion Probabilistic Models"
excerpt: "[PMLR2021]Improved Denoising Diffusion Probabilistic Models"
categories:
  - Paper & Knowledge
  
tags:
  - Diffusion Models
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2023-01-26T15:04:00-05:00
---

DDIM이 fidelity vs diversity trade-off를 보여주고, sampling step을 줄였다면, 본 논문에서는 trade-off와 sampling rate를 줄이면서 추가적으로 성능을 높이는 몇가지 방법을 제시한다. 본 논문에서 제시한 것들을 나열하면 아래와 같다.

1. $L_{vlb}$ 도입. vlb로스를 단독으로 사용하려면 resampling이 필수적으로 사용되어야한다. 또한 $L_{vlb}$만 단독으로 사용하게 되면 FID가 너무 높아지기 때문에 좋지 않지만, 기존 $L_{simple}$와 같이 사용하는 $L_{hybrid}$는 NLL을 낮춰주고 FID를 약간 높이므로 사용하는 것이 좋다.
2. 기존의 linear scheduling은 t=T에 가까워 질수록 굉장히 많은 노이즈를 준다. 이에 반해 t의 샘플링은 uniform하므로 균형이 맞지 않다. 이를 완화하기 위해서 cosine scheduling을 도입하였다. 
3. resampling. 1번에서 말한대로 훈련과정에서 time을 뽑을 때 uniform이 아니라, loss에 가중치를 두어서 t에서 로스가 클수록 뽑히는 확률도 높게 설정된다. 실제 코드에서는 특정한 t에 대하여 최신 10개의 값을 저장하고 평균을 내준다. 

결론적으로, cosine scheduling은 사용하고, $L_{vlb}$는 단독으로 사용하지 않는것이 좋다. $L_{hybrid}$를 resampling을 적용해서 사용하자. 

# Method

### Improving the Log-likelihood
log likelihood는 generative model이 훈련 데이터에 있는 드문 분포도 잘 생성할 수 있도록 하는 지표이다. 기존 DDPM에서는 vlb를 최소화 하는 것이 아닌, 분산을 고정시키고, 평균만 최소화 하는 $L_{simple}$을 사용하였다. 

사실 본 논문에서 확장된 hybrid loss에 사용하는 $L_{vlb}$는 DDPM 논문에서 증명하였듯이, lower bound를 그대로 사용하는 것이기 때문에, 수식 자체로는 DDPM에서 본 것과 같다. 즉, t=0일때의 NLL과 t=1~1000까지의 kl loss를 합친것이다. 주목할 점은 DDPM에서는 KL loss함수에서 분산을 무시하였다는 것인데, DDPM 논문에서는 분산을 학습시키는 것이 더 나쁜 성능을 보였다고 말한다. 본 논문에서는 분산을 고정하여 성능이 하락한 것은 noise의 관점에서 불안정한 학습이 이루어지기 때문이라고 추측하였다. 따라서 본 논문에서는 먼저 DDPM에서 고정된 분산으로 상요한 prior beta와 posterior beta의 비율을 시간에 대해서 살펴보았다.  

![](/assets/images/2023-01-26-IDDPM/1.PNG)

위의 그림과 같이 prior와 posterior는 극초반의 t를 제외하고는 거의 동일한 값을 갖는다. 즉, 둘 중 어느값을 사용하더라도 DDPM에서 말한 것과 같이 비슷한 성능을 보일수 있다는 것이 증명되었다. 하지만 이것이 분산을 고정시키는 것이 좋다고 하는 것은 아니다. 

![](/assets/images/2023-01-26-IDDPM/2.PNG)
또한 위의 그림에서 볼 수 있듯이, diffusion 모델의 loss는 초반 과정에서 굉장히 많이 줄어든다. t=T일때의 beta는 0.02부터 t=0일때의 0.0001까지 linear하게 줄어드는데, 이 과정에서 beta들은 굉장히 작은 값이다. 따라서 이것에 log를 취하게 되더라도, 모델이 직접적으로 이러한 작은 beta값들을 학습할것이라는 보장이 없다. 따라서 본 논문에서는 beta를 직접적으로 학습하는 것이 아닌, 두 beta의 log 스케일의 보간값을 만들도록 학습한다. 즉, 아래식의 $v$를 학습하도록 훈련된다. 

\begin{equation}
\Sigma_{\theta}(x_t, t) = exp(vlog\beta_t + (1-v)log\tilde{\beta_t})
\end{equation}
코드에서는 모델이 기존의 평균에 회귀하기 위해서 입력 이미지 shape $[H \times W \times 3]$을 output하도록 설계되었다면, 각 차원에서 $v$까지 예측하기 위해서 $[H \times W \times 6]$의 shape을 예측하도록 한다. 

###  Reducing Gradient Noise
위의 $L_{vlb}$로 학습을 하면 NLL은 낮아지지만, FID는 굉장히 높아지는 것을 볼 수 있다. 즉, 분산을 학습함으로써 생성 모델의 목표인 VLB는 낮출수 있으나, 이미지의 인지적 생성 능력은 떨어진다. 저자들은 이것이 학습 과정에서 많은 noise를 야기하기 때문이라고 주장하였다. 아래 그림은 각 loss별 loss값을 훈련 시간에 대해서 그린것이다. 
![](/assets/images/2023-01-26-IDDPM/4.PNG)
![](/assets/images/2023-01-26-IDDPM/5.PNG)
확실히 vlb는 전체적으로 학습이 이루어지지만 분산이 큰 것을 볼 수 있고, $L_{simple}$과 결합된 hybrid도 마찬가지이다. 또한 두번째 그림에서 $L_{vlb}$의 gradient noise scale이 큰것도 뒷받침해준다. 저자들은 이 문제를 loss그래프의 추이에서 발견하였다. 논문에서는 diffusion model의 학습 과정에서 loss의 추이에서 영감을 받았다고 하였으나, 개인적으로는, diffusion model의 reverse process에서 t=T에 가까울수록 더 semantic한 이미지를 생성한다. 즉, reverse 초기에 많은 정보를 생성하고, 후반으로 갈수록 fine grained를 생성한다. 직관적으로, 초기의 샘플링이 더 중요하다는 것을 주목할 수 있고, 학습에서는 초기의 loss를 줄이는데에 더욱 집중을 하면 생성 과정의 기반을 잡는다고 생각이 된다. 논문에서는 time별이 아니라 전체 학습 iteration의 loss그래프를 언급하였으나, time별로 접근하는 것이 좀 더 직관적인 이해를 도출할 수 있는 접근이라고 생각이 든다. 

결과적으로 time을 샘플링 할때에는 각 time 별로 최근 10개의 손실 함수를 저장해두고, 이를 평균한 것에 비례하도록 확률을 정한다. 즉, loss가 큰 time일수록 뽑힐 확률이 증가한다. 

### Improving the Noise Schedule
다음으로 주목한 것은 linear scheduling이다. linear scheduling의 알파값을 그리면 아래와 같다. 
![](/assets/images/2023-01-26-IDDPM/3.PNG)

즉, t=1000에 가까울수록 굉장히 많은 노이즈를 주게 된다. 따라서 이를 보완하기 위해서 cosine scheduling을 제시했는데, 굳이 이것을 채택한 이유는 가운데 부분의 gradient가 가장 크다는 것에 주목하였다고만 언급하였다. 수학적인 분석은 추후에 관련이 된다면 알아보자. 

# Experiment
실험파트에서 주목할만한 결과를 나열하면 아래와 같다. 
1. hybrid는 NLL을 낮춰주고 FID를 약간 증가시키므로 사용하는 것이 좋다.
2. vlb만 사용하는 것은 FID를 너무 높이므로 별로다.
3. 샘플링 속도 면에서는 DDIM이 50 timestep미만에서는 좋지만 그 이상일때는 IDDPM이 더 좋다. 
4. 100 timestep이면 4000 time step과 비슷하다. 
5. GAN은 likelihood 비교가 불가능하여 대체제로 precison recall로 평가하였다. IDDPM이 BigGAN보다 FID는 높고, precision은 약간 낮지만 recall이 높아서 diversity면에서 여러 mode를 잘 cover했다고 볼 수 있다. 다만, precision과 fidelity가 비례한다는 high-level이해에서는 BIgGAN의 FID가 낮게 측정된 것은 diversity가 많이 차이나기 때문이라고 추측된다.