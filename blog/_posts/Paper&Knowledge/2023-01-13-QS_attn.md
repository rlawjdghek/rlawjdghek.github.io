---
title: "[CVPR2022]QS-Attn: Query-Seleced Attention for Contrastive Learning in I2I translation"
excerpt: "[CVPR2022]QS-Attn: Query-Seleced Attention for Contrastive Learning in I2I translation"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2023-01-13T15:04:00-05:00
---

이 논문은 "[CVPR2022]Exploring Patch-wise Semantic Relation for Contrastive Learning in Image-to-Image Translation Tasks"과 마찬가지로 CUT 논문의 contrastive loss를 구성하기 위한 feature map의 patch selection을 고도화하는 논문이다. 두 논문의 기본적인 아이디어는 동일하게 아래와 같다. 
**가존 CUT은 patch를 무작위로 뽑기 때문에 도메인 변환에 있어서 중요한 패치(얼룩말의 머리, 다리 등)를 고려하지 않는다. 이미지 내의 변환해야할 물체에 해당하는 패치를 뽑을 수 있을 것이다.**

본 논문에서는 attention을 활용한 두가지의 주요 연산을 한다.
1. 중요한 query를 선택하기 위한 attention map와 entropy 계산 후, 작은 entropy를 기반으로 중요한 패치 선택. 
2. 선택된 중요한 패치에 대한 attention map을 value (feature map)에 행렬곱. 

1번 연산에 의하여 패치의 중요도를 고려하여 선택할수 있고, 2번 연산에 의하여 패치간의 유사도를 반영할 수 있다.

![](/assets/images/2023-01-13-QS_attn/1.PNG)
# Method 
CUT에서는 HW개의 패치에서 N개를 뽑아, real과 gene의 contrastive를 계산한다. 나머지 디테일은 넘어간다. 

먼저 본 논문에서 제시한 query selection알고리즘을 자세히 나열한다. 목표는 중요한 patch N개를 뽑는 것이다. 
### Global Attention
1. feature F = [H x W x C]를 [HW x C]로 변경하고, Q=F, K=F.T라 하자. self-attention과 달리 projection연산을 하지않고 attention map A=QK.T를 구한다.
2. A=[HW x HW]이고, 열을 기준으로 softmax를 취한다. 그러면 i행의 의미는 i번째 패치에 대해서 HW개의 패치의 유사도를 확률분포로 나타내었다. 
3. i번째 확률분포, 즉 i행의 엔트로피를 계산한다. 엔트로피는 확률값들이 비슷하면 높고, 일정 값이 높으면 낮아진다. 따라서 **엔트로피가 낮으면 이 패치는 적은 패치들과 관계성이 높기 때문에 중요하다. 반대로, 엔트로피가 높으면 모든 패치에 대해서 비슷비슷하기 때문에 덜 중요하다. 얼룩말의 머리는 잔디받과 달리 몸통, 다리와 높은 상관관계를 가질 것이다.**
4. 엔트로피를 오름차순으로 정리하고, 낮은값 N개를 뽑는다. 즉, A는 $A_{QS}$=[N x HW]가 된다.
5. 기존의 feature map F = [H x W x C]를 [HW x C]로 바꾸고 이는 value V이다. value V를 $A_{QS}$와 곱해준다. 즉, $A_{QS}V$=[N x C]
6. $A_{QS}$를 gene 이미지의 feature map에 동일하게 적용하여 [N x C]의 행렬을 얻는다.
7. NCE loss를 계산할 때에 주의할점은 CUT에서는 gene 이미지에 대하여 패치를 무작위로 뽑았는데, 여기서는 real이미지에 대하여 attention map을 계산한다. 
attention map $A_{QS}$는 N개의 중요한 패치와 나머지 HW의 패치의 관계성을 가중치로 갖고, 이를 V와 곱하면, 관계성을 반영한 N개의 패치에 대한 feature가 나온다. $[N \times C]$.

### Local Attention
global은 모든 패치의 관계를 고려하는 과정에서 패치가 너무 많다면 smoothing 현상이 일어날 수 있다. 따라서 모든 패치의 관계를 고려하는 것이 아닌, local의 접근을 제시한다. global은 attention map을 계산할 때 모든 HW개의 패치와 HW개의 패치의 관계를 고려했고, local은 주위의 $w^2$개의 패치만을 고려한다. 즉, 
query Q = [HW x C], key K = $[HW \times w^2 \times C]$이고 attention map A = $[HW \times w^2]$이다. 즉, HW개의 패치에 대해서 주변 $w^2$개의 패치와의 관계만을 고려한다. 엔트로피는 global과 같이 계산후 $A_{QS} = [N \times w^2]$. 위의 global 5번 연산을 수행하려면 먼저 value를 뽑는데 문제는 global과 달리 모든 패치를 안보고 주변 $w^2$만을 고려했기 때문에 value도 projection을 위해서 $w^2$개만을 뽑아서 convolution연산을 한다. 기존 feature map에서 N개의 선택된 패치에 해당하는 패치를 뽑을 때 주변까지 $w^2$개를 뽑고 attention map $A_{QS}$와 곱한다. $A_{QS}V = [N \times w^2] \times [N \times w^2 \times C]$

global과 local에서 위에서 언급한 2가지 주요연산을 다시 생각해보면, 1번에 해당하는 entropy 기반의 query패치 선택은 중요한 패치를 선택한다는 직관적인 설명이 가능하다. 2번 연산인 $A_{QS}$을 기존 feature map에 projection하는 것은 관계성을 patch에 반영한다고 볼 수 있다. 패치간의 관계성을 고려함으로써 더 넓은 receptive field를 가질수 있고, real과 gene이미지의 feature map에 동일하게 반영됨으로써 source와 target 도메인의 관계성을 일치하게 해준다.


# Experiments
실험에서 볼 것은 한가지 정도인듯 하다. local과 global을 동시에 사용하는 것을 설명한다. 먼저 중요한 query를 고르는것은 local로 진행한다. 그 결과로 우리는 중요한 패치의 위치 인텍스를 얻는다. 다음으로 [HW x HW]의 global attention map에서 local의 중요한 패치 N개를 뽑는다. projection은 global attention map으로 진행한다. 정리하면, 중요한 패치를 뽑는것은 주변만 보고 결정하고, 선택된 주요 패치에서 feature map에 관계성을 부여하는 것은 모든 패치를 이용한다. 