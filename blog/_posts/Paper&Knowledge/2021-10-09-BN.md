---
title:  "배치정규화 정확한 계산"
excerpt: "배치정규화 정확한 계산"
categories:
  - Paper & Knowledge
  
tags:
  - Normalization
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2021-10-09T15:04:00-05:00
---

Batch Normalization의 실제 계산을 맨날 궁금해 하다가 제대로 기록해본다. 

아래와 같은 코드를 돌리면
```python
import torch
sample_feature = torch.tensor([
    [1,0,0],
    [3/5, 4/5, 0]
])


bn_1d_layer = torch.nn.BatchNorm1d(3, affine=False)
print(bn_1d_layer(sample_feature))

bn_1d_layer.eval()
print(bn_1d_layer(sample_feature))
print(bn_1d_layer.running_mean)
print(bn_1d_layer.running_var)
```

다음과 같이 나온다
```python
tensor([[ 0.9999, -1.0000,  0.0000],
        [-0.9999,  1.0000,  0.0000]])
tensor([[ 0.9655, -0.0414,  0.0000],
        [ 0.5457,  0.7872,  0.0000]])
tensor([0.0800, 0.0400, 0.0000])
tensor([0.9080, 0.9320, 0.9000])
```

어떤 원리로 돌아가는지 이제 직접 계산해보자. 수식이 복잡해서 손으로 풀고 스샷 ㄱㄱ.
![](/assets/images/2021-10-09-BatchNorm/1.jpg)