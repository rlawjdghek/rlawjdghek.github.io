---
title: "[NeuRIPS2021]TransGAN: Two Pure Transformers can Make One Strong GAN, and That Can Scale Up"
excerpt: "[NeuRIPS2021]TransGAN: Two Pure Transformers can Make One Strong GAN, and That Can Scale Up"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2022-03-16T15:04:00-05:00
---

GAN을 Transformer로 구현한 최초의 논문. 구현은 오피셜 코드도 잘 되어있다. 단점은 학습이 너무 오래걸리는 것. 또한 StyleGAN류와 달리 1024까지 고해상도 이미지를 만드는 데에는 실패한 것 같다.
논문에서는 256x256까지만 실험했고, 이것도 단일 GPU로 돌릴때 배치를 D는 2, G는 4로 지정해 주어야한다. 1024까지 가면 모델이 너무 커질듯. 구현은 [링크](https://github.com/rlawjdghek/Generative_Models/tree/main/GANs/TransGAN) 참조.
하지만 최근 Transformer가 굉장히 다양한 task에 접목되면서 GAN에서의 transformer가 처음 적용되었다는 것이 큰 첫걸음이라 보인다. 최근 CVPR2022에서도 StyleGAN2를 Swin Transformer로 구현한 StyleSwin이 나왔다. 
구조를 자세히 그린 아래 도면 참조.
![](/assets//images/2022-03-21-TransGAN/4.PNG)
![](/assets//images/2022-03-21-TransGAN/5.PNG)
![](/assets//images/2022-03-21-TransGAN/6.PNG)
![](/assets//images/2022-03-21-TransGAN/7.PNG)
![](/assets//images/2022-03-21-TransGAN/8.PNG)
![](/assets//images/2022-03-21-TransGAN/3.PNG)
### Abstract & Introduction
지금까지는 Transformer로 업샘플링을 사용하지 않아 GAN같은 생성모델에서 Transformer의 활용에 어려움이 있었다. 기존 연구들은 D에만 적용하여 ViT처럼 사용할 수 있었다. 
또한 GAN의 수렴 자체에서도 Transformer의 사용이 거의 불가능하다고 여겨졌는데, 이 논문에서는 Transformer의 수렴성을 돕기 위해서 data augmentation, modified normalization,
relative position encoder등을 사용하여 수렴을 도왔다. CelebA, LSUN등에서 SOTA를 달성하였으나 초 고해상도의 이미지에서의 결과가 없는것이 아쉽다.

### Method
**지금부터 다루는 것은 256x256에 해당**
논문에 나온 그림은 실제 구현을 굉장히 단순화 한 것이기 때문에 디테일한 것은 구현을 참고해야한다. 
![](/assets//images/2022-03-21-TransGAN/1.PNG)

#### Mrmory-friendly Generator
논문에 실린 G의 구조 표는 3번째블록까지는 Upsampling할때 bicubic을 사용했다고 나오는데, 실제 구현에서는 전부다 pixelshuffle을 사용했다. pixelshuffle은 입력 feature map이
[BS x 1024 x 64 x 64]라면, 아웃풋은 채널이 1/4로 줄고, H, W가 2배씩 늘은 [BS x 256 x 128 x 128]이 된다. 즉 채널면에서 더 효율적인 장점을 가질 수 있다. 아마 저자들이 헷갈려서 reproducibility 상에 문제가 
있는듯 하다. 
<br/>
1. Transformer는 3차원 텐서 [BS x N x C]를 주로 다루는데, 4차원의 이미지로 처리하기 위해서 중간중간 permute와 reshape을 자주 활용한다. N이 H*W가 되고 C는 채널을 의미하기 때문에 [BS x N x C]를 변형하면
[BS x C x H x W] (H*W = N)이라 볼 수 있다. 
2. 가장 앞 단을 보면 z가 MLP를 거쳐 [BS x 1024 x 8 x 8]로 된다. 여기서 256까지 총 5번을 upsampling하는데 중간 레이어들은 5개의 StageBlock과 각 StageBlock은 여러개의 Block으로 이루어졌다. 맨 위의 구조 도면 참조.
3. 각 StageBlock은 upsample - position embedding - block 연산으로 이루어진다.
4. upsample은 bicubic 또는 pixelshuffle (실제 구현에서는 pixelshuffle만 사용)
5. position embedding은 처음 z에서 embedding transformation을 거쳐 embedding pos을 더한 embedding 벡터를 각 stageblock에 더해준다. 
6. block연산은 cross attention - window partition - attention - window reverse - mlp연산으로 이루어진다. (중간중간 normalization과 dropout은 생략)
7. cross attention은 임베딩벡터와 인풋의 multi-head attention연상을 한 것.
8. window partition은 아래 이미지와 같이 항상 보는 window size를 16으로 유지하기 위해서 인풋 feature를 16x16으로 자른뒤, [(BS*가로갯수*세로갯수) x 16 x 16 x C]로 reshape해준다. 이를 attention에 넣음. 이 작업을
해주지 않으면 중간 N이 굉장히 커져 연산을 할 수 없게 된다. 
![](/assets//images/2022-03-21-TransGAN/2.PNG)
9. window reverse는 [(BS*가로갯수*세로갯수) x 16 x 16 x C]를 다시 [BS x (16*가로갯수) x (16*세로갯수) x C]로 배열. 

### Multi-scale Discriminator
1. D는 multi scale을 학습하기 위해서 맨 처음 입력 이미지를 conv를 통해 [BS x 96 x 128 x 128], [BS x 96 x 64 x 64], [BS x 192 x 32 x 32]로 나눔. 
2. 맨 처음 [BS x 96 x 128 x 128]부터 [BS x (128*128) x 96]으로 reshape한뒤 window partition - attention - window reverse 연산을 거친뒤 average pool으로 가로세로를 2배만큼 줄여줌. 채널은 그대로
3. 2번에서 나온 [BS x (64* 64) x 96]과 2번째 conv를 통과한 [BS x (64*64) x 96]을 concat한 뒤 다시 window partition - attention - window reverse 연산.
4. 3번도 마찬가지.

**구현단에서 window 나누는 shape 계산이 복잡하고 attention모듈과 cross attention모듈을 구현해 두면 나머지는 모두 반복. 코드를 보자.**







