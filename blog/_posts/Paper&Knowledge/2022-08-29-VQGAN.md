---
title: "[CVPR2021]Taming Transformers for High-Resolution Image Synthesis"
excerpt: "[CVPR2021]Taming Transformers for High-Resolution Image Synthesis"
categories:
  - Paper & Knowledge
  
tags:
  - GANs
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2022-12-26T15:04:00-05:00
---

![](/assets/images/2022-08-29-VQGAN/1.jpg)
![](/assets/images/2022-08-29-VQGAN/2.jpg)

Dall-E의 기반이 된 모델. 이 때까지 GAN 분야에서 SOTA를 달성하였는데, 논문을 읽어보면, VQ-VAE에 discriminator와 샘플링 단계에서 기존의 pixelcnn계열의 모델을 트랜스포머로 바꾸었다. 위의 이미지처럼 인코더 디코더는 크게 달라진 구조가 없고, 중간에 vector quantization 하는 구간만 잘 보면 된다. 소스코드는 [링크](https://github.com/rlawjdghek/Generative_Models/tree/main/GANs/VQ-GAN) 참조. 

훈련 요약
1. vQ-VAE, GAN으로 encoder, quantizer, decoder 훈련
2. transformer가 quantized index를 예측하도록 훈련

추론 요약
1. quantized index를 transformer가 생성.
2. decoder로 이미지 생성.

**또한 본 논문에서 자주 사용하는 용어인 context, context-rich는 이미지의 픽셀 하나에서 주변의 픽셀 정보와의 조화를 뜻한다. 이미지 전체의 context도 될 수 있지만, 이는 너무 broad한 뜻이기 때문에 약간의 locality를 추가했다는 것으로 이해하자. 따라서 CNN이 context-rich codebook을 학습한다는 것은 downsample되면서 주변의 정보를 이해하는 것이다. 즉, 본 논문에서 context-rich는 CNN에 대응되는 용어. Transformer는 global consistency.**

# Abstract & Introduction
이 논문에서는 최근 transformer vs CNN 분석 논문들에서 주장하는 것과 마찬가지로, CNN과 transformer를 같이 사용하는 것이 효과적이라고 말한다. 즉, low-level은 CNN으로 충분하고, high-level은 transformer로 관계성을 학습하여 생성의 성능을 높이는 것이다. 이는 SAGAN에서 보여준것과 마찬가지로 self-attention layer를 가장 깊은 레이어에 추가함으로써 성능 향상을 보여주는 것과 같은 말이다. 이미지의 low-level은 CNN의 local한 부분만 보는 kernel를 통한 local-connectivity에 의하여 묘사된다. 하지만, high semantic level에서는 이해하는 데에서 어려움이 발생한다. 따라서 CNN은 기존에 갖고있는 강력한 이미지의 local correlation에 대한 지식을 뽑아내는데에 효율적이고 (또한 이는 낮은 cost로 이어질수 있다.), 이를 완화하기 위한 transformer를 사용해야 한다. 

본 논문에서는 위의 CNN과 transformer가 갖고 있는 장점을 살리기 위하여 encoder와 decoder는 CNN을 사용하고, vQ-VAE를 활용하여 quantized 된 vector index를 이해하는 데에는 transformer를 사용하였다. 따라서 본 논문의 contribution을 정리하면 아래와 같다.

1. convolution을 사용하여 주변의 context를 풍부하게 잡는 codebook을 만들고, 이미지를 구성하고 있는 요소들을 학습한다. 
2. 이미지 내의 구성 요소들 간의 long-range interactions을 학습하기 위해서 transformer 구조를 사용한다. 
3. 마지막으로 GAN을 활용하여 (discriminator) transfoemer가 low-level을 학습하도록 한다. (이는 본 논문에서는 자세히 설명하지는 않았으나, PatchGAN과 perceptual loss를 사용한것을 GAN으로 묶어서 low-level을 학습할 수 있다고 한 것 같다.)

![](/assets/images/2022-08-29-VQGAN/3.PNG)
# Method 
VQ-GAN의 가장 큰 특징은 기존 CNN에 transformer와 VQ-VAE의 개념을 더하여 생성 이미지를 더욱 locally realistic(CNN), globally consistent patterns(transformer)하게 표현하는 것이다. 그 중에서 transformer로 VQ-VAE에서 제시한 codebook을 효과적으로 예측함으로써 추론 과정에서 더욱 현실적인 이미지를 생성한다. 

### Learning an effective codebook of image constituents for use in transformers (CNN part)
VQ-VAE에서 추론에 사용할 입력은 N개의 latent vector를 가진 codebook에서 적절한 것을 골라 디코더로 넣는 것이다. 본 논문에서는 이 codebook을 학습하는 법을 VQ-VAE에서 제시한대로 사용한다.
1. $[BS\times H\times H\times C]$의 입력 이미지를 CNN 인코더를 통과하여 $[BS\times h \times w \times n_z]$의 shape인 z를 계산하고, conv한개를 통과해서 $[(BS\times h\times w)\times embed]$로 변형. 
2. 미리 초기화 해둔 codebook $\mathcal{Z}$ (실제 구현에서는 nn.Embedding)에서 가장 가까운 벡터를 $[(BS\times h \times w)]$개 뽑는다. codebook에 있는 벡터만으로 이루어진 벡터를 $z_q$라 하자. 뽑는 방법은 포스팅 맨 위에서 2번째 그림에서 보자. 
3. $z_q$를 디코더에 통과시켜 reconstruct한 이미지를 얻고, VQ-VAE에서 제시한 loss function $L_{VQ}$로 인코더, 디코더, codebook을 학습한다. $L_{VQ}$는 아래와 같이 정의된다. 정확한 구현은 마찬가지로 2번째 이미지의 6,7번을 보자.

\begin{equation}
L_{VQ}(E,G,Z) = \parallel x - \hat{x} \parallel + \parallel sg[E(x)] - z_q \parallel_2^2 + \parallel sg[z_q] - E(x) \parallel_2^2
\end{equation}

4. VQ-GAN에서는 patchGAN의 discriminator와 LPIPS loss를 활용하여 추가적으로 학습한다. 이 떄 adv loss에는 adaptive weight $\lambda$를 사용한다. $\lambda$는 아래와 같이 정의 된다.

\begin{equation}
\lambda = \frac{\nabla_{G_L}[L_{rec}]}{\nabla_{G_L}[L_{GAN}] + \delta}
\end{equation}

$L_{rec}$는 LPIPS perceptual loss이다. 

### Learning the composition of images with transformers (transformer part)
위에서 한 대로 훈련을 마치면, 이제 추론을 위한 transformer를 학습한다. 우리는 codebook에서 어떤 벡터를 사용할지를 정하는 인덱스만 있으면 이미지를 생성할 수 있다. 트랜스포머는 주어진 i개의 인덱스를 통해서 다음 인덱스를 예측하도록 학습된다. 즉, 모든 인덱스간의 관계를 학습하면서, 지금까지의 부분 이미지를 보고 다음으로 올 적절한 이미지에 대한 예측을 한다. 논문에서 있는 식은 기존의 다음 인덱스 예측을 나타내는 식이고, condition에서는 트랜스포머에 condition을 추가하여 예측한다. condition도 마찬가지로 인덱스화 되어 들어간다. ImageNet과 같은 이미지 클래스는 1개의 벡터로 인코딩되고, segmentation과 같은것은 위에서 설명한 것과 똑같이 추가적인 VQ-GAN을 학습하여 condition index를 만든다. 그 이후 트랜스포머를 학습할 때 사용된다.

![](/assets/images/2022-08-29-VQGAN/4.PNG)
하지만 위와 같이 i개의 모든 인덱스를 트랜스포머에 활용하면 megapixel 정도의 이미지에서는 입력이 매우 길어지기 떄문에, 트랜스포머가 다음 인덱스를 예측하는 것이 불가능하다. 따라서 본 논문에서는 sliding-window를 활용하여 모든 인덱스를 보는 것이 아닌, 약간의 locality를 추가하여 다음 인덱스를 예측하도록 하였다. 

# Experiments 
실험은 주목해 볼 결과만 나열한다. 
1. 논문의 4.1섹션에서는 index를 예측하는 과정에서 Transformer와 pixel snail을 비교하였는데 transformer가 더 나은 성능을 보인다. 
2. 다양한 (condition, uncondition)에서 모두 좋은 성능을 보인다. (image inpainting, semantic image synthesis, pose-guided synthesis, class conditional image synthesis, superresolution)
3. 2번까지는 256해상도로 실험하였고, 그 이상의 해상도에서는 downsample을 5번 하였고, sliding window방식으로 초고해상도의 이미지를 생성한다. 
4. 논문의 4.3섹션에서는 transformer를 고정시키고, downsample의 횟수를 다르게 하여 실험하였다. 이 실험에서는 downsample이 많이 될수록 CNN이 볼 수 있는 영역이 넓어지고, codebook이 더욱 context rich하게 학습된다. 논문에서는 입력 이미지와 latent의 사이즈의 비율을 f라 하였고, 이 f값을 변화시키면서 실험하였다. f가 커지면 downsample을 많이 한 것이다. 또한 트랜스포머가 고정되어있기 때문에 latent의 사이즈는 $16\times 16$으로 하였고, 입력 이미지의 사이즈를 변화시켰다. 결론적으로는 receptive field를 키울수록, 즉, downsample을 많이 할수록 좋은 성능을 보였는데, 본 논문에서는 f=16이상의 실험을 진행하지는 않았다. 
5. VAE이기 때문에 모델이 reconstruction을 얼마나 잘하느냐도 중요하다. Reconstruction된 이미지에 대하여 VQ-VAE2는 10의 FID를 보이는 반면, VQ-GAN은 1.7의 FID를 보인다. 