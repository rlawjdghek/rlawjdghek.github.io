---
title: "[IJCV2019]DRIT++ - Diverse Image-to-image Translation via Disentangled Representations"
excerpt: "[IJCV2019]DRIT++ - Diverse Image-to-image Translation via Disentangled Representations"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2022-06-05T15:04:00-05:00
---

MUNIT과 비슷한 시기에 나온 논문. 둘다 구현해본 결과, DRIT++은 잘 안되는 것 같다. 또한 학습 변수나 모듈이 너무 많아서 속도가 느리고 메모리도 많이 잡아먹는다. 차라리 CycleGAN이나 MUNIT이 나은듯. 
논문을 읽으면서 MSGAN (Mode Seeking GAN)과 비슷한 문장이 많이 있다는 것을 느낌. 코드는 [링크](https://github.com/rlawjdghek/GANs/tree/master/DRIT%2B%2B) 참조. 구현상에는 forward 과정이 
굉장히 복잡하기 때문에 encoder에 두개의 도메인의 content encoder를 모두 넣고 입력 받을때는 real_A와 real_B가 들어간다. 또한 공식 코드는 파이토치 최신버전에서 더블 업데이트 문제로 학습이 안되므로 
중간에 forward를 한번 더 해야한다. 내 코드에서도 업데이트 할 때마다 업데이트를 했다. 
논문에서는 style이란 표현을 사용하지 않고 attribute를 사용하지만, 본 포스팅에서는 일관성을 위해 style이란 표현을 사용한다. 
스케치는 realA, geneA등으로 네이밍 되어있는데, **style을 기준으로 A,B를 붙였다.**
![](/assets/images/2022-06-04-DRIT_pp/1.jpg)

# Abstract & Introduction
기존의 CycleGAN과 Pix2Pix등은 모두 한개의 도메인에서 한개의 도메인으로 변환하는 Unimoal 학습 모델이다. MUNIT과 비슷하게 이 논문에서는 domain-invariant인 content space와 domain-specific attribute space로 나누어
한가지 content에 대하여 여러 attribute를 입힐 수 있는 multimodal 모델을 제시한다.

Multimodal을 학습하기 위해서는 random noise vector를 generator에 주입한다. MSGAN에서도 말했듯이, 또한 content, style개념을 사용하는 여러 image-to-image translation 모델을 학습하다보면, 
content 정보가 매우 강력하여 mode collapse가 일어나기 쉽다. 즉, style을 담당하는 noise vector가 무시된다. 논문 굵직한 내용을 나열하면 아래와 같다.
1. multimodal unpaired image-to-image translation
2. cross cycle loss 사용
3. mode seeking regularization 사용
4. 2개의 generator와 discriminator를 사용
5. content를 분류하는 D 도입.
6. 논문의 대부분 설명은 dual domain으로 이루어지지만, multi-domain으로도 확장 가능하다.

# Method 
### Disentangled Representation for I2I Translation
목표를 다시 한번 상기하면, 두개의 도메인 A와 B가 있을 때, A에서 B로 변환하는 multimodal mapping 하는 모델을 학습하는 것이다. DRIT++은 multi-domain도 가능하지만, multi-domain에서는 도메인을 나타내는 원핫 벡터가 필요하므로
multi-domain 섹션외에는 두개의 도메인에 대하여 다룬다. 

DRIT++은 5개의 모듈을 2개씩 갖는다. 
1. 이미지에서 content를 뽑는 encoder $\mathcal{E}_c$ 단 content encoder는 shared weight를 사용함.
2. 이미지에서 style을 뽑는 encoder $\mathcal{E}_s$
3. content와 style을 받아 이미지를 생성하는 generator $\mathcal{G}$
4. 이미지를 받아 분류하는 discriminator $\mathcal{D}$
5. content feature를 받아 분류하는 discriminator
따라서 가장 위의 스케치를 보면 서로 다른 모델 (네모 상자)가 총 10개 있는 것을 알 수 있다. (Encoder는 도메인을 구분하지 않았다.) 코드 구현과 스케치의 차이점은 네모 박스 단위로 모듈을 구현한 것이 아닌, 한방에 입력이 들어가서 
forward_A, forward_B로 구현되었다. 모듈이 10개나 되므로 하나씩 하면 복잡함. 

#### Disentangle Content and Attribute Representation
논문에서 주장하는 것은 content가 공유되기 때문에 각 도메인의 이미지의 content는 한개의 content 집합 $\mathcal{C}$의 원소들이고, style들은 각각의 도메인의 style 집합 $\mathcal{A}$에 포함된다. multi-domain셋팅이라도 학습에서는
2개의 입력 쌍이 들어간다. 이를 $\mathcal{X}, \mathcal{Y}$라 하자. 그러면, 공유되는 content $\mathcal{C}$와 함꼐 attribute $\mathcal{A_{\mathcal{X}}}, \mathcal{A_{\mathcal{Y}}}$로 나눌 수 있다. 이는 content encoder와 
style encoder를 거쳐서 추출한다. 

한편, content가 공유되는다는 가정에 더욱 치중하기 위해서 content encoder에 shared weight를 도입한다. content encoder의 forward함수 코드는 아래와 같다.
```python
    def forward(self, A, B):
        outputA = self.convA(A)
        outputB = self.convB(B)
        outputA = self.conv_share(outputA)
        outputB = self.conv_share(outputB)
        return outputA, outputB
```
convA, convB는 A와 B를 각각을 위한 encoder이고, 이는 모두 conv_share를 거쳐 최종 content map을 뽑게 된다. 
shared weight로 high-level feature를 공유한다 하더라도, 두 도메인의 content 정보를 완전히 같게 하기는 힘들다. 따라서 저자들은 
content discriminator를 도입하여 두 도메인 이미지에서 나온 content를 더욱 가깝게 한다. 즉, 두 도메인의 content가 같은 space에 놓이도록 제약을 건다.
content discriminator의 손실 함수의 식은 아래와 같다.
$$
\begin{aligned}
\mathcal{L}\_{adv}^{content}(E\_{\mathcal{X}}^c, E\_{\mathcal{Y}}^c, D^c) &= \mathbb{E}\_x[\frac{1}{2}log D^c(E\_{\mathcal{X}}^c(x)) +\frac{1}{2}log(1-D^c(E\_{\mathcal{X}}^c(x)))] \\
&+ \mathbb{E}\_y[\frac{1}{2}log D^c(E\_{\mathcal{Y}}^c(y)) + \frac{1}{2}log(1-D^c(E\_{\mathcal{Y}}^c(y)))]
\end{aligned}
$$
위의 식은 일반적인 GAN loss랑 약간 다르다. content discriminator가 $\frac{1}{2}$로 예측할 때 최적이 된다. 

#### Cross-cycle Consistency Loss
CycleGAN에서 처음 제시된 cycle-loss는 content를 학습하는데 도움이 된다. 이를 활용하여 본 논문에서도 cycle 이미지를 만들고 손실 함수로 설정한다. 이해는 위의 스케치에서 가장 오른쪽에 있는 cycleA, cycleB를 보자.
1. realA로부터 CA, SA를 뽑고, realB로부터 CB, SB를 뽑는다. 
2. geneB를 만들기 위해 CA, SB를 합치고, geneA를 만들기 위해 CB, SA를 합친다. 
3. geneA에서 다시 recon_CB, recon_SA를 뽑고, geneB에서 recon_CA, recon_SB를 뽑는다.
4. recon_CA와 recon_SA를 합쳐 cycleA를 만들고, recon_CB와 recon_SB를 합쳐 cycleB를 만든다.
5. cycleA와 realA, cycleB와 realB를 L1 loss로 한다. 

#### Other Loss Functions
마지막으로 다른 손실함수를 정리한다. 모두 스케치에 있으나 별도로 표시는 안했다.
1. Domain adversarial loss : geneA와 geneB에 대한 adversarial loss이다.
2. Self-reconstruction loss : reconA와 reconB에 대하여 realA와 realB의 복구 loss
3. Latent regression loss : 스케치를 보면 z1과 z2또한 Generator를 통과하여 gene_random_A등을 만든다. gene_random_A에서 다시 encoder를 통과하여 style을 뽑고, random noise z는 multimodal을 주는
style에 해당하므로 gene_random에서 뽑힌 style과 z를 맞춰준다.
4. mode seeking loss : MSGAN에서 제시한 mode seeking loss이다. 스케치에서 z1과 z2를 사용하는 이유이다.

최종 손실함수를 식으로 나타내면 아래와 같다. 
\begin{equation}
\mathcal{L} = -\mathcal{L}\_{D, D^c} + \lambda^{cyc}\mathcal{L}^{cyc} + \lambda^{recon}\mathcal{L}^{recon} + \lambda^{latent}\mathcal{L}^{latent} + \lambda^{KL}\mathcal{L}^{KL} + \lambda\_{cls}^{domain}\mathcal{L}\_{cls}^{domain}
\end{equation}
1번째는 기본 adversarial, content adversarial, 2번째는 cycle loss, 3번째는 reconstruction loss, 4번째는 latent regression, 5번째는 코드에서 concat버전인데, 이는 크게 성능변화가 없어 빼도됨. 6번째는 multi-domain일때만 적용된다. 

### Multi-Domain Image-to-Image Translation
포스팅 Method 초기에 말했듯, DRIT++은 Multi-domain에도 적용가능하다. 지금까지 봤던 두개의 도메인에서 여러 도메인을 나타내는 원핫 벡터를 조건으로 주면 된다. 

# Experiment
FID : 생성 이미지들의 quality를 비교하기 위한 metric이다. DRIT++논문의 FID에 대한 설명은 generated distribution과 real의 거리를 잰다고만 한다. 실제 구현 방법은 나오지 않음.
LPIPS : diversity를 측정하는 metric이라고 설명됨.
JSD and NDB : 실제 분포화 생성분포의 유사성을 재기 위하서 두개의 bin-based metric을 제시한다. MSGAN에서도 사용된 metric인데 전체적인 알고리즘을 설명하면 아래와 같다.
1. 훈련 데이터를 K-means를 사용하여 k개의 클러스터로 나눈다. 
2. 생성 데이터 또한 K-means를 사용하여 k개의 클러스터로 나눈다. 
3. bin화 된 데이터들의 비율을 계산한다.
만약 mode collapse가 일어났다면 한개의 클러스터에만 집중될 것이므로 안좋은 결과를 보여줄 것이다. 낮은 것이 좋다.

