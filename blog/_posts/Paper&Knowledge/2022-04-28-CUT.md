---
title: "[ECCV2020]Contrastive Learning for Unpaired Image-to-Image Translation"
excerpt: "[ECCV2020]Contrastive Learning for Unpaired Image-to-Image Translation"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2022-04-28T15:04:00-05:00
---

constrastive learning을 사용했다는 것 제외하면 나머지 구현은 cycle-gan과 거의 유사하다. 코드 구현은 [링크](https://github.com/rlawjdghek/Generative_Models/tree/main/GANs/CUT) 참조.
### Abstract & Introduction
MUNIT의 가장 기저가 되는 가정, 두개의 서로 다른 도메인에서 content는 공유하고 style은 다름,을 전제로 한다. 또한 MUNIT에서 언급했던 cycle-consistency 제약이 bijection 성질을 강화하기 때문에,
이미지 변환의 목표를 확실하게 하는 대신, 다양성을 줄이는 단점을 갖고 있다는 문제를 해결하기 위해 contrastive learning을 활용하여 cycle-consistency문제를 완화한다.
이 논문의 저자는 cycle-gan의 저자로써 기존 cycle-gan이 갖고 있는 강한 cycle-consistency를 줄이면서 content는 유지하고 appearance는 달리하는 방법론을 제시한다. 자세히 말하면, 두 도메인의 mutual
information을 최대화 하기 위해서 여러 input과 하나의 output에 대하여 contrastive learning을 수행한다. 이 때 NCE loss를 사용. 이로 인하여 cycle-gan과 달리 Generator와 Discriminator를 2개씩 사용할 
필요가 없어진다.


![](/assets/images/2022-04-28-CUT/1.PNG)
### Method 
#### Adversarial loss
먼저 기본적인 Least Square GAN으로 adversarial loss를 정의한다. 

#### Mutual information maximization
Contrastive learning의 메인 아이디어는 query와 query에 대응하는 positive example, negative example을 학습에 활용한다. 본 논문의 표기에 따르면, query vector는 $v$, 
positive vector는 1개이므로 차원이 K라고 할 때, $v^+ \in \mathcal{R}^K$, negative는 n개가 있다고 하면, $v^- \in \mathcal{R}^{N\times K}$라 하자.
논문에서는 각 positive와 negative를 같은 unit sphere space에 두고, 이는 collapsing 또는 exploding을 방지한다고 한다. 
주어진 1개의 query에 대하여, 1개의 positive, n개의 negative로 인하여 우리는 $n+1$ 경우의 수의 classification을 수행할 수 있다. positive만 학습한다는 classification을 생각해 보면, 주어진 쿼리 벡터와 positive의 
내적은 정답이고, 나머지 n개의 negative 쌍에 대해서는 오답이 된다. 
다시 말하면 정답 레이블이 [1, 0, 0, ..., 0]일 때, 예측 벡터는 $\[v \cdot v^+ , v \cdot v^-_1 , v \cdot v^-_2 , ... , v \cdot v^-_n \]$가 되고, 이를 수식으로 나타내면 아래와 같다. 

$l(v, v^+, v^-) = \log[\frac{exp(v \cdot v^+ / \tau)}{exp(v \cdot v^+ / \tau) + \sum_{n=1}^{N}exp(v \cdot v^-_n / \tau )}]$


#### Multilayer, patchwise contrastive learning
위의 contrastive loss를 만족하기 위해서 가장 먼저 생각해 볼 수 있는 것은 생성된 1개의 output을 query라 생각하고, 이 output을 만들기 위해 사용된 domain A의 input이 positive, 나머지 domain A의 이미지들이
negative가 될 것이다. 본 논문에서는 전체 이미지가 아닌 하나의 이미지에서 positive와 negative를 패치로 뽑아 사용하였다. 이렇게 패치화의 장점으로는

1. 한 장의 이미지로도 GAN의 학습이 가능하다.
2. 전체 이미지를 사용하지 않기 때문에 코드의 효율성이 높아진다. 패치화는 구현에서 64개의 샘플을 사용했기 때문에 전체 이미지를 사용하면 코스트가 크다.

한장의 이미지에서 패치를 뽑는 데에는 다양한 방법이 있을 수 있지만, 여기서는 multi-scale을 고려하기 위해서 G의 각 encoder layer마다의 feature map에서 patch를 추출하였다 (공식 코드에서는 G의 0,4,8,12,16번째 레이어의
feature map). 그 다음 뽑은 패치들을 [BS x H*W X C]로 변환한 뒤에 MLP레이어 (코드에서 F 네트워크)를 통과하여 벡터화 한다. G에서 patch들을 뽑는 코드는 아래와 같다. layers변수는 [0,4,8,12,16]이 들어가고 encode_only=True.
즉, feature가 레이어들을 계속 넘어가면서 layers에 있는 인덱스번째의 레이어의 feature map이 저장된다. 마지막 layers인덱스 이후로는 forward안되도 됨.
```python
def forward(self, input, layers=[], encode_only=False):
    if -1 in layers:
        layers.append(len(self.model))
    if len(layers) > 0:
        feat = input
        feats = []
        for layer_id, layer in enumerate(self.model):
            # print(layer_id, layer)
            feat = layer(feat)
            if layer_id in layers:
                # print("%d: adding the output of %s %d" % (layer_id, layer.__class__.__name__, feat.size(1)))
                feats.append(feat)
            else:
                # print("%d: skipping %s %d" % (layer_id, layer.__class__.__name__, feat.size(1)))
                pass
            if layer_id == layers[-1] and encode_only:
                # print('encoder only return features')
                return feats  # return intermediate features alone; stop in the last layers

        return feat, feats  # return both output and intermediate features
    else:
        """Standard forward"""
        fake = self.model(input)
        return fake
```

이미지 한 장에서 뽑은 patch의 갯수를 n_patch라 할 때, 하나의 이미지에서 추출된 패치의 positive와 negative를 자세히 말하면 다음과 같다.
query : output이미지에서 뽑은 n_patch개의 패치들이 각각 query가 된다. 즉, 각 패치마다 query가 되므로 n_patch만큼 계산을 해야함
positive : input에서 output과 같은 위치에서 patch를 n_patch개 뽑는다. 그 중 현재 query과 같은 위치에 있는 patch
negative : 현재 query와 다른 위치에 있는 patch

패치를 뽑는 과정은 아래 코드와 같다.
```python
def calc_NCE_loss(self, src, target): 
    n_layers = len(self.nce_layers)
    feat_q = self.G(target, self.nce_layers, encode_only=True)
    feat_k = self.G(src, self.nce_layers, encode_only=True)
    feat_k_pool, sample_ids = self.F(feat_k, self.args.num_patches, None)  # 이 버전에서는 0,4,8,12,16번째 레이어에서 뽑음. 총 5개의 feature map
    feat_q_pool, _ = self.F(feat_q, self.args.num_patches, sample_ids)  # feat k의 패치위치는 랜덤, q는 k에 맞춰서 간다.        

    total_nce_loss = 0.0
    for f_q, f_k, cri, nce_layer in zip(feat_q_pool, feat_k_pool, self.criterionNCE, self.nce_layers):
        loss = cri(f_q, f_k) * self.args.lambda_NCE
        total_nce_loss += loss.mean()
    return total_nce_loss / n_layers
```
src, target은 이미지로써 calc_NCE_loss함수는 G의 손실함수는 계산하는 코드에서 아래의 코드와 같이 호출되는 것에 유의하자. 코드를 보면 src와 target이 query와 key와 잘 매칭이 안되보일 수 있는데, target이 domain B의 생성된 이미지라서
query가 되기 때문이라는 것에 주의하자. 중간 feature를 feat_q, feat_k를 먼저 만들고, feat_k_pool이 F네트워크를 통과하면서 위치정보를 담는 sample_ids를 같이 받고, 같은 위치에 feat_q_pool을 만든다. 

```python
self.loss_G_NCE_A = self.calc_NCE_loss(self.real_A, self.gene_B)
self.loss_G_NCE_B = self.calc_NCE_loss(self.real_B, self.idt_B)
```
첫번째 loss는 domain A의 이미지와 A를 넣어 생성된 domain B의 이미지의 NCE로스를 구한다.<br/>
두번째 loss는 domain B의 이미지와 B를 넣어 생성된 domain B의 이미지의 NCE로스를 구한다.<br/>

패치들을 벡터화 한뒤 F네트워크를 통과시킨 벡터들의 NCELoss구하는 코드는 아래와 같다.
```python
class PatchNCELoss(nn.Module):
    def __init__(self, opt):
        super().__init__()
        self.opt = opt
        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='none')
        self.mask_dtype = torch.bool

    def forward(self, feat_q, feat_k):  # feat_q는 src (=gene_B)가 netF를 통과한 [(bs*n_patches=64) x dim], feat_k는 target (= real_A)이 통과
        num_patches = feat_q.shape[0]  # bs*n_patches
        dim = feat_q.shape[1]
        feat_k = feat_k.detach()

        # pos logit
        l_pos = torch.bmm(
            feat_q.view(num_patches, 1, -1), feat_k.view(num_patches, -1, 1))  # 내적
        l_pos = l_pos.view(num_patches, 1)  # [(bs*n_patches) x 1]



        # neg logit
        batch_dim_for_bmm = self.opt.batch_size
        # reshape features to batch size
        feat_q = feat_q.view(batch_dim_for_bmm, -1, dim)  # [bs x n_patches x dim]
        feat_k = feat_k.view(batch_dim_for_bmm, -1, dim)  # [bs x n_patches x dim]
        npatches = feat_q.size(1)
        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))  # [bs x n_patches x n_patches]

        # diagonal entries are similaritytween same features, and hence meaningless.
        # just fill the diagonal with very small number, which is exp(-10) and almost zero
        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]
        l_neg_curbatch.masked_fill_(diagonal, -10.0)
        l_neg = l_neg_curbatch.view(-1, npatches)

        out = torch.cat((l_pos, l_neg), dim=1) / 0.07  # [(bs*n_patches) x (n_patches + 1)]

        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long, device=feat_q.device))  # 정답은 가장 앞쪽에 있는 l_pos이므로 0으로 둔다.
        return loss
```

먼저 배치사이즈(BS)를 고려하면 전체 패치갯수는 BS * n_patches가 된다. 따라서 각각의 BS * n_patch개의 query에 대하여 positive logit의 shape는 [BS*n_patches x 1]이 되고, 
negative는 BS*n_patches의 query에 대하여 n_patches-1개의 값이 나온다. 이는 단순히 행렬곱을 하여 BS * n_patches * n_patches의 텐서를 구하고 각 배치의 대각원소는 positive이므로 이를 -10으로  
만들어주면 softmax를 취했을 때 0에 가까운 값이 나온다. 그 다음 positive logit ([BS * n_patches x 1])과 negative logit ([BS * n_patches x n_patches])를 마지막 차원에 대하여 concat해주고,
temperature (=0.07)을 나눠주어 최종 loss를 만들어 준다. 정답은 맨 앞에 있는 positive이므로 정답 레이블은 모두 0으로 준다 (type=torch.long). 

#### Final objective
최종 손실함수는 아래와 같다.
$\mathcal{L}\_{CUT} = \mathcal{L}\_{adv}(G, D, X, Y) + \lambda_X \mathcal{L}\_{PatchNCE}(G, H, X) + \lambda_Y \mathcal{L}\_{PatchNCE}(G, H, Y)$
H는 MLP로 이루어진 F 네트워크라 생각하면 된다. 두번째 항, $\mathcal{L}_{PatchNCE}(G, H, X)$는 우리가 현재 풀고자 하는 task를 다시 상기할 해보면 (domain A의 이미지를 domain B로 변환), 
A의 이미지를 G에 넣고 그 때 나온 생성된 domain B의 이미지를 다시 G에 넣어 나오는 feature map들과 input A가 G를 통과하면서의 feature map들을 NCEloss.

반면, 세번째 항은 위의 self.loss_G_NCE_B에 해당되는데, idt_B는 cycle-gan의 identity loss와 비슷하게 domain B의 이미지와 이를 입력으로 하는 생성 이미지 (idt_B)를 NCEloss한다. 

#### Discussion
cycle-gan의 cycle-consistancy loss는 conditional entropy H(X|Y)의 upper boundary와 같다. 따라서 cycle-consistancy loss를 최소화 하는것은 H(X|Y)를 최소화하고, 이는 X와 Y의 dependancy를
최대화 하는 것과 같다. 이는 mutual information 관점에서 생각해 볼 수 있는데 먼저 mutual information의 정의를 보자.

I(X,Y) = H(X) - H(X \| Y)

위의 정의에 따르면 cycle-consistency를 최소화 하는것은 H(X\|Y)를 최소화 하는 것이고, 이는 mutual information을 높여 X와 Y의 dependancy를 높이는 효과를 준다. 이는 H(X)가 상수이므로 mutual information을 
최대화 하는 것은 H(X|Y)를 최소화 하는 것과 같은 말이다. 본 논문에서는 mutual information을 최대화 했으므로 결국 cycle-gan과 같은 효과를 적은 연산량으로 누린 것을 의미한다. 


### Experiment
단순 베이스라인 비교는 생략한다. 주목해 볼만한 것은 제시한 multilayer와 id loss, patch-based approach가 최종 성능에 어떻게 도움이 되는지를 보자. 

![](/assets/images/2022-04-28-CUT/2.PNG)

위의 표를 정리하자면 다음과 같다.
1. Patch-based가 External, 즉 이미지 한장 한장을 positive, negative sample로 보는 것보다 좋다.
2. multiple layer를 선택해서 다양한 스케일의 feature map을 고려하는 것이 좋다. 
3. identity는 학습에 안정성을 준다. 비록 horse2zebra에서는 best가 아니지만 cityscapes에서는 사용하는 것이 월등히 좋다.
4. 총 패치의 갯수를 몇개로 하는지는 ablation이 되지 않았지만 적게 사용하지는 말자. 










