---
title: "[ECCV2018]MUNIT-Multimodal Unsupervised Image-to-Image Translation"
excerpt: "[ECCV2018]MUNIT-Multimodal Unsupervised Image-to-Image Translation"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2022-05-31T15:04:00-05:00
---

형태를 유지하고 싶으면 CycleGAN, 도메인을 바꾸려면 MUNIT을 써라 라는 가이드가 있다. MUNIT은 CYcleGAN만큼 constraint가 세지 않은 결과를 생성한다. 코드 자체는 어렵지 않아 금방
이해할 수 있다. MUNIT의 핵심가정인 두 이미지의 content가 공유되고 style은 공유되지 않는다는 사실을 염두해두자. [링크](https://github.com/rlawjdghek/Generative_Models/tree/main/GANs/MUNIT) 참조.

# Abstract & Introduction & Related work
Unpaired Image translation, 특히 Cyclic constraint는 deterministic의 성질이 매우강하다 (noise 개념이 들어가지 않는다.). 이는 unimodal의 성질을 띄게하며, 전체적인 분포를 잡는데 어려움이 있다.
MUNIT에서 주장하는 것은, 
1. 특정 이미지가 content space와 style space로 나눌수 있다.
2. 다른 도메인의 이미지는 공통된 content를 갖지만 다른 style을 가져 이미지가 다르게 보인다.
는 두개의 핵심 가정을 내포한다. StyleGAN, Style Transfer에서 눈에 띄게 볼 수 있듯이, 특정 이미지가 스타일을 가지는 것은 널리 알려진 사실이지만, 두개의 다른 도메인이 content를 공유한다는 추상적인
개념을 통하여 새로운 unpaired image translation을 제시한다. 

기존 연구들, 특히 UNIT에서는 MUNIT과 비슷한 개념을 사용한다. 두 도메인이 같은 latent를 공유한다는 사실을 이용하여 MUNIT과 마찬가지로 cyclic constraint없이 Unpaired image translation을 수행한다.
하지만, UNIT은 content, style의 개념이 없기 때문에 unimodal의 성질을 가진다. CycleGAN, UNIT과 같이 기존 unpaired Image translation 모델은 생성 이미지에 대한 diversity가 매우 부족하다는 단점이 있다.
이것들은 모두 훈련 과정에서 두개의 쌍을 이루는 이미지를 어떠한 개입 없이 일대일로 대응시키기 때문이다. 


# Method 
### Assumptions
MUNIT은 위에서 언급한 2가지 중요한 가정을 갖고 시작하기 때문에 문제 정의와 함꼐 가정을 수식으로 정리한다. 먼저, Image-to-Image translation에서 해결하고자 하는 것은 A도메인 이미지($x_1$)가 주어졌을때, B도메인($x_2$)으로의 변형이다. 
하지만 두 도메인의 결합분포 ($p(x_1, x_2)$)는 알지 못한다. A도메인 $\rightarrow$ B도메인, B도메인 $\rightarrow$ A도메인으로 변형을 하는 것은 각각 $p(x_2 | x_1)$, $p(x_1 | x_2)$를 구한다. 단, 이 조건부 분포는 multimodal의 개념으로
조건 $x_2$가 들어왔을 때, $x_1$은 B 도메인이 갖고있는 어떠한 스타일을 여러개 가질 수 있다. 

이제 위의 첫번째 가정, **특정 이미지가 content space와 style space로 나눌수 있다.**활용하기 위해 수식으로 정의해보자. 

도메인 $\mathcal{X}$에 포함되어 있는 data point $x_i$는 content latent code $c \in \mathcal{C}$와 style latent code $s_i \in \mathcal{S}$로 나타낼 수 있다. 우리는 현재 두개의 도메인에서 translation을 수행하므로, $i$는 1,2가 될 수 있다. translation을 위해 두 도메인에서 이미지를 한장 씩 뽑았을 때, 이미지, 콘텐츠, 스타일을 표기하면, $x_1, c, s_1$, $x_2, c, s_2$로 나타낼 수 있다. 주목할 점은 c는 아래첨자로 나뉘지 않았는데, 이유는 content를 공유하고 있다는 가정이 
내포되어 있기 때문이다. 이 때, generator는 content와 style latent code를 받아 이미지를 생성하는 역할을 한다. 즉, $x_1 = G_1(c, s_1)$, $x_2 = G_2(c, s_2)$라 할 수있다. **논문을 쉽게 이해하기 위헤서는 Style은 domain-specific하기 떄문에 Generator와 항상 같은 쌍을 유지한다는 점을 기억하자.**

![](/assets/images/2022-05-28-MUNIT/1.jpg)
### Model
모델이해는 위의 그림으로 끝난다. $x_1$을 도메인 $\mathcal{X}_2$의 어떤 이미지로 변형하려면, 잘 학습된 encoder와 decoder를 사용한다. encoder는 이미지를 style과 content로 나누고, deocder는 style과 content를 결합하여 이미지로 생성한다. 
이 때, encoder와 decoder가 2개의 도메인에서 주어진 $x_1, x_2$만을 이용한다면 기존의 방법론과 같이 일대일 매칭이 될 수 있다. **즉, 여러 스타일을 continuous하게 뽑기 위해서는 우리가 훈련 데이터 포인트만을 훈련하는 것이 아니라 continuous분포에서 뽑은 latent code를 활용해야 한다.** 이 부분을 논문의 문장으로 그대로 쓰자면, "to translate an image $x_1 \in \mathcal{X}_1 to \mathcal{X}_2$, we first extract its content latent code $c_1 = E\_1^c(x\_1)$ and randomly draw a style latent code $s_2$ from the prior distribution $q(s_2) \sim \mathcal{N}(0, \mathbf{I})$."

위의 그림의 파란색 3,4,5,6,7,8번 손실함수를 보자. 이는 bidirectional reconstruction loss로, encoder와 decoder가 서로의 역함수 기능을 수행할 수 있도록 하는 손실 함수이다. 3,4은 우리가 갖고 있는 훈련 데이터 포인트만을 사용하는 손실함수들이다. 이 로스만 있다면 multimodal이 될 수 없고 그냥 autoencoder랑 다를 바가 없어진다. 주목해야 할 점은 reconstruction 이미지 외에 5,6번 손실함수처럼 어떤 prior latent code (q분포) $z_A, z_B$를 뽑고 기존의 이미지에서 뽑은 content와 결합한 것을 생성이미지로 표시한다. 즉, style space는 어떠한 미리 정의된 정규분포를 따르며, 이 정규분포는 continuous distribution이므로 스타일 분포는 data point가 아닌 연속적인 어떤 분포로 나타내 질 수 있다. 우리가 갖고 있는 스타일 분포를 prior분포로 지정한 정규분포와 같게 하기 위해서 5,6번 style loss를 주었고, 더욱이 decoder가 prior와 기존의 content로 만든 이미지의 content를 보존하기 위한 content loss, 7,8번을 주었다. 

두번쨰로 3,4,5,6,7,8로 어떤 continuous 분포이 포인트 들과 우리가 가진 style data point를 맞춰주었다면, 생성된 이미지와 target domain을 맞춰주기 위한 adversarial loss를 추가해준다 (빨간색 글씨). 

번외로, 논문에서는 다양성을 줄이지만 quality는 높일 수 있는 추가적인 손실 함수를 제시한다. 공식 코드에서는 사용하지 않는다. 위의 부분까지가 최종 손실함수라 생각하자.
마지막으로 논문 method에는 style-augmented cycle consistency라고 되어있는 부분이 있다. 그림에서는 9,10번에 해당하는 손실함수이다. 이는 cyclegan의 reconstruction loss의 역할을 하며 constraint가 너무세서 diversity가 죽을 수 있다. 

# Theoretical Analysis
4가지의 분석을 제시하였다. 각 proposition의 결론만 간단히 적어보자.
1. reconstruction loss와 adversarial loss가 수렴하면, encoder와 decoder는 역함수 관계이고, 생성 이미지 분포 $p(x_{1\rightarrow 2})$는 $p(x_2)$가 된다. 즉, MUNIT을 완벽히 학습한다면 생서이미지는 목표 도메인의 분포를 따를 수 있다. 
2. MUNIT이 완벽히 수렴한다면, $p(c_1) = p(c_2), p(s_1) = q(s_1), p(s_2) = q(s_2)$, 즉, 두 도메인의 content의 분포는 같고, 각각 도메인의 style분포와 prior분포가 같아진다. 즉, 우리가 A도메인 이미지의 content와 B도메인의 style을 합성하고 싶을 때,
B도메인에 있는 style만 사용할 수 있는 것이 아니라, 연속 분포 $q(s_2)$로부터 무수히 많은 style을 뽑을 수 있다. 
3. MUNIT이 완벽히 수렴한다면, joint distribution은 같다. 즉, $p(x_1, x_{1 \rightarrow 2}) = p(x_2, x_{2 \rightarrow 1})$.
4. MUNIT이 완벽히 수렴한다면, 각 도메인의 Generator (encoder + decoder)는 역함수 관계이다. 

# Experiments
결과를 다 볼 필요는 없고 여기서 언급한 모델 아키텍쳐의 중요한 팁들만 나열하자.
1. Content Encoder는 instance normalization을 사용한다.
2. **Style Encoder는 instance normalization을 사용하지 않는다.** 이는 매우 중요한데, 스타일은 주로 mean과 variance로 표현되는데 normalization은 이를 없앤다. 
3. Decoder는 style과 content를 합치기 위하여 AdaIN을 사용하였다. 
4. Discriminator는 LSGAN을 사용하였다. 
5. VGG loss는 이미지 다양성에 큰 제한을 두어 VGG loss를 사용하면 이미지의 다양성을 없앤다. MUNIT에서는 VGG를 약간 변형하였다. domain-invariant perceptual loss를 소개하면서 input을 reference로 삼고, VGG feature에 IN을 취한다. 즉, 기존의 mean과 variance를 없앨 수 있다. 
6. Ablation에서 MUNIT에 style reconstruction (그림에서 5,6번 손실함수)를 없앴더니, mode collapse가 일어났다. 즉, prior분포와 style분포를 맞춰주는 힘이 줄어들어 diversity가 낮아진다. 
