---
title:  "[NIPS2018]Improved Training of Wasserstein GANs"
excerpt: "[NIPS2018]Improved Training of Wasserstein GANs"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2021-11-05T15:04:00-05:00
---

제목에서도 볼 수 있듯이, WGAN의 단점을 보완하기 위해서 제안된 논문이다. 따라서 WGAN을 사용하기 보다는 WGAN-GP를 기본적으로 사용한다고 생각하자. 
나중에 간단하게 생성에 관련된 application을 다룰 떄 자주 사용할 듯 하다. 논문을 작성할때에는 의존도가 증가하기 떄문에 WGAN-GP떄문인지 새로운 방법론 때문인지 
구별하기가 어렵기 떄문에 논문에서는 잘 안쓰는 것이 좋을듯 하다. 

# Abstract & Introduction
WGAN은 GAN의 원초적인 문제인 수렴의 안정성을 위하여 개발된 방법이다. 하지만 WGAN 또한 여전히 converge가 실패하는 모습을 보인다. (WGAN에서는 언급하지 않았다.)
WGAN의 핵심 아이디어인 Earth-Mover distance의 제약조건 Lipschitz조건을 만족하기 위해서 WGAN에서는 critic (Discriminator)를 일정한 상수 c로 weight
clipping 하였다. 이 논문에서는 weight clipping의 단점들을 소개하고, 이를 개선하는 방법을 제안한다. 

# Background
### GAN
GAN의 G는 노이즈를 입력으로 받아 주어진 훈련 데이터 분포에 맞추는 함수이다. D는 G가 만든 이미지를 훈련 데이터와 구별하는 역할을 하고, G는 D를 속이기 위해 더
실제같은 훈련 데이터를 만든다. 하지만 GAN을 다루다 보면 가장 보편적으로 발생하는 경우가 있다: D가 G가 업데이트 되기 전에, 최적으로 훈련된다면, GAN의 minmax 
손실 함수는 실제 훈련 데이터와 생성 데이터의 분포에 대한 Jensen-Shannon divergence를 최소화 하게 된다. 하지만 이는 D가 이미 훈련되었기 때문에 vanishing 
gradient문제를 일으킬 수 있다. 또한 보통의 경우 D를 훈련하는 것이 쉽기 때문에 일반적인 코드에서 G와 D를 번갈아 가면서 훈련하더라도, D가 먼저 수렴할 가능성이 크다. 
이러한 경우 다시 vanishing gradient나 mode collapse가 일어날 수 있다. 

### WGAN
WGAN에서 D를 critic이라 명명한 것은, WGAN 손실함수에서 D는 더이상 분류기가 아니기 때문이다. D는 단순히 G를 도와주는 역할을 하기 때문에 분류기라는 이름이 아닌
비평가라고 할 수 있다. WGAN논문에서 볼 수 있듯이 WGAN의 손실 함수값은 generator의 생성 능력과 관계가 있다는 큰 장점이 있다. \
하지만 WGAN은 Lipschitz조건을 만족하기 위하여 weight clipping이라는 필수적이지만 큰 단점을 갖고 있다. weight clipping은 critic을 매우 단순하게 만들어 최적화를 어렵게 한다. 
또한 weight constraint와 loss function 사이의 상호작용이 또한 최적화 과정을 어렵게 만든다. 
![](/assets/images/2021-11-05-WGAN_GP/1.JPG)
위의 왼쪽 그림부터 보면, 실제 toy distribution을 생성하는 GAN을 만들었을 때, WGAN은 WGAN-GP보다 단순하게 만드는 것을 볼 수 있다.\
오른쪽 그림은, WGAN-GP가 exploding되거나 vanishing되는 것 없이 안정적으로 학습되는 것을 볼 수 있다.


### Gradient penalty
논문에서는 핵심 정리 2가지를 증명하였다. 
![](/assets/images/2021-11-05-WGAN_GP/2.JPG)
결론은 최적의 critic은 훈련데이터와 생성데이터 분포에서 gradient norm을 1로 가진다. 이를 이용하여 gradient penalty를 적용하여 critic의 gradient norm을 1로 맞춰준다. 
![](/assets/images/2021-11-05-WGAN_GP/3.JPG)
람다는 10을 사용하였다. 

이전 GAN모델에서는 학습의 안정화를 위하여 batch normalization을 사용하였다. 하지만 batch normalization은 한개의 입력에 대한 한개의 출력이 아닌 
batch to batch의 결과를 만들어 내가 때문에 노이즈로와 생성 이미지에 대한 매핑을 모호하게 만든다. 이에 반해, WGAN-GP는 더이상 batch normalization을 사용하지 않아도
학습이 안정적으로 되기 때문에 layer normalization을 사용하는 것을 추천하였다.

# Experiment 
![](/assets/images/2021-11-05-WGAN_GP/4.JPG)
WGAN보다 안정성이 더 좋다는 것을 증명하기 위해 activation function, model capacity, batch normalization, filter channel을 변경하면서 모델의 경우의 수를 늘렸다.
위의 표는 이러한 모델들 중에서 inception score의 threshold를 높이면서 이 스코어를 만족하는 GAN조합의 갯수를 골랐다. Only WGAN-GP를 보면 알 수 있듯이, WGAN-GP에서만 
수렴하는 모델이 대다수이다. 즉, WGAN-GP의 학습 안정성이 더 높다.

![](/assets/images/2021-11-05-WGAN_GP/5.JPG)
또한 GAN의 학습 정도 여부를 확인 할 수 있는 것은 매우 큰 장점이므로 WGAN과 마찬가지로 학습 진행률과 손실함수의 값의 관계가 뚜렷이 나타난다. 







