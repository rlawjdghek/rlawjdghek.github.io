---
title: "[CVPR2020]StarGAN v2: Diverse Image Synthesis for Multiple Domains"
excerpt: "[CVPR2020]StarGAN v2: Diverse Image Synthesis for Multiple Domains"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2022-06-14T15:04:00-05:00
---

논문이 굉장히 깔끔하고 명확하다. 이해하기도 쉬워서 구현이 크게 어렵지 않다. 하지만 FID, LPIPS등의 성능 재현방법이 논문에 자세히 나와있고, 공식코드에서도 공개하였으므로 좋은 공부가 됨.
코드는 [링크](https://github.com/rlawjdghek/Generative_Models/tree/main/GANs/StarGANv2) 참조.

스케치를 보기전 주의해야할 점이 있다. A, B는 I2I translation에서 도메인을 표시 할 때, 내가 많이 사용하는 표기법인데, 특히 content-style을 나눌때에는 A,B가 스타일을 나타낸다. 만약 realA와 styleB가 합쳐져서 A의 content와 B의 style을 가진
이미지가 생성된다면, 생성된 이미지는 B의 스타일을 갖고 있으므로 geneB라고 표시된다. StarGANv2에서는 여러 도메인을 한번에 사용하므로 A,B,C... 여러 스타일을 가질수 있지만, 결국 학습때에는  
2개의 도메인에서(도메인이 같아도 됨) 뽑힌 이미지가 realA, realB로 되기 때문에 A와 B는 그때 그때 달라지는 도메인 네이밍이다. 

첫번째 그림은 StarGANv2에서 사용하는 4개의 네트워크의 구조이다. Generator를 제외한 나머지 Discriminator, Mapping network, Style Encoder는 마지막에 레이블을 사용하여 최종 출력을 결정한다.
두번째 그림은 전체 학습과정을 나타낸다. 내 코드와 같이 update_D와 update_GEF를 따라 그렸다. 1과 2의 차이는 입력으로 z가 들어가느냐, real_B가 들어가는냐 차이.
![](/assets/images/2022-06-14-StarGANv2/1.jpg)
![](/assets/images/2022-06-14-StarGANv2/2.jpg)

# Abstract & Introduction
StarGANv1 도메인 레이블을 활용하여 여러 스타일을 조건으로 줄 수 있다. StarGANv1이 좀 더 구체적인 스타일을 주었다면, StarGANv2는 조금 더 넓은 범위의 도메인 (개, 고양이 등)에서 
전체적인 스타일을 입힌다. StarGANv1처럼 얼굴 부위별로 레이블을 주는 것은 굉장히 비용이 큰 작업이므로 현실적으로 StarGANv1을 사용하는 것은 어렵다. DRIT++, MUNIT등의 Unpaired-I2I Translation
모델들이 등장하면서, multimodal 이미지 생성이 주력으로 되고, 추가적으로 multi-domain까지 하나의 모델이 학습할 수 있었다. StarGANv2는 앞선 multimodal, multi-domain 생성모델이지만, 훨씬 직관적이고,
한개의 Generator와 Discriminator로 구현되었다. 이는 discriminator가 도메인 갯수에 맞는 예측을 하고, 미리 주어진 도메인 레이블을 통하여 각 도메인의 스타일을 합성할수 있게 하였다. 또한 multimodal을 
구현하기 위하여 cLR-GAN에서와 같이 $z \rightarrow gene \rightarrow \hat{z}$를 사용하였다. 

StarGANv2도 여타 Unpaired I2I translation GAN들처럼 content - style disentangle을 사용한다. StarGANv1에서 언급하였듯이, K개의 도메인을 학습하기 위해서는 CycleGAN과 같은 옛날 모델들은 
K(K-1)개의 모델이 필요한 반면, StarGANv2는 classifier부분의 에측하는 갯수만 늘리면 되므로 아주 작은 추가 메모리만 요구된다. 

# StarGANv2
### Proposed framework
StarGANv2의 큰 장점중 하나는 여러 도메인을 하나의 모델로 학습하는데 모든 모듈이 1개씩 필요하다는 것이다. 스케치 그림을 볼 때 Generator의 forward에서 cycle loss가 realA와 cycleA에 있다. 
즉, realB는 styleB를 뽑기위한 도구로만 작용하고, 어차피 각 도메인의 모든 이미지가 realA와 realB가 되므로 크게 상관이 없다.

StarGANv2에서는 4개의 모듈을 사용하고 각 모듈의 입력, 출력, 역할을 정리하면 아래와 같다.
1. Generator : 입력으로 realA와 styleB를 받고 A의 content, B의 스타일을 가진 geneB를 생성한다. 특히 여러 도메인에 대하여 이 generator 모두 처리할 수 있다. 또한 generator만
레이블을 필요로 하지 않아 마지막 레이어가 tanh로 끝남.
2. Discriminator : 어떤 이미지와 이미지의 도메인 레이블을 받아 real / gene를 가린다. 특히, 마지막 레이어는 첫번째 스케치에서 볼 수 있듯이 도메인 갯수만큼 예측 확률로 분류하기 때문에 받은 레이블에 해당하는
확률값이 real / gene를 가리는 값이 된다. 즉, [BS x n_domain]이 출력이 되고 각 이미지마다 1개의 값을 뽑아서 BCE에서 사용되는 pred값처럼 [BS x 1]이 사용된다.
3. Mapping Network : stylegan에서의 mapping network와 비슷한 역할을 하지만 다른점은 마지막 레이어가 n개의 도메인으로 가지치기 되어 각각을 예측한다. 스케치를 보면,
구현상의 편의를 위해서 모든 aux classifier를 통과한다음 concat후에 레이블을 활용해서 최종 style을 뽑는다.
4. Style Encoder : real / gene이미지에서 스타일을 뽑는 역할을 한다. 이것도 마찬가지로 도메인별 정보를 따로 기억하게 하기 위해 도메인 갯수만큼 네트워크를 가지고, 마지막에 합친다. 그 후 mapping network랑
똑같이 레이블을 활용해서 레이블에 해당하는 도메인을 뽑는다. 

### loss functions 
논문에서 다루는 loss function은 4개이다.
1. Adversarial loss : 일반적인 GAN loss 구현에서도 일반적인 BCE를 사용했다.
2. Style reconstruction : 스케치를 보면 알수 있듯이, styleB와 style_gene_B를 이는 StyleEncoder를의 출력과 latent z를 맞춰준다.
3. Style diversification : MSGAN에서 나온 mode seeking loss를 약간 변형했다. MSGAN에서는 분자에 output, 분모에 z의 차를 주었지만, StarGANv2에서는 분모를 없앴다. z가 작으면 분모가 너무 작아져서 
결과적으로 값이 너무 커져 학습이 불안정해진다. 또한 ds loss에 붙는 hyper-parameter의 값도 2에서 0으로 줄어드는데, ds loss가 나중에는 학습을 불안정하게 해서 그런듯 하다. 
4. Preserving source characteristics : 스케치에서 보면 cycleA와 realA를 맞추는 cycle loss이다. 
5. 논문에는 없지만 마지막으로 r1 reg loss가 있는데 Discriminator의 gradient합을 regularization해준다.

# Experiment 
### AFHQ
StarGANv2는 AFHQ데이터셋을 배포하였다. multi-domain을 증명하기 위하여 AFHQ에는 약 5000장씩 개, 고양이, 야생동물의 카테고리로 훈련 데이터셋을 구성하였고, 정확히 500장씩 검증셋을 만들었다. 

### FID
FID는 훈련 데이터와 생성 이미지간의 거리를 측정하며, 생성 이미지의 퀼리티를 측정한다. StarGANv2에서 구현한 FID 측정을 설명한다.
1. 테스트 데이터셋의 한장의 이미지에 대하여 latent-guided / reference-guided로 10장을 생성한다. 모든 도메인에 대하여 측정해야한다. AFHQ면 전체 과정을 도메인마다
3번 반복하여 평균 구한다. 
2. 훈련 데이터셋의 target 도메인과 생성 이미지의 모두 비교하여 FID의 평균값을 계산한다. AFHQ기준 테스트셋에 도메인당 500장씩 있으므로 총 5000장의 이미지가 나온다.
3. 모든 도메인에 대하여 측정해야 하므로, AFHQ이면 1,2번 과정을 고양이, 개, 야생동물에 대하여 모두 평균해야한다.

### LPIPS
1. 한장의 이미지에 대하여 latent-guided / reference-guided로 10장 생성한다.
2. 10장에서 45개의 조합으로 2장씩 뽑아 모두 LPIPS를 구하고 평균값을 계산한다.
3. 이를 모든 도메인의 테스트 이미지에 대하여 수행한다.

### Discussion
성능은 MUNIT, DRIT, MSGAN과 비교하였을때 더 낮은 FID, 높은 LPIPS를 보인다. StarGANv2에서 제시한 방법론들에 대하여 분석 후 정리하면 아래와 같다.
1. mapping network는 branch network를 통하여 latent를 더 도메인에 맞는 style로 변환한다. 만약 mapping network를 사용하지 않으면 diversity가 죽는다. latent그대로는 multi-domain의 
스타일을 담는 capacity가 부족한듯.
2. mapping network를 사용하는 것이 style을 정제하는 데 큰 도움을 준다.
3. StarGANv2에서 제시한 단 한개의 generator는 모든 도메인의 이미지를 다 볼 수 있다는 큰 장점이 있다. 스타일을 나누기 위해서 branch network가 있는 mapping network를 도입했고, generator는 
모든 도메인의 이미지를 학습하므로 기본적으로 깔려있는 공통된 content를 다른 모델들 보다 더 많이 볼 수 있다는 것이 장점으로 작용한다. 






