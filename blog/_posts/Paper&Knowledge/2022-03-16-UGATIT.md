---
title: "[ICLR2020]U-GAT-IT: Unsupervised Generative Attentional networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation"
excerpt: "[ICLR2020]U-GAT-IT: Unsupervised Generative Attentional networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2022-03-16T15:04:00-05:00
---

CycleGAN의 구조와 비슷하지만, 중간에 CAM을 기반으로 하여 어텐션을 사용하였다. 논문 supplementary에 나온대로 구현하였다. [링크](https://github.com/rlawjdghek/Generative_Models/tree/main/GANs/U-GAT-IT)에서 확인. 
<br/>
G와 D의 전체적인 모델 구조는 아래 그림을 참고하자. 각각 모듈의 자세한 내용은 코드와 논문의 supplementary 참고. 코드에 모듈마다 (e.x. Decoder Bottleneck) 주석으로 구분해 두었음.
<br/>
![](/assets/images/2022-03-16-UGATIT/1.jpg)
![](/assets/images/2022-03-16-UGATIT/2.jpg)

### Abstract & Introduction
특별한 내용은 없다. 기존 모델들은 뛰어나게 image-to-image를 잘하지 못한다는 것. 예를 들어 CycleGAN은 형태를 못바꾼다는 내용등. 주목할 만한 것들은 아래와 같다.
1. CAM을 활용한 attention module
2. Adaptive Layer-Instance Normalization

### Method 
#### AdaLIN
논문에서는 다 쓰는 것처럼 얘기하지만 실제 들어가는 것은 Generator의 Decoder bottleneck (decoder에는 feature map크기를 유지하는 bottleneck단과 upsampling 단으로 나뉜다)에 들어간다. 논문의 식을 그대로 구현한 코드를 보자.

\begin{equation}
AdaLIN(a, \gamma, \beta) = \gamma \cdot (\rho + a_I + (1-\rho) \cdot a_L) + \beta   \\\\\
a_I = \frac{a - \mu_I}{\sqrt{\sigma_I^2 + \epsilon}}, a_L = \frac{a-\mu_L}{\sqrt{\sigma_L^2 + \epsilon}} \\\\\
\rho \leftarrow clip_{[0,1]} (\rho - \Delta\rho)
\end{equation}
맨 위에서 부터 보면, $a_I$는 input feature map의 instance normalization의 결과이다. IN은 feature 한개의 channel마다의 평균과 분산을 이용해서 계산한다. 즉, HxW개의 숫자를 이용해서 평균, 분산을 계산후 normalization. $a_L$은 한개의 배치안에서 CxHxW의 숫자를 이용하여 평균과 분산을 계산. $\rho$는 가중치인데 이것도 학습을 통해서 진행되고, 중간에 튀지 않도록 cliping을 해준다. 마지막으로 $\gamma, \beta$는 레이어를 통해서 구해지는데 가장 위에있는 구조 그림을 보면 오른쪽 하단에 gamma_FC와 beta_FC를 통해서 구해지는 것을 볼 수 있다. 두 개의 shape는 [BSx256]으로 decoder bottleneck에 들어갈 때 feature map과 channel wise로 곱해진다. 코드는 아래와 같다.

구현에서 instance, layer normalization을 하기 위해 파이토치 구현 대신 순수 계산으로 구현하였다 (dim매개변수 주목)
```python
class AdaLIN(nn.Module):
    def __init__(self, dim, eps=1e-5):
        super().__init__()
        self.eps = eps
        self.rho = nn.Parameter(torch.Tensor(1, dim, 1, 1).data.fill_(0.9))
    def forward(self, x, gamma, beta):
        inst_var, inst_mean = torch.var_mean(x, dim=[2,3], keepdim=True)
        inst_out = (x - inst_mean) / torch.sqrt(inst_var + self.eps)
        layer_var, layer_mean = torch.var_mean(x, dim=[1,2,3], keepdim=True)
        layer_out = (x - layer_mean) / torch.sqrt(layer_var + self.eps)
        out = gamma.unsqueeze(2).unsqueeze(3) * (self.rho * inst_out + (1 - self.rho) * layer_out) + beta.unsqueeze(2).unsqueeze(3)
        return out
```

논문에서는 StyleGAN의 AdaIN은 content를 전달하는 능력이 강하다 (IN자체가 디코더에서 쓰일때 통계량을 굉장히 국소적으로 보기 때문에 전체적인 구조를 잘 전달할 수 있다. ). AdaIN 기법 대신 WCT기법이 optimal이지만 계산이 많이 소모되므로 AdaIN을 사용한다고 한다.
<br/>
Layer normalization은 content를 전달하지 못한다고 한다. 왜냐하면 target의 도메인과 가까운 decoder에서 볼 때, 통계량을 전체 feature에서 보게 되므로 채널별로 존재하는 content 대신 texture를 볼 수 있다.

#### CAM based atttention
이건 논문에서 그림으로도 잘 설명하지 않았다. auxilary classifier generator와 discriminator에 두고 마지막 fully connect layer의 가중치 (shape은 [1 x 256])을 통과하기 위해서 GAP (global average pooling)와 GMP (global max pooling)를 사용한다. 통과한 뒤에 두 FC layer의 가중치를 encoder를 통과한 x에 채널별로 곱해준다. 그 뒤에 GAP, GMP가 곱해진 feature 2개를 concat하는데 맨 위의 그림을 보면 이해하기 쉬움. 그러고 채널을 줄이기 위해 conv1x1으로 concat된 feature를 반으로 줄인다. 

```python
# generator의 forward 함수
    def forward(self, x):
        BS = x.shape[0]
        x = self.encoder(x)

        # global average pooling을 통한 attention
        gap = F.adaptive_avg_pool2d(x, 1)
        gap_logit = self.gap_fc(gap.reshape(BS, -1))
        gap_weight = list(self.gap_fc.parameters())[0]  # [1 x 256]
        gap = x * gap_weight.unsqueeze(2).unsqueeze(3)

        # global max pooling을 통한 attention
        gmp = F.adaptive_max_pool2d(x, 1)
        gmp_logit = self.gmp_fc(gmp.reshape(BS, -1))
        gmp_weight = list(self.gmp_fc.parameters())[0]  # [1 x 256]
        gmp = x * gmp_weight.unsqueeze(2).unsqueeze(3)
        
        # concat하고 1x1 conv 통과
        cam_logit = torch.cat([gap_logit, gmp_logit], 1)
        x = torch.cat([gap, gmp] ,1)
        x = self.conv1x1(x)
        x = self.relu(x)  # [BS x 256 x 64 x 64]

        heatmap = torch.sum(x, dim=1, keepdim=True)  # [BS x 1 x 64 x 64]
        if self.light:  # 이 다음 x에서 FC를 gap로 씀
            x_ = F.adaptive_avg_pool2d(x, 1)
            x_ = self.FC(x_.reshape(BS, -1))
        else:
            x_ = self.FC(x.reshape(BS, -1))
        # gamma, beta를 위한 x를 x_로 따로 뺌.
        gamma, beta = self.gamma_fc(x_), self.beta_fc(x_)
        for dec_bottle in self.decoder_bottleneck:
            x = dec_bottle(x, gamma, beta)
        out = self.decoder_upsample(x)
        return out, cam_logit, heatmap
```

#### Loss
loss함수는 총 4가지, adversarial loss, cycle loss, identity loss, CAM loss를 사용함. 모델에서 forward 함수의 return 값들을 보면 output, cam_logit, heatmap이 있는데 heatmap은 훈련에서는 사용하지 않고 시각화 할 때만 사용한다. 
1. adversarial loss : Discriminator에서 나온 output과 cam logit으로 사용됨. 
2. Cycle Loss : generator를 사용한 reconstruction loss. G_AB에 real_A가 들어가면 real_B가 나와야됨. 
3. Identity loss : 이것도 마찬가지로 generator를 사용해서 G_AB에 real_A가 들어가면 real_A가 나와야됨.
4. CAM loss : generator만 해당됨. forward의 cam logit (GAP와 GMP의 fc layer를 통과한 logit)을 이용해서 BCEloss로 들어간다. G_AB를 예를 들면, G_AB의 aux classifier가 real_A를 보면 진짜로 예측해야하고, real_B를 보면 가짜로 예측해야한다. 즉, 말 그대로 G의 보조이기 때문에 G와 같은 목표를 갖고 있다. 

로스 함수 계산에서 주의할 것은
1. A 도메인 vs B 도메인 (CycleGAN)
2. Discrimination의 global vs local
을 다 고려해야 하기 때문에 좀 길다. (코드 참조)

### Experiments
결과는 CycleGAN, MUNIT, DRIT등이랑 비교할 떄 제일 좋게 나왔다. 주목할 만한 점은 instance와 layer norm의 분석.

![](/assets/images/2022-03-16-UGATIT/3.PNG)

위의 그림은
(a) source <br/>
(b) U-GAT-IT result <br/>
(c) only IN <br/>
(d) only LN <br/>
(e) only AdaIN <br/>
(f) GN <br/>
결론적으로는 LN만 사용하면 d에서 볼수 있듯이 target (애니메이션)의 특징은 잘 드러내고, c에서 볼 수 있듯이 IN만 사용할 경우 source (실사)잘 드러낸다. 논문에 이유는 나와있지 않는데 개인적인 생각으로는 **IN은 더 세밀하므로 input의 통계값을 더 잘 보존하고, LN은 전체적인 통계량, target domain을 표현**한다고 생각됨.