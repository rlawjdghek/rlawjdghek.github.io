---
title: "[NeurIPS2019]Time-series Generative Adversarial Networks"
excerpt: "[NeurIPS2019]Time-series Generative Adversarial Networks"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2022-05-20T15:04:00-05:00
---

2022년 5월 19일 랩세미나로 발표했던 논문이다. 피피티는 다음 [링크](https://docs.google.com/presentation/d/1l98QrIiGgzl_Sw2a-44SKpQSjvsJOUQg/edit?usp=sharing&ouid=115578388967484997653&rtpof=true&sd=true) 참조.
이전 시계열 GAN을 본적이 없어서 이 논문이 얼마나 발전되었는지는 파악되지 않았으나, 논문이 굉장히 깔끔하고 명확하게 쓰여졌다. 이해하기도 쉽고 크게 어려운 수식은 없다. 직접 구현은 하지 않았다. 단, 파이토치로 잘 되어있는 코드가 있으니
참고하자 [](https://github.com/birdx0810/timegan-pytorch).

# Abtract & Introduction
기존의 시계열 생성 모델은 크게 2가지로 나뉜다.
1. GAN-based 생성모델.
2. Seq2Seq 생성모델.

먼저, GAN 기반의 생성 모델은 컴퓨터 비전에서 처음 등장했기 때문에 시간적인 정보를 사용하지 않았다. 이미지는 공간적정보가 있지만 그 관계를 RNN과 같이 시간적인 정보를 사용하지 않는다. 

Seq2Seq 생성모델은 Supervise train이기 때문에 deterministic하다는 특징이 있다. 따라서 훈련 데이터 이외의 데이터를 생성하기 힘들다. 논문에서 비슷한 의미의 단어들이 굉장히 다양하게 등장하는데, 크게 위의 2개의 모델과 같다.
GAN == unsupervision == non-deterministic == open-loop <br/>
Temporal correlations == supervision == inherently deterministic == closed-loop <br/>
이 논문은 unsupervised와 supervised를 동시에 학습하는 GAN 모델을 제시한다. unsupervised는 학습이 어려운 대신 새로운 훈련 데이터 분포를 제공하고, supervised는 학습이 쉬운대신 뻔한 데이터를 생성한다. 두 개의 장점을 모두 섞었다. 

논문의 contribution을 정리하면 아래와 같이 3가지로 나눌 수 있다.
1. Unsupervised loss와 Supervised loss를 동시에 사용하였다.
2. Embedding network를 통하여 generator가 훈련 데이터의 latent representation을 생성하는 방법을 제시했다. 실험에서 이렇게 embedding을 생성한 뒤 실제 데이터로 recover하는 것이 성능이 더 좋았다.
3. 시계열 데이터에는 거의 변하지 않는 static feature (예를 들어, 나이, 이름 등)과 시간마다 변하는 temporal feature를 구분하여 훈련한다. 

# Method 
먼저 Closed-loop과 Open-loop를 알아보자.
### Closed-loop
Autoregressive model처럼 이전의 데이터들의 선형 조합으로 새로운 데이터를 얻는 예가 있다. 즉, 과거의 데이터들로 부터 현재 데이터를 새롭게 얻는것인데, 이는 학습은 쉽지만 기존의 데이터들을 조합한 것이기 때문에 보여지는 훈련 데이터보다 
크게 다르지 않은 데이터를 생성한다. 수식으로 쓰면 아래와 같다.
\begin{equation}
\prod_{t} p(x_t | x_{1:t-1})
\end{equation}
위의 수식에서 product 기호를 빼고 자세히 살펴보면 과거의 시간 1에서 t-1까지 주어졌을때 현재 데이터 t를 예측한다. 식을 풀어보면 결국 모든 
시간의 joint distribution을 구하는 것과 같은데 GAN과 달리 한번에 모든 시간을 구하지 않기 때문에 더 쉽게 학습이 된다. 아래 그림은 대표적인 Closed-loop인 Markov Chain이다.

![](/assets/images/2022-05-20-TimeGAN/1.JPG)

### Open-loop
open loop는 GAN을 생각하면 된다. GAN도 훈련 데이터의 분포를 아는 것이지만 closed-loop와 달리 조건이 없으므로 훈련 데이터의 분포를 한번에 생성한다고 볼 수 있다.
\begin{eqaution}
p(x_{1:T})
\end{equation}
하지만 위에서 언급하였듯이 데이터를 한번에 얻을 수 있고 훈련데이터에 국한되지 않는 데이터를 생성할 수 있는 장점이 있지만, 시계열 데이터가 갖고 있는 좋은 정보인 시간적인 상관관계를 고려하지 않을 수 있다. (논문에서도 학습하지 않는다고는
표현하지 않음.) 

### Problem Formulation
기호가 많이 등장하기 때문에 정리해둠.
![](/assets/images/2022-05-20-TimeGAN/2.JPG)

이 논문에서는 Unsupervised와 Supervised loss를 둘 다 학습하기 때문에 손실 함수를 크게 2가지로 표현할 수 있다. 
\begin{equation}
Minimize(D(\hat{p}(S, X\_{1:T}), p(S, X\_{1:T})))
\end{equation}
Unsupervised loss이다. D는 두 확률 분포를 나타내는 함수인데, 만약 D가 JS Divergence라면 GAN loss와 완전히 같다. 

\begin{equation}
Minimize(D(\hat{p}(X\_t | S, X\_{1:t-1}), p(X\_t | S, X\_{1:t-1})))
\end{equation}
Supervised loss이다. Unsupervised loss에서 과거 변수들이 조건으로 들어갔다. 

### Model Architecture
이 논문에서 제시하는 모델은 크게 4가지로 구성되어있다.
1. Embedding Layer : $e_s, e_x$
2. Recovery Layer : $r_s, r_x$
3. Generator : $g_s, g_x$
4. Discriminator : $d_s, d_x$
s와 x로 나뉜것은 static, temporal 두 개의 feature를 각각 다뤄야 하기 때문이다. 

앞으로 이 소단원에서 모델을 3가지 (논문, Supple., 코드) 버전으로 보여준다.
![](/assets/images/2022-05-20-TimeGAN/3.JPG)
![](/assets/images/2022-05-20-TimeGAN/4.JPG)
![](/assets/images/2022-05-20-TimeGAN/5.JPG)
그냥 따라가면 쉽게 이해할 수 있다. 

손실함수까지 표현하면 다음과 같다. 이는 Supple.에 있는 알고리즘대로 나타낸 것이다. 실제 코드는 더 아래에 있는데, 더 자세히 나타내었다.
![](/assets/images/2022-05-20-TimeGAN/6.JPG)

코드대로 따라가면 아래와 같다. 코드는 총 5단계로 학습하는데,
1. autoencoder
2. supervisor
3. generator
4. autoencoder
5. discriminator
로 된다. 

![](/assets/images/2022-05-20-TimeGAN/7.JPG)
![](/assets/images/2022-05-20-TimeGAN/8.JPG)
![](/assets/images/2022-05-20-TimeGAN/9.JPG)
![](/assets/images/2022-05-20-TimeGAN/10.JPG)
![](/assets/images/2022-05-20-TimeGAN/11.JPG)


# Experimental Results
결과는 정성적 평가와 정량적 평가를 사용했는데, 컴퓨터 비전에서의 GAN과는 다른 재밌는 metric을 사용하였다

### Qualitative Results
먼저 정성정 평가로는 T-SNE를 사용하였다. 실제 데이터와 생성 데이터를 2차원에 표현하여 분포를 비교한다. 잘 섞여있으면 좋은 데이터이다.
![](/assets/images/2022-05-20-TimeGAN/12.JPG)
위의 그림을 보면 다른 베이스라인들은 빨간(실제) 점들과 파란(생성) 점들이 클러스터링 될 수 있는 것을 볼 수 있다. 

### Quantitative Results.
정량적 평가는 Discriminative Score, Predictive Score2가지를 사용하였다. 

먼저 Discriminative Score는 
1. real과 gene를 분류하는 모델을 만든 뒤
2. 예측하여 정확도를 계산한다. 
![](/assets/images/2022-05-20-TimeGAN/13.JPG)
 
Predictive Score는
1. gene데이터로 과거의 데이터로부터 현재 데이터를 예측하는 모델을 훈련한다.
2. 실제 데이터로 테스트한 다음 실제 데이터와 MAE를 구한다.

두 메트릭 모두 낮을수록 좋은 생성모델이라 할 수 있다.
















