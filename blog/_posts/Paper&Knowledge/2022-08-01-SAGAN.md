---
title: "[PMLR2019]Self-Attention Generative Adversarial Networks"
excerpt: "[PMLR2019]Self-Attention Generative Adversarial Networks"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2022-08-01T15:04:00-05:00
---

제목만 보면 Self-Attention을 GAN에 적용한 것이 큰 contribution이라 생각될 수 있으나, 또한 중요한 것은 spectral normalization과 G와 D의 learning rate의 imbalance를 활용하였다는 사실.
코드는 [링크](https://github.com/rlawjdghek/GANs/tree/master/SAGAN) 참조.

![](/assets/images/2022-08-01-SAGAN/4.jpg)
# Introduction
Convolution을 활용하는 GAN들은 모두 convolution 연산의 한계점인 local receptive field의 제한을 받고 있다. 예를 들어, 개 이미지에서 개의 머리와 다리 부분이 이미지에서 멀리 떨어져 있을 때, z로부터 convolution 연산으로 이미지를 생성 할 떄, 이 두 부위를 동시에 인지할 수 있는 커널은 feature map이 작을 떄, 즉 receptive field가 클 때 존재할 수 있다. 따라서 receptive field가 클 때 많은 레이어들을 쌓아서 이미지의 long-term dependency가 해결될 수 있으면 좋겠지만, 다음과 같은 3가지의 한계점이 존재한다.
1. 모델이 충분히 크지 않아 표현력이 부족할 수 있다. 
2. 최적화를 할 수 없는 multiple layer가 될 수 있다. 즉, 모델을 커스터마이징 하기 위한 비용이 많이 든다.
3. 통계적으로 불안정한 모델이 될 가능성이 크다. 특히, unseen input에 대해서 취약할 수 있다.

따라서 본 논문에서는 같은 깊이의 레이어를 가진 모델에서 self-attention을 활용하여 long-term dependency를 해결하고, ImageNet에서 SOTA성능의 FID와 IS를 보이는 모델을 제안한다. 

# Method 
Method에서 기억해야할 것은 딱 3가지이다.
1. self-attention module을 활용하였다.
2. 원래에는 D에만 적용되던 Spectral Normalization을 G와 D에 모두 적용하였다.
3. TTUR이라는 G와 D에 imbalance한 learning rate를 사용하는 방법을 적용하였다. 
먼저 1번부터 보면 아래 그림과 같다.
![](/assets/images/2022-08-01-SAGAN/1.PNG)
conv1x1이 총 4개가 있고, transformer의 qkv module과 굉장히 흡사하다. 논문에 수식으로 정리한 것이 있는데 이것도 vit와 굉장히 비슷하므로 쉽게 이해된다. 단, 한가지 단점으로 생각되는 것은 본 논문에서는 imagenet 데이터셋으로 128x128의 이미지를 생성하는 작업을 하였으나, 해상도가 커진다면 self-attention에 적용되는 비용을 무시하지 못할 것이다. 

다음으로 2번은 단지 G와 D에 spectral normalization을 모두 적용한다. spectral normalization과 같은 regularization 기법은 모델의 update를 늦추기 떄문에 원래 D에만 적용될 경우 G를 한번 update할 때, D를 5번 update하였다. 하지만 G와 D에 spectral normalization을 적용할 경우 G또한 update가 느려지므로 1:1 비율로 update할 수 있다. 이는 적당한 update 비율을 맞출 시간을 줄여준다.

마지막으로 3번은 Two timescale update rule (TTUR)라는 방법인데, G와 D에 다른 learning rate를 적용하는 기법이다. 본 논문에서는 D=0.0004, G=0.0001의 learning rate를 적용하였다. 이것 또한 G를 한 번 업데이트 할 떄 D의 업데이트 횟수를 줄여주는 역할을 한다. 

# Experiment 
실험 파트에서 주목할 것은 위의 3가지 방법이 어떻게 차근차근 효과가 있는지에 대한 ablation과 self-attention을 어디에 적용하는지만 보면 된다.
![](/assets/images/2022-08-01-SAGAN/2.PNG)
위의 그림에서 왼쪽부터 각 열이 나타내는 것을 정리하면 아래와 같다.
1. D에만 SN적용.
2. G와 D 모두 SN 적용 + 1:1 update
3. G와 D 모두 SN 적용 + 1:1 update + 4배 learning rate 차이.
주목할 것은 2번과 3번인데, SN을 모두 적용하는 것 뿐만아니라, TTUR을 적용하는 것이 더 향상된 FID를 보여준다. 

이제 self-attention을 어디에 적용해야 하는지 알아보자.
![](/assets/images/2022-08-01-SAGAN/3.PNG)
위의 표는 각 feature map의 해상도 별로 self-attention module을 적용해본 결과이다. 왼쪽의 feat32, feat64가 좋은 성능을 보이는 것을 알 수 있는데, 최종 해상도가 128일 때, 비교적 높은 해상도에서 self-attention을 적용하는 것이 좋다. 이유는, 낮은 해상도의 feature map은 픽셀 자체의 수가 적으므로, self-attention이 적용될 자유도가 낮아진다. 또한 introduction에서 언급하였듯이, receptive field가 큰 낮은 해상도에서 self-attention과 비슷한 역할을 수행하는데, 여기에 중복하여 self-attention을 적용하면 큰 효과가 없다. 

또한 표의 오른쪽에서 볼 수 있듯이, self-attention 대신 각 해상도에 residual blokc을 사용하였을 때 성능이 no attention 보다 하락한 것을 알 수 있다. 이는 SAGAN으로 인한 성능 향상이 model의 depth나 capacity와는 별개로 이루어 졌다고 볼 수 있다.

