---
title: "[CVPR2020]Analyzing and Improving the Image Quality of StyleGAN"
excerpt: "[CVPR2020]Analyzing and Improving the Image Quality of StyleGAN"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2022-02-06T15:04:00-05:00
---

코드 구현까지 완성한 StyleGAN2. 코드는 다음 [링크](https://github.com/rlawjdghek/Generative_Models/tree/main/GANs/StyleGAN2) 참조.
전체 모델 그림은 아래 그림을 참고하자.
![](/assets/images/2022-02-06-StyleGAN2/5.PNG)
# Abstract & Introduction
StyleGAN1과 비교했을 때, StyleGAN2에서 개선한 부분을 요약하면 아래와 같다.
1. characteristic artifacts 개선
2. model architecture 변형
3. training method (not PGGAN) 변경
4. the generator normalization 변경
5. path length regularizer 추가
위의 5가지를 주로 다룰 것이고, 주목할 만한 점은 stylegan1에서 metric으로 사용하였던 perceptual path length (PPL)을 regularizer로 사용한 것이다.

![](/assets/images/2022-02-06-StyleGAN2/1.PNG)
# Removing Normalization Artifacts
먼저 저자들이 주목한 것은 AdaIN 연산이다. 이것은 굉장히 강한 normalization을 수행하고, 이는 통계수치 (평균, 표준편차)에 많은 영향을 주는 값이 생겨도 전체적인 통계수치를 고려하기 때문에
이 큰 값들이 무시될 수 있다. 따라서 중간 feature map에서 이러한 이상치들이 생기면 결국 최종 이미지에서 droplet이 생긴다는 것이다. 이러한 가설을 입증하기 위해서 stylegan에 normalization을 제거하였더니
droplet 현상이 없어진 것을 확인할 수 있었다.

### Generator Architecture Revisited
위의 그림에서 (a), (b)와 비교했을 때, stylegan2에서 제시한 내용은 아래와 같다.
1. 기존의 styleblock내에 포함되어 있던 bias (b)와 noise (B)를 styleblock 밖으로 뺌.
2. constant input에 적용된 bias, noise, normalization을 뺌
이 변화들의 이유는 자세히는 나와있지 않고, 이렇게 변경시켰더니 잘 나왔다고 서술되어있다. 아무래도 bias와 noise가 중간 conv를 지난 feature map에 직접적으로 영향을 주고 normalization이 되면
feature map 자체의 통계값에 영향을 주기 때문이라 생각이 된다.

### Instance Normalization Revisited
기존 adain 연산의 역할인 style mixing은 style에 따라 자릿수를 바꿀 정도의 엄청난 영향을 줄 수 있다. 따라서 저자들은 이러한 style 주입에서도 스케일링을 맞추는 것을 제안한다.
아래 코드를 참조하자.
```python
style = self.modulation(style).reshape(bs, 1, -1, 1, 1)
weight = self.scale * self.weight * style  # 스타일을 weight에 입힘.

if self.demodulate:
    dcoefs = (weight.square().sum((2,3,4)) + 1e-8).rsqrt()
    weight = weight * dcoefs.reshape(bs, -1, 1, 1, 1) # weight : [bs x out_ch x in_ch x k x k]
weight = weight.reshape(bs * self.out_ch, in_ch, self.kernel_size, self.kernel_size)
if self.upsample:
    x = x.reshape(1, bs * in_ch, h, w)
    weight = weight.reshape(bs * self.out_ch, in_ch, self.kernel_size, self.kernel_size)
    weight = weight.transpose(1, 2).reshape(
        bs * in_ch, self.out_ch, self.kernel_size, self.kernel_size
    )
    out = conv2d_gradfix.conv_transpose2d(x, weight, padding=0, stride=2, groups=bs)
    _, _, h, w = out.shape
    out = out.reshape(bs, self.out_ch, h, w)
    out = self.blur(out)
elif self.downsample:
    x = self.blur(x)
    _, _, h, w = x.shape
    x = x.reshape(1, bs * in_ch, h, w)
    out = conv2d_gradfix.conv2d(x, weight, padding=0, stride=2, groups=bs)
    _, _, h, w = out.shape
    out = out.reshape(bs, self.out_ch, h, w)
else:
    x = x.reshape(1, bs * in_ch, h, w)
    out = conv2d_gradfix.conv2d(x, weight, padding=self.padding, groups=bs)
    _, _, h, w = out.shape
    out = out.reshape(bs, self.out_ch, h, w)
return out
```
이 코드는 stylegan2 구현의 ModulatedConv2d의 forward 부분이다.
먼저 mapping network를 통과한 style을 modulation 연산 (그냥 차원 맞추기)을 통과 시키고, conv2d weight에 scale과 style을 곱해준다. 식은 아래와 같다.

$w_{ijk}^{\'} = s_i \cdot w_{ijk}$

위의 식에서 짚고 가야할 것은, 예를 들어 styleblock에서 64 -> 512채널로 갈 때, i=64, j=512, k=3이렇게 대입된다. 우선 이 스케일링은 64채널별의 fan_in과 같다. 실제 구현은 아래와 같다.
```python
fan_in = in_ch * kernel_size ** 2
self.scale = 1 / math.sqrt(fan_in)
```
그 다음 weight의 dcoef를 구해주는데 이는 아래와 같다.
$w_{ijk}^{\'\'} = w_{ijk}^{\'} \ \sqrt{\sum_{i,k}{w_{ijk}^`}^2 + \epsilon}$
이 식에서도 이번엔 분모 시그마에 i,k의 합을 구하는데 이는 out_channel-wise로 연산이 이루어 진다. 
이 연산은 normalization을 하는데 feature map에 직접적으로 하는 것이 아니라 conv 커널에 하는 것이고, 평균은 고려하지 않는다. 논문에서는 input activation을 i.i.d random variable로 가정하였다
개인적으로 위의 코드를 봤을때 그림 (d)는 약간 이해가 안될 수 있을 듯하다. demodulation 레이어가 있다고 착각할 수 있음. 
이렇게 새로 구현한 normalization은 adain으로 인해 발생할 수 있는 droplet현상을 줄여준다.

# Image Quality and Generator Smoothness
StyleGAN1에서 제시한 Perceptual Path Length은 간단하게, generator의 입력 z가 **일정** 작은 스텝의 변화가 있을 경우 출력 이미지의 변화가 **일정하게** 변하느냐를 측정하는 지표이다 (이를 한 단어로 generator smoothness).
(기존에 이와 비슷한 지표로는 약간의 perturbation을 준 latent input의 생성이미지 사이의 거리를 비교하는 average LPIPS distance를 사용하였다.)
즉, 좋은 generator일수록 입력이 일정하게 변하면 출력 이미지도 약간 변한다는 것인데, stylegan1에서는 이를 단순 generator를 평가하는 지표로만 사용하였다. 하지만 GAN이 학습 되면서 generator가 low-quality의 이미지를 생성하면,
D는 이러한 이미지들이 안나오도록 penalize 할 수 있다. 이는 mode collapse와 비슷한 개념이지만 여기서는 그렇게 극한의 상황은 아니고 잘 나오는 이미지의 범위를 줄인다고 생각을 하자. 그렇게 되면, 생성하는 이미지의 quality가 높아지도록
latent space의 범위를 줄일 수 있다. 이런 경우에는 PPL이 높게 나오겠지만, generator의 diversity가 낮아져서 좋지 못한 평가 메트릭이 된다. 따라서 논문에서는 minimal PPL을 권하지는 않지만, stylegan2에서는 여전히 PPL이 좋은 generator의
중요한 측도이므로 이를 regularizer로 사용하였다. 

### Lazy Regularization
stylegan에서 사용한 주 손실함수는 logistic loss와 R1 regularizer이다. generator smoothness를 위하여 첫번째로 제시한 것은 lazy regularizer로 단순히 regularization을 몇 iteration마다 한번씩 수행하는 것이다. 

### Path Length Regularizer
이 regularizer의 목표는 **$\mathcal{W}$의 small, fixed-size step에서 이미지의 non-zero, fixed magnitude change를 유도하는 것**이다. 좀 더 자세히 서술하면, 이미지에서의 random step이 
w의 gradient 값을 비교했을 때, 이 gradient는 w의 값이나 이미지가 변한 방향과 관계없이 일정한 길이를 가져야한다는 뜻이다. 이를 식으로 표현하면 아래와 같다.
$\mathbb{E}_{w,y\sim\mathcal{N}(0, I)}(||\mathbf{J_w^T}\mathbf{Y}||_2 - a)^2$
이를 풀어서 해석하면, $\mathbf{J_w}=\partial g(w) / \partial w$이므로 어떤 생성 이미지에 대한 기울기와 생성이미지의 변화량이 일정한 step size를 나타내는 a로 맞춰진다는 것이다. 이 때 a는 하이퍼파라미터처럼 사전에 고정적으로 정해진 상수가 
아니라 학습되면서 최적으로 수렴하게 된다. 
이 regularizer는 결과적으로 **reliable and consistently behaving model**을 만든다. 또한 **easier to invert**하게 한다.

![](/assets/images/2022-02-06-StyleGAN2/3.PNG)
# Progressive Growing Revisited
이 장에서는 model architecture에 대해서 논한다. stylegan1에서는 PGGAN에서 제시한 점진적으로 증가하는 구조를 사용하였는데 이 구조의 대표적인 문제가 strong localization이다. 즉, 위의 그림에서 봤을 때, 
이빨, 눈은 계속 정면을 보는 것을 알 수 있다. 이는 generator가 점점 자랄수록 각각의 해상도는 일시적으로 출력 이미지의 해상도로 여겨진다는 것이다. 예를 들어, 최종적으로 1024 해상도를 생성할 때 현재 256의 generator가 완성되었다고 하면,
이미 이 256사이즈의 이미지의 영향력이 커서 1024에 가서도 local한 변화는 잡지 못한다. 따라서 progressive growing 기법은 훈련은 쉽게 할 수 있으나 결정적인 단점이 존재한다.

![](/assets/images/2022-02-06-StyleGAN2/2.PNG)
### Alternative Network Architecture
결론적으로 말하자면, StyleGAN2의 generator는 skip network를 활용하였고, discriminator는 residual network구조를 활용하였다. Discriminator는 그냥 resnet을 사용했으므로 여기서는 generator만 살펴보자. 위의 그림에 자세히 나와있는데,
generator에서 사용한 skip은 resnet과는 사뭇 다르다. 아래 코드를 보면 알 수 있듯이, 먼저 conv block를 2개 넣고, ToRGB라는 블록을 넣는다. 이 ToRGB블록은 입력채널을 이전 conv block의 채널로하고, 출력채널을 3으로 하여 아웃풋 이미지를
만든다. PGGAN은 한번에 훈련하지 않고 중간중간 작은 네트워크들을 끊어서 점점 성장시키는데에 반해, 이 아이디어를 한번에 구현하기 위해서 완성된 RGB 이미지를 마지막 최고해상도까지 끌고 간다. 이 때 upsample로는 bilinear를 사용한다. 
```python 
            self.convs.append(
                StyleConv(
                    in_ch=in_ch,
                    out_ch=out_ch,
                    kernel_size=3,
                    style_dim=style_dim,
                    upsample=True,
                    blur_kernel=blur_kernel
                )
            )
            self.convs.append(
                StyleConv(
                    in_ch=out_ch,
                    out_ch=out_ch,
                    kernel_size=3,
                    style_dim=style_dim,
                    blur_kernel=blur_kernel
                )
            )
            self.to_rgbs.append(
                ToRGB(
                    in_ch=out_ch,
                    style_dim=style_dim
                )
            )
            in_ch = out_ch
```

![](/assets/images/2022-02-06-StyleGAN2/4.PNG)
### Resolution Usage
위에서 말한 skip generator가 PGGAN이 추구하는, 초기의 낮은 해상도에서 feature에 집중하였다가 나중에 fine한 디테일한 feature를 나타내는것을 가능하게 한다. 하지만 이것이 절대적으로 맞는지는 모르기 때문에, 
generator는 이러한 학습방법이 유리할 경우에만 한다. 즉, PGGAN보다 더 폭넓게 학습가능한 프레임워크로 동작하기 때문에 PGGAN이 추구하는것이 맞다면, 그것을 학습하고, 아니라면 학습을 안할 수 있다. 따라서 저자들은
과연 최종 출력이 특정 해상도에 얼마나 의존하는지를 분석하였다. generator의 skip연산이 각 해상도의 RGB이미지를 summing하기 때문에, 이 RGB이미지들이 최종 이미지에 얼마나 기여하는지를 분석하였다. 위의 그림을 보면, 
각각의 RGB이미지(논문에서는 tRGB라 하였는데 이는 alternative network architecture의 그림에서 skip generator (1행 2열 그림)의 tRGB라는 것을 표현)의 픽셀값의 표준편차의 상대적인 비중을 y축, 훈련 시간을 x축으로 그렸다. 
첫번째 그림에서 볼 수 있듯이, 생성된 이미지가 실제로는 최종 해상도를 전부 다 활용하지 못할 것이라는 것을 짐작한다. 이는 capacity 문제일 수도 있다고 여겨, 더 큰 네트워크로 실험한 결과, 두번째 그림에서 볼 수 있듯이 1024의 
비중이 가장 크게 나왔다. 또한 더 큰 네트워크의 성능이 FID나 recall에서 더 좋게나온다. 

# Projection of Images to Latent Space
이 장에서는 generator의 inverting에 관하여 다룬다. 좋은 generator일수록 latent code를 찾는 inverting작업이 쉽다. 
### Attribution of Generated Image
먼저 저자들은 실제 또는 생성된 이미지 $x$와 이를 inverting한 latent code $g^{-1}(x)$를 다시 generate하는 $g(g^{-1}(x))$를 거쳐서 이 둘의 LPIPS거리를 측정한다. 
위의 그림은 StyleGAN과 StyleGAN2의 원본 이미지 vs (진짜와 가짜이미지를 inverting하고 다시 generate한 이미지)의 LPIPS값의 히스토그램을 보여준다. 우선 LPIPS는 높을수록 더 다양하고 다르다는 것을 의미한다.
따라서 모델이 생성한 이미지의 유사성이 좀 더 낮은 것은 당연한 결과이다. 주목해야할 점은 StyleGAN에 비해서 StyleGAN2의 히스토그램 분포가 좀 더 명확히 나누어지는 것을 볼 수 있다. 정확히는 
원본이미지의 스코어는 그대로 있지만, 생성된 이미지의 LPIPS가 더 낮아진것을 볼 수 있고, 이는 latent code로의 projection이 잘 되어서 다시 generator를 통과 했을 때 더 유사한 이미지를 만들어내는 것을 의미한다.


