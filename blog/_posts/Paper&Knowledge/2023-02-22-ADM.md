---
title: "[NeuRIPS2021]Diffusion Models Beat GANs on Image Synthesis"
excerpt: "[NeuRIPS2021]Diffusion Models Beat GANs on Image Synthesis"
categories:
  - Paper & Knowledge
  
tags:
  - Diffusion Models
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2023-02-22T15:04:00-05:00
---

Guided Diffusion Model로도 불리는 논문. Diffusion Model(DM) 논문에서 GAN이 아직까지 더 좋은 성능을 보이는 것은 지금까지 오래 연구되었고, spectral norm등의 좋은 스킬들과 BIGGAN, stylegan2와 같은 최적의 모델 구조가 연구되어왔기 때문이라고 지적하였다. 이 논문에서는 모델의 
구조적인 ablation을 진행하여 성능을 끌어 올리고, 초기의 GAN 연구에서 제시한 auxiliary classifier를 통한 fidelity vs diversity를 DM에 적용하였다. 본 논문의 contribution 2가지는 아래와 같다.

1. BIGGAN과 같이 DM에 적합한 모델 구조를 ablation 하였다.
2. Classifier-guidance를 통하여 diversity를 줄이는 대신 fidelity를 증가하였다.

abstract와 introduction, background는 기존 연구들을 설명하는 단계이므로 넘어간다. 또한 코드적인 측면에서, architecture improvement는 특별한 것이 없고, classifier guidance는 아래의 코드와 같이 샘플링 할 때, classifier의 gradient를 mean에 반영하므로 직접 구현은 하지 않았다. 

# Method 

### Architecture Improvements 
긍정적인 효과를 보이는 설정들을 정리하면 아래와 같다.
1. adaptive group norm사용. 이 때, time embedding과 label을 모두 임베딩한다.
2. attention layer는 32, 16, 8 해상도에서 사용하는것이 8에서만 사용하는 것보다 좋다.
3. BIGGAN의 up/down sample 사용하는것이 좋다.
4. resblock에서 skip connection할 때 $\frac{1}{\sqrt{2}}$를 곱하는 것은 좋지 않다.
5. self-attention에서 head갯수가 1개보다 4개가 좋다.

### Classifier Guidance
![](/assets/images/2023-02-22-ADM/1.jpg)
classifier guidance의 시작은 개인적으로 $q(x_t \| x_{t+1}, y)$에서 시작되었다고 본다. y를 denoising process에서 레이블로 넣어줌으로써 모델이 클래스 중의 하나의 mode에 집중하게 하고 이는 diversity를 줄이는 대신 fidelity를 늘린다. 
위의 풀이에서 볼 수 있듯, 베이지안으로 $q(x_t \| x_{t+1}, y)$을 두 분포로 나누고, 두번째 항 $q(y \| x_t)$를 classifier로 예측하는 것이 포인트이다. 그 이후 실제 샘플링 단계에서 q==p로 훈련된 denoising 모델 p의 분포 $p(x_t \| x_{t+1}, y)$를 유도한다. 
이는 가장 아래와 같은 분포로 나타낼 수 있고, 이는 기존의 평균에 분산*기울기만큼 이동한 것과 같다. 따라서 샘플링 과정에서 미리 훈련된 classifier의 기울기를 denoising process에 추가한다. 이에 해당하는 코드들은 아래와 같다. 

기울기를 구하는 함수.
```python
def cond_fn(x, t, y=None):
    assert y is not None
    with th.enable_grad():
        x_in = x.detach().requires_grad_(True)
        logits = classifier(x_in, t)
        log_probs = F.log_softmax(logits, dim=-1)
        selected = log_probs[range(len(logits)), y.view(-1)]
        return th.autograd.grad(selected.sum(), x_in)[0] * args.classifier_scale
```

계산된 기울기를 평균에 대입하는 함수
```python
def condition_mean(self, cond_fn, p_mean_var, x, t, model_kwargs=None):
    """
    Compute the mean for the previous step, given a function cond_fn that
    computes the gradient of a conditional log probability with respect to
    x. In particular, cond_fn computes grad(log(p(y|x))), and we want to
    condition on y.
    This uses the conditioning strategy from Sohl-Dickstein et al. (2015).
    """
    gradient = cond_fn(x, self._scale_timesteps(t), **model_kwargs)
    new_mean = (
        p_mean_var["mean"].float() + p_mean_var["variance"] * gradient.float()
    )
    return new_mean
```

샘플링 과정에서 $x_t$를 계산하는 함수.
```python
def p_sample(
    self,
    model,
    x,
    t,
    clip_denoised=True,
    denoised_fn=None,
    cond_fn=None,
    model_kwargs=None,
):
    out = self.p_mean_variance(
        model,
        x,
        t,
        clip_denoised=clip_denoised,
        denoised_fn=denoised_fn,
        model_kwargs=model_kwargs,
    )
    noise = th.randn_like(x)
    nonzero_mask = (
        (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))
    )  # no noise when t == 0
    if cond_fn is not None:
        out["mean"] = self.condition_mean(
            cond_fn, out, x, t, model_kwargs=model_kwargs
        )
    sample = out["mean"] + nonzero_mask * th.exp(0.5 * out["log_variance"]) * noise
    return {"sample": sample, "pred_xstart": out["pred_xstart"]}
```

