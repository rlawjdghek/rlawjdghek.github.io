---
title:  "[ICLR2019]BIGGAN-Large Scale GAN Training for High Fidelity Natural Image Synthesis"
excerpt: "[ICLR2019]BIGGAN-Large Scale GAN Training for High Fidelity Natural Image Synthesis"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2021-11-10T15:04:00-05:00
---

구글 DeepMind에서 발표한 논문. 이것과 StyleGAN, StyleGAN 2가 있는데 실험한 데이터셋이 달라서 (StyleGAN은 FFHQ와 LSUN, BIGGAN은 Imagenet, JFT) 아직 
어느 것이 더 높은 IS를 보이는지 못찾았다. StyleGAN이 novelty측면에서 GAN의 대표 논문이 되었다면 BIGGAN은 실험적인 측면에서 많은 기여를 한다.

# Abstract & Introduction
BIGGAN이전의 논문들은 모두 이미지넷과 같은 아주 큰 네트워크에서 GAN을 학습하지 못하였다. BIGGAN은 이러한 큰 데이터를 학습한 GAN을 만드는 법을 소개하고 이러한 빅데이터를 
GAN이 학습할 때 사용하면 좋은 기존의 방법들을 실험하였다. 논문에서는 크게 이론적인 내용은 없고, 모두 실험과 결과 나열에 관한 내용이다. \
BIGGAN의 주장은 현재 GAN의 연구 방향은 크게 2가지가 있다. 
1. 훈련의 안정성 (훈련 가능과 연관)
2. 모델의 수렴성 (성능과 연관)
또한 이 2가지의 방향에 대한 연구가 단시간에 굉장히 많이 이루어졌기 때문에 이 방법들을 적절히 조합한다면 충분히 이미지넷과 같은 빅데이터를 학습 할 수 있다고 말한다. 즉, 
이 논문에서는 새로운 방법론을 제시하지 않고 기존 훈련 방법들 중 일반적으로 GAN 훈련에 도움이 되는 방법을 채택하여 조합을 찾았다. \
**개인적인 생각으로는 BIGGAN의 훈련데이터는 워낙 큰 데이터셋이다 보니 작은 데이터셋에서는 일반적으로 통하지 않을 수 있다고 생각이 든다. 이는 직접 BIGGAN을 구현하고 다른 데이터셋으로
훈련하면서 알아보자.**

# Related Work 
related work에서 주목할만한 것은 위에서 언급하였듯이 훈련의 안전성, 모델의 수렴성에 대하여 BIGGAN에서 사용한 방법들의 소개를 간단히 한다. \
훈련의 안정성으로는 Spectral Normalization을 사용하였고, 수렴을 높이기 위해 SA-GAN을 기본 구조로 사용하였다고 한다. 하지만 뒤늦게 resnet으로 실험하여 현재 기록상으로는
resnet을 기본구조로 한 BIGGAN-Deep이 훨씬 높은 성능을 보인다. 또한 이 논문 저자들은 GAN을 통제하지 않으면 의미가 없다고 느꼇는지 기본적으로 훈련을 할 떄 conditional GAN으로 학습을 하였다. 
이미지넷은 클래스를 1000개 갖고 있으므로 이를 조절하지 못하면 FID등의 score를 도출하기 힘들기 때문도 있는듯 하다. 


![](/assets/images/2021-11-10-BIGGAN/1.JPG)ㅜ
# Methods
이제부터 BIGGAN의 성능을 높인 방법들 하나씩 알아보자. 표에 적힌 성능은 모두 Collapse가 일어나기 전의 모델로 평가한 것이다. 
 
### Scale
1. 배치 사이즈. 논문에서 밝혔듯이 2048의 배치사이즈가 가장 좋은 성능을 보였다. (46% 향상)\
논문에서는 배치사이즈가 커질수록 더 많은 mode를 볼 수 있기 때문에 성능이 향상되지만, Collapse가 일어날 가능성이 높다고 한다. 

2. Width. 즉, 채널의 갯수를 늘려 비교한다. 논문에서는 더 큰 모델, 채널이 클수록 좋은 성능을 보였다. 

3. Conditional Batch Normalization. 일반적인 BN대신 CBN을 사용.

4. 입력 노이즈 z를 나누어 각 레이어에 concat 하였다. 이것의 해석은 G가 다른 해상도에서 random noise를 받을 수 있다. 하지만 BIGGAN-Deep에서는 사용되지 않았다. 
아마도 skip connection 덕분인 듯 하다.

### Trading off variety and fidelity with the truncation trick
prior정복 없는 GAN은 기본적으로 입력으로 normal distribution을 사용한다. 하지만 분포 그래프에서 알 수 있듯이 끝으로 갈수록 likelihood가 낮아져 나올 가능성이 낮아진다. 
따라서 논문에서는 truncation trick이라는 trick을 사용한다.\
truncation trick은 어떤 값 c에 대하여 이 값을 넘는 값이 나오면 다시 샘플링하여 대체하는 것을 말한다. truncation trick의 잘리는 부분을 c라고 할 때, c가 작아질 수록 나올 수 있는 값의 범위가 작야져
입력은 더 작은 범위에서 벡터를 뽑게 되고, 아는 다양성의 감소를 야기한다. 하지만 좋은 수렴도를 주기 떄문에 trade-off가 발생한다고 말하는 것이다. 하지만 몇몇 모델에서
truncation trick은 입력의 정의역을 줄여 G를 더 빽빽하고 smooth하게 하는 것이므로, 저자들은 smooth와 반대되는 Orthgonal Regularization을 사용하였다. 하지만 
Orthogonal regularization는 너무 크게 제한을 주기 때문에 이를 그대로 사용하는 것이 아니라 약간 다른 버전을 사용하였는데 결론적으로 식은 아래와 같다. \
$R\_{\beta}(W)=\beta ||W\_T W - (1-I)||\_F^2$

$\beta$값은 1e-4이다. 즉, 아주 작은 제한을 걸어준다. \
또한 중요한 것은 2가지 metric에 대하여 truncation trick이 어떠한 영향을 주는지 발견하였다.
1. IS = precision = fidelity => 섬세한 output 
2. FID = recall = variaty => 다양한 output \
이 논문에서 사용한 metric은 이 2개이다. 논문에서는 두 metric을 precision-recall에 비유했는데, 이것은 정확한 등식은 아니다. 왜냐하면 FID와 IS모두 높아질 수 있고, 동시에 낮아질 수도 있다. 즉,
반비례 관계가 아니다. 하지만 다양한 모델에서 그림을 그려보면 일정 수준 이상에서는 반비례하는 모습을 볼 수 있다. truncation trick의 제한을 많이 걸면, 즉, c값을 낮춰 입력 노이즈의 범위를 좁게 하면
다양성이 줄어 비슷한 이미지들이 나오지만, 섬세함은 증가한다고 볼 수 있다. 반면, c가 커지면 입력의 정의역의 범위가 넓어지고 이는 덜 연속적인 출력을 야기할 가능성을 높인다. 
반대로 다양성은 커지지만 완벽하지 않은 이미지가 나올 가능성이 높다. 예를 들어 두개의 클래스가 합쳐진 이미지가 나올 수도 있다는 것이다.


# Analysis
이제 앞의 여러 방법들을 조합한 GAN이 훈련 도중 어떤 특징이 있는지를 살펴보자.
 
### Instablity in G
BIGGAN은 Spectral normalization을 사용하였으므로 collapse가 일어날 때 singular value를 그래프로 그렸다. 
![](/assets/images/2021-11-10-BIGGAN/2.JPG)
위의 그림을 보면 오른쪽 G그림에서는 smooth하게 가다가 Collapse가 일어난 직후에 singular value가 발산하는 것을 볼 수 있다. 따라서 
아래 식과 같은 추가적인 constraint를 주었다. \
$W = W - max(0, \sigma_{0} - \sigma_{clamp})v_0 u_0^T$

### Instability in D
위의 그림에서 D에서는 중간중간 튀는 것을 볼 수 있는데, 이는 G에서 D를 방해하는 batch를 생성하기 때문이다. 즉, 생성된 이미지들이 D의 gradient를 튀게 만들고,
이는 GP를 추가하여 대응할 수 있다. 따라서 gradient panelty를 주었다. \
$R_1 = \frac{\gamma}{2} \mathbb{E}\_{p\_{\mathcal{D}}}\[||\nabla D(x)||_F^2\]$\
논문에서는 $\gamma$를 10으로 주었지만, 이 값을 크게주면 학습이 안정화 되지만, 성능이 낮아진다. 반면, 값이 작으면 성능은 좋아지지만 학습의 안정성이 떨어진다. \
또한 BIGGAN에서 모델의 사이즈가 커지면서 GAN의 학습의 이해 관점에서 중요한 것이 있다. 먼저 D의 입장을 생각해 본다면, WGAN loss를 사용하면 D의 손실함수는 0으로 가는데, 이는 D가 훈련데이터에 overfit된 것이라 할 수 있다. 이를 증명하기 위해서
저자들은 imagenet 데이터에 잘 훈련된 GAN에서 D를 뽑아 생성된 이미지에 대해 training data와 validation을 구분하였다. training에서는 98%임에 반해 validation에서는 50%이므로 
training에 overfit 되었다고 할 수 있다. 즉, D는 real과 fake에 대하여 decision boundary를 그리는 것 뿐만 아니라, training data를 외운다고 볼 수 있다. 하지만 이는 training data를 외운다고만 
할수는 없다. G가 잘 훈련되는 케이스도 있기 때문에 G에 의미있는 정보도 준다. 그렇다면 G의 입장에서는 어떤지도 생각해 봐야한다. 만약 G가 training data를 외운다고하면, interpolation 결과를 잘 보여주지
못할 것이다. 하지만 BIGGAN은 이런 경우도 잘 보여주기 때문에 G가 모델 사이즈가 커진다고 해서 overfit이 된다고 할 수 없다.   

결론적으로 G와 D 둘 중 하나만 제약을 주어서는 GAN을 안정화 시키기 어렵다. G와 D는 끊임없이 적대적인 상호작용을 하기 때문에 둘 다에게 적절한 제약을 걸어주는 것이 맞다. 또한,
현재 방법으로는 Collapse를 막을 수 없다. 하지만 constraint를 줌으로써 (특히 D에) 안정성을 올릴수는 있지만 이는 성능 하락과 비례하기 때문에 Trade off라는 말을 사용한다. 

# Experiment
실험 조건은 아래 3가지로 하였고 각각의 의미하는 바가 있다.
1. best FID에 대한 FID, IS값 => 다양성이 최대일 경우 quality
2. validation과 IS값이 같을때의 FID => objectness가 어느정도 좋을 때의 다양성
3. IS가 최대일 떄 FID => quality가 최대일 때 다양성
![](/assets/images/2021-11-10-BIGGAN/3.JPG)
위의 표에서 3가지 기준에 대하여 BIGGAN-DEEP이 월등한 성능을 보여준다.

이제 BIGGAN이 생성에 실패한 이미지를 분석해보자. 지금까지의 GAN들에서는 local artifact, texture bias, mode collapse가 자주 일어나는데 BIGGAN에서는 두가지 클래스가 섞인 이미지가 나온다. 
저자들은 이를 class leakage라고 부르는데 이는 BIGGAN의 훈련 데이터상의 문제일 수 있다. imagenet은 1000개의 서로 다른 클래스로 나누있고, conditional GAN으로 훈련한다. PGGAN과 달리 
클래스 정보가 들어가지만 비슷한 시각적 정보를 담고 있는 다른 클래스가 있기 때문에 이는 훈련 데이터상의 한계라고 볼 수 있다.

다음으로는 JFT에 대해서도 실험을 진행 하였다. 이 데이터는 imagenet의 약 3배가까이 되므로 더 큰 모델이 어울린다(width 96 -> 128). truncation trick에 대한 변화를 보여주었는데
Imagenet에서는 가장 높은 IS를 뽑기 위해 c가 0에 거의 가까운 값을 가져야 했다 (당연히 섬세한 이미지를 뽑기 위해서는 같은 input만 들어가는 것이 안정성있다). 하지만 JFT에서는
c가 0.5~1.0의 값에서 가장 높은 IS를 보였는데, 이는 JFT는 모든 이미지가 항공과 관련되었기 때문에 비슷한 클래스가 워낙 많고 비슷한 이미지도 많다. 따라서 오히려 input에서 다양성이 조금 있어야 
더 높은 IS가 나오는 것을 확인 할 수 있다. 

# Summary
결론적으로 BIGGAN에서 사용한 방법을 나열하자면 아래와 같다.
1. SAGAN보다 resnet이 더 좋다
2. SAGAN에서는 skip z를 사용하였으나, resnet을 사용할거면 skip z를 사용하지 마라
3. truncation trick으로 fidelity와 variety를 조절 할 수 있다.
4. batch size는 2048을 사용하였다.
5. 훈련 데이터 크기에 맞게 모델 크기도 키워라
6. 안정성은 Spectral normalization을 베이스로 잡았다. 
7. G의 안정성을 추가하기 위해 Orthogonal regularization을 추가하였다.
8. D의 안정성을 추가하기 위해 gradient penalty를 추가하였다.
9. D를 2번 훈련할 동안 G를 1번 훈련하였다.
10. G와 D를 Orthogonal initialization으로 초기화 하였다. 
11. Adam optimizer를 $\beta_1 = 0, \beta_2=0.999$로 사용하였다. 
12. BIGGAN의 모든 해상도에서 learning rate를 D에 2e-4, G에 5e-5를 사용하였다.
13. BIGGAN-Deep의 128 해상도에서 learning rate를 D에 2e-4, G에 5e-5를 사용하였다. 256, 512해상도에서는 G와 D에 2.5e-5를 사용하였다.  
14. Spectral normalization의 low-precision numerical 문제를 해결하기 위해 batch normalization에 입실론 값을 1e-8에서 1e-4까지 늘렸다.
15. width (CNN 레이어의 채널)은 96을 사용하였다. 데이터셋이 커지면 더 큰 것을 사용해라.
16. Conditional Batch Normalization을 사용하였다. 
17. G와 D 모두 또는 한개만 learning rate를 늘려도 collapse가 발생한다. 논문에서는 D: 2e-4 -> 4e-4, G: 5e-5 -> 1e-4 로만 해도 발생했다고 한다. 즉, learning rate는 매우 중요하다.
WGAN류의 단점으로 수렴속도가 느리다는 것이 있다. 이는 작은 learning rate의 사용 또한 원인이 된다.
18. Adam에서 momentum을 증가시키는 것도 collapse가 발생하였다. 
19. 큰 데이터셋을 훈련하는 것은 collapse가 늘어날 확률이 높다. 
20. D를 고정시키고 G를 계속 훈련하면 바로 collapse가 발생한다.
21. G를 고정시키고 D를 계속 훈련하면 손실 함수가 0으로 간다.
22. 20, 21에서 도출할 수 있는 결론은 D는 항상 조금 더 optimal값에 있어야 한다. 하지만 optimal에 있으면서도 G에게 의미있는 gradient를 주어야 한다. 또한 G가 이기면 바로 collapse가 일어난다.
23. D가 optimal에 가까운 것은 맞지만 이것이 최적의 답이 아니다. 즉, 이것이 항상 안정성을 보장하는 것은 아니다.
24. G의 upsample 전략으로는 nearest-neighbors를 사용하였다. 다른 것은 성능이 하락되었다. 
25. Batch Normalization은 G와 D 둘다에 있어야 한다.
