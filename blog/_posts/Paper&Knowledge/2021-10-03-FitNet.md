---
title:  "[ICLR2015]FitNets: Hints for Thin Deep Nets"
excerpt: "[ICLR2015]FitNets: Hints for Thin Deep Nets"
categories:
  - Paper & Knowledge
  
tags:
  - Knowledge Distillation
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2021-10-03T15:04:00-05:00
---


# Abstract & Introduction
Hinton의 KD에서 영감을 얻었으나 ensemble method는 고려하지 않고 단지 크고 성능이 좋은 모델을 teacher로 사용한다. **뱅지오 교수님의 논문답게 잘 썼다는 느낌이 든 것은, 방법론에 대한 
novelty에 그치지 않고, 이 방법론이 소개됨으로써 도출되는 결론들이 아주 유용할 것이라는 것을 생각이 들었다 (e.g. This allows one to train deeper students that can generalize better or run faster, a 
trade-off that is controlled by the chosen student capacity.).**

Introduction 마지막에서; 더 깊은 모델이 더 잘 generalize 되어있다. 또한 모델을 얇게 하면 computation cost까지 줄일수 있으므로 얇고 깊은 (thin and deep) 모델을 만들어야 한다.\
**즉, 이 논문에서는 cost는 같지만, 1. 더 좋은 성능을 보여주는 모델을 만드는 메커니즘(i.e. 얇고 깊은 모델)을 소개하였고, 이를 해결하기 위해 2. Hint-Based KD를 제시하였다. 방법론이 부차적이라는 느낌을 받았다.**

# Methods
우선 용어부터 정리하자.
1. FitNet: student네트워크
2. Hint: teacher의 중간 layer부터 나오는 output
3. Guieded layer: hint를 받는 student의 중간 layer
4. regressor: Hint와 guided layer의 output 사이즈를 맞추가 위해 guided layer에 붙는 layer

![](/assets/images/2021-10-03-FitNets/1.PNG)
제시한 손실함수는 아래와 같고, 그림을 보면 쉽게 어떻게 구성되는지 알 수 있다.\
$\mathcal{L}\_{HT}(\mathbf{W\_{Guided}}, \mathbf{W\_r}) = \frac{1}{2} \parallel u_h(\mathbf{x}; \mathbf{W_{Hint}} - r (v_g (\mathbf{x}; \mathbf{W_{Guided}}); \mathbf{W_r})) \parallel^2 $\
$\mathbf{W_{Guided}}$는 guieded layer의 파라미터, $\mathbf{W_{Hint}}$는 Hint layer의 파라미터, $\mathbf{W_r}$는 regressor layer의 파라미터, $u_h, v_g, r$은 각각 들어있는 파라미터의 함수를 지칭한다. \
또한 $r$, regressor는 FC layer일 경우 height, width가 큰 초반 레이어의 경우 파라미터가 $N_{i, 1} \times N_{j, 1} \times C_{1} \times N_{i, 2} \times N_{j, 2} \times C_{2} $이므로, (각각은 height, width, channel)
너무 커진다. 따라서 regressor는 convoluation으로 맞춘다.

알고리즘은 아래와 같다.
![](/assets/images/2021-10-03-FitNets/2.PNG)

FitNet은 단순히 student model이므로 이제부터 저자들이 소개하는 feature matching을 Hint-based Training (HT)라고 하자. HT는 student의 중간 layer들이 Hint layer의 output을 예측한다고 할 수 있다.
이것은 Deeply-Supervised Net (DSN)에서 소개된 중간 레이어들에 CE를 주는 것과 비슷한 개념인데, DSN은 CE를 주었고 HT는 feature 텐서를 맞춰주기 때문에 L2를 사용하였다. 어쨋든 **둘다 regularzation이라고 할 수 있는데, 
이는 student network를 덜 flexible하게 만든다는 것과 같다.** 따라서 더 깊은 layer의 hint를 준다면, student는 over-regularized되어 성능이 안 나올수 있다.  


# Results
CIFAR-10, CIFAR-100, SVHN, MNIST, AFLW에 대하여 성능을 보여줌. KD랑만 비교했는데 잘 나온다.

# Analysis of Empirical results
이 부분이 abstract에서 첫번째로 주장하는 얇고 깊은 모델이 더 성능이 좋음.을 보여주는 섹션이다. 우선 ablation용으로 convolution block을 단일화 하였다. \ 
3x3 conv -> maxout -> max-pooling & 마지막엔 GAP\
cost는 같아야 하므로 깊은 모델은 block을 더 쌓고, 앞의 convolution에서 channel을 줄인다. 그러면 parameter는 많아지지만 FLOPS수는 같다. 
![](/assets/images/2021-10-03-FitNets/3.PNG)
위 그림에서 볼 수 있듯이 레이어가 늘어남에 따라 CE, KD, HT 세 모델의 보여주는 성능이 다르다. **특히, 각 그림에서 모든 node들은 연산량이 30M, 107M으로 같다는 것에 주목하자.**
1. CE는 5-layer 이하에서만 작동한다. 즉 더 깊게 갈 수 없다.
2. KD는 7-layer이하에서 작동한다.
3. HT는 13-layer까지 작동한다. 
이것은 HT가 중간에 계속 정보를 주입하는 것과 관련이 있다. 저자들은 parameter space로 이 현상을 설명하였는데, 우리가 He initialization처럼 모델 가중치 초기화가 상당히 수렴에 중요하다는 것을 알고있다. 무작위로 초기화 된 
parameter는 local minima와 saddle point가 많기 때문에 수렴이 안될 수 있다. 따라서 CE와 KD에서는 초기 parameter space에서 이 두개가 많아 훈련이 안된 것이다. 반면, HT는 initial position을 잘 잡게 하여 훈련이 
된 것.

![](/assets/images/2021-10-03-FitNets/4.PNG)
위의 테이블은 깊이에 대하여 분석한 표이다. 주목할 것은 FitNet3와 FitNet4인데 깊이와 multiplication 연산량을 보자.\
FitNet1와 FitNet2에서는 깊이는 같지만 FitNet2의 연산량이 더 높다. 따라서 성능이 당연히 증가하였고, FitNet3와 FitNet4에서는 연산량이 같지만, 
깊이가 다르게 설정되어있고, 성능도 깊이가 깊은 모델 FitNet4가 더 높은것을 볼 수 있다. 

  
