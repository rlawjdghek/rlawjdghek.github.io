---
title:  "[CVPR2019]ArcFace: Additive Angular Margin Loss for Deep Face Recognition"
excerpt: "[CVPR2019]ArcFace: Additive Angular Margin Loss for Deep Face Recognition"
categories:
  - Paper & Knowledge
  
tags:
  - Representation Learning
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2021-09-19T22:33:00-05:00
---

SphereFace, CosFace 다음으로 나온 논문. 모두 CVPR에 1년 간격으로 나왔다. 그런데 식은 거의 다 비슷한게 아주 조금씩 다름.
제일 중요한 점은 역시 W의 각 column은 각 클래스의 Center와 같다는 것. softmax값이 커지는 방향으로 모델이 훈련 되는데, softmax가 커지는 것은 결국
W의 각 열과 feature의 내적값이 커지는 것이고, 이는 각도가 줄어든다는 것이다. 

### Abstract & Introduction
이 논문에서는 representation learning에서 소개된 loss의 큰 개념들을 4개로 소개한다. 그림과 연관지어서 식을 보면 이해 잘됨.
![](/assets/images/2021-09-19-Arcface/1.PNG)
1. Margin Loss: 하나의 sample (이미지)를 기준으로 다른 클래스의 센터를 비교한다. 
2. Intra Loss: sample과 그 sample이 속해있는 센터와의 거리 -> 가까워져야 한다. 
3. Inter Loss: sample과 다른 클래스의 센터와의 거리 -> 멀어져야 한다.
4. Triplet Loss: sample과 sample끼리의 거리 비교. 총 3개의 sample로 할 수 있다. -> 기준 sample - 같은 클래스 sample, 기준 sample - 다른 클래스 sample


### Methods
cosface와 같이 softmax loss의 심화버전인 손실 함수를 소개하므로 앞의 대부분의 내용은 겹침. loss를 enumerate하여 점점 더 발전시켜나간다. 
![](/assets/images/2021-09-19-Arcface/2.PNG)
그림에서와 같이 표시를 해보면, 논문에 쓰여져 있는 $L_1$, $L_2$, $L_3$는 점점 더 발전해 나가는 것을 알 수 있다.

![](/assets/images/2021-09-19-Arcface/3.PNG)
먼저 $L_1$은 normalization도 하기전의 가장 기본적인 Softmax loss이다.
 
![](/assets/images/2021-09-19-Arcface/4.PNG)
두번째로 $L_2$는 cosface에서도 말했듯이, normalization을 하지 않으면 x의 절댓값만 줄여도 loss가 줄어드므로 원하지 않는 방향으로 업데이트 될 수도 있다. 따라서 이를 방지하기 위해
스케일링을 해줌. 

![](/assets/images/2021-09-19-Arcface/5.PNG)
Arcface loss인 $L_3$는 cosface와 달리 각도를 변화시킨다. cosface가 단지 cosine similarity에 margin을 주는 거였다면, arcface는 각도에 margin을 줌으로써 기하학적인 해석을 가능하게 한다. 

![](/assets/images/2021-09-19-Arcface/6.PNG)
Sphereface, Cosface에서 제시한 방법론과도 연결 시킬수 있다. 두 논문에서 제시한 접근방법을 적용한 loss는 $L_4$와 같다. 하지만 Arcface와 비교해 본 결과 이것또한 좋은 성능을 보인다고 한다. 

Arcface에서 제시한 각도의 margin을 기존의 개념과 연결시켜서 소개한다. 
![](/assets/images/2021-09-19-Arcface/7.PNG)
$L_5$는 Arcface의 각도개념을 Intra-class에 적용시킨 것이다. 앞의 $L_2$는 기본이 되는 normalization만 적용한 것이고, 두번째 항은 각도를 더한다. 즉, 같은 클래스에 있을 때에는 각도의 합이 최소화, 즉 0이 되는 것을 
유도하는 손실함수이다. 하지만 이 손실함수는 다른 클래스일 때에 고려를 하지않는다.

![](/assets/images/2021-09-19-Arcface/8.PNG)
$L_6$는 다른 클래스와의 거리를 최대화 시킨다. 두번째 항을 잘 보면 y와 j가 같지 않다는 것을 알 수 있다. 두 센터의 각도를 arccos함수로 구하고 앞에 음수를 붙여 최대화 시킨다. 하지만 이 역시 
같은 클래스에 대해서는 고려하지 않았다.
 
![](/assets/images/2021-09-19-Arcface/9.PNG)
$L_7$은 기준 sample ($x_i$)를 잡고, 같은 클래스 (positive $x_{pos}$), 다른 클래스 (negative $x_neg$)를 잡아 비교한다. arccos값에 margin을 더해줌으로써 이는 각도르 더하는 것을 일 수 있다.   


# Code
코드로 살펴보자. 주석처리로 메모. 

```python
class ArcFaceLoss(nn.Module):
    def __init__(self, in_features, out_features, eps=1e-7, s=None, m=None):
        super().__init__()
        self.s = 64 if s is None else s
        self.m = 0.5 if m is None else s
        
        self.in_features = in_features
        self.out_features = out_features
        self.fc = nn.Linear(self.in_features, self.out_features)
        self.eps = eps

    def forward(self, x, labels):
        '''
        x: [Batch x in_features]
        '''

        assert len(x) == len(labels)
        assert torch.min(labels) >= 0
        assert torch.max(labels) < self.out_features
        
        # W를 normalize 해준다.
        for W in self.fc.paramters():
            W = F.normalize(W, p=2, dim=1)
            
        wf = self.fc(x)
        #### L_3를 계산 하기 전에 먼저 풀어보자. 분수를 log에 대하여 풀면 (분자) - (분모)가 되고, numberator는 분자부분, excl은 2번째 항의 2번째 페널티 합 (정답 레이블은 포함 안하는 항).

        # 식에서 log(e^(s * cos(\theta + m))을 나타냄. log를 안붙이는건 log와 e가 상쇄되어서.
        numerator = self.s * torch.cos(torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1 + self.eps, 1 - self.eps)) + self.m)
        
        # 패널티 항은 아직 지수 승 안붙는다. 페널티 항의 cos(\theta)를 나타내는 부분.
        penalty_theta = torch.cat([torch.cat((wf[i, :y], wf[i: y+1])).unsqueeze(0) for i, y in enumerate(labels)], dim=0)
        
        # e^(s * cos(\theta + m)) + \sigma{e^(s * cos(\theta))}
        denominator = torch.exp(numberator) + torch.sum(torch.exp(self.s * penalty_theta), dim=1)
        loss = numerator - torch.log(denominator)
        return -torch.mean(loss)
```

또는 2번째 코드 아개 좀 더 직관적이고 쉬운것 같다. 
```python
class ArcFaceLoss(nn.Module):
    def __init__(self, embed_features, out_features, scale_factor=64.0, margin=0.50):
        super(ArcFaceLoss, self).__init__()
        self.embed_features = embed_features
        self.out_features = out_features
        self.criterion = nn.CrossEntropyLoss()

        self.margin = margin
        self.scale_factor = scale_factor

        self.weight = nn.Parameter(torch.FloatTensor(out_features, embed_features))
        nn.init.xavier_uniform_(self.weight)

        self.cos_m = math.cos(margin)
        self.sin_m = math.sin(margin)
        self.th = math.cos(math.pi - margin)
        self.mm = math.sin(math.pi - margin) * margin

    def forward(self, embed_vec, label):
        # 먼저 normalize 해서 cos(\theta)를 구한다.
        cos = F.linear(F.normalize(embed_vec), F.normalize(self.weight))
        sin = torch.sqrt(1.0 - torch.pow(cos, 2))
        
        # 코사인 법칙으로 cos(\theta + m) = cos(\theta) * cos(m) - sin(\theta) * sin(m)
        phi = cos * self.cos_m - sin * self.sin_m
        # 이 코드가 가장 중요한데, 우선 \theta는 pi보다 작다는 가정을 주의하자. 
        # \theta + m < pi일 경우에는 문제가 되지 않기 때문에 그대로 cos(\theta + m)을 사용
        # 그렇지 않은 경우 (\theta + m > pi)에는 테일러 급수로 cos(\theta + m)의 근사를 구할 수 있다. 
        # f(x) = f(a) + \frac{f'(a)}{1!} * (x-a) + \frac{f''(a)}{2!} * (x-a)^2 + ... 로 표현되고, 잘 풀어보면 
        # cos(\theta + m) \sim cos(\theta) - m*sin(\theta)가 된다. 하지만 \theta < pi - m이고, \theta는 거의 pi에 가까우므로 sin그래프를 그려보면
        # sin(\theta) >  sin(pi - m)이 된다. 따라서 위의 근사값은
        # cos(\theta + m) \sim cos(\theta) - m*sin(\theta) > cos(\theta) - m*sin(pi-m) 이 되어 아래 코드의 마지막 매개변수가 되는 것.
        phi = torch.where(cos > self.th, phi, cos - self.mm)

        one_hot = torch.zeros(cos.size(), device=embed_vec.device)
        one_hot.scatter_(1, label.unsqueeze(1).long(), 1)
        logit = (one_hot * phi) + ((1.0 - one_hot) * cos)
        logit *= self.scale_factor
        loss = self.criterion(logit, label)

        return loss
```