---
title: "[NIPS2020]Denoising Diffusion Probabilistic Models"
excerpt: "[NIPS2020]Denoising Diffusion Probabilistic Models"
categories:
  - Paper & Knowledge
  
tags:
  - Diffusion Models
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2022-08-16T15:04:00-05:00
---

![](/assets/images/2022-08-16_DDPM/5.jpg)
![](/assets/images/2022-08-16_DDPM/6.jpg)
![](/assets/images/2022-08-16_DDPM/7.jpg)

최근 생성모델 분야에서 Diffusion Model이 GAN을 뛰어 넘었다는 논문이 등장하였다. 이외에도 Diffusion을 통한 생성이 generative model에서 아주 강력한 elbo를 보여 기본적인 성능이 뛰어난 것을 들었었는데,
이번 기회에 제대로 공부하여 글을 남긴다. 여태껏 읽었던 논문 중 가장 이해하기 힘들고, 그만큼 자료도 찾기 힘들어 읽는데 가장 오래걸린 논문이다. 또한 아주 많은 수학적 배경지식을 요하며,
다른 분들이 말하는 대로 이 논문을 제대로 이해하고 직관력을 기르기 위해서는 모든 수식을 따라가며 증명해 보는 것이 제일 좋다. 또한 논문에서 정리된 수식들을 구현하는 과정이 필수적이다. 증명과 같은 긴 수식을 마크다운으로 적기 굉장히
번거롭기 때문에 수기로 정리하였다. 
코드는 [링크](https://github.com/rlawjdghek/Generative_Models/tree/main/Diffusion_Models/DDPM) 참조.

[나중에 상기할 때 보기.]큰 틀을 정리하면 아래와 같다.
1. $q(\mathbf{x}\_{t-1} \| \mathbf{x}\_t, \mathbf{x}\_0)$과 $p\_{\theta}(\mathbf{x}\_{t-1} \| \mathbf{x}\_t)$를 맞추는 것이 목표
2. $q(\mathbf{x}\_{t-1} \| \mathbf{x}\_t, \mathbf{x}\_0) = \mathcal{N}(\mathbf{x}\_{t-1}; \tilde{\mu}, \tilde{\beta})$가 주어져 있으므로 평균과 분산만 맞추면 된다.
3. 직접적으로 평균 분산을 맞추지 않고 reparametrize를 사용해서 모델이 노이즈를 예측하도록 만든다.

# Abstract & Introduction
Diffusion Model의 가능성을 연 논문이다. Diffusion Probabilistic model은 variational inference를 활용한 매개변수화 된 Markov chain이라 할 수 있다. 이 chain은 노이즈를 입히는 diffusion process (forward process)
의 역을 학습한다. 즉, 어떠한 random noise에서 noise가 없어질 떄까지 어떠한 의미있는 sampling 작업을 유한하게 반복한다. Diffusion model은 훈련하기 효율적이고 직관적이지만, 이 논문이전까지는 GAN 같은 다른 생성 모델에 비하여
높은 퀼리티의 이미지를 생성하지 못하였다. 하지만 Diffusion model은 훌륭한 inductive biase를 보이고 이 연구를 토대로 diffusion model의 sota성능을 기대한다. 

![](/assets/images/2022-08-16_DDPM/1.PNG)
# Background
Diffusion model은 시간 t에 대하여 t=0에서 t=T까지의 상태를 거치는 형태를 가진다. 먼저 reverse process 부터 살펴보자.
### Reverse Process
\begin{equation}
p_{\theta} = \int p_{\theta}(\mathbf{x}_{0:T})d\_{\mathbf{x}\_{0:T}}.
\end{equation}
이 때 중간 상태 $\mathbf{x}_1$, ..., $\mathbf{x}_T$들은 실제 이미지 $\mathbf{x}_0$과 동일한 차원을 갖는다(동일한 shape을 갖는다).

Joint distribution $p\_{\theta}(\mathbf{x}\_{0:T})$는 reverse process라 부르고, 위의 그림과 같이 노이즈를 더해가는 $q$와 달리 노이즈를 점차적으로 제거하는 과정이다.**우리가 모델을 훈련하여 해야하는 과정이다**. 또한 가장 처음 상태는 
표준 가우시안 분포를 따른다고 정의된다. $p(\mathbf{x}\_T) = \mathcal{N}(\mathbf{x}; \mathbf{0}, \mathbf{I})$인 Markov chain이라 정의 된다. Markov chain의 가정에 의하여 $p(x\_{t-1} | x_t) = p(x\_{t-1 | x_{t:T}})$를 사용하면,
$p_{\theta}(\mathbf{x}\_{0:T})=p(\mathbf{x}\_T)\Pi_{t=1}^{T}p\_{\theta}(\mathbf{x}\_{t-1} | \mathbf{x}\_t)$이고 각 조건부 확률은 $p\_{\theta}(\mathbf{x}\_{t-1} | \mathbf{x}\_t) = \mathcal{N}(\mathbf{x}\_{t-1}; \mu\_{\theta}(\mathbf{x}\_t, t), \sum\_{\theta}(\mathbf{x}\_t, t))$.
즉, reverse process $p$에서 각 상태마다 가지는 평균과 분산이 모두 다르고, 이는 우리가 설정한 모델 parameter $\theta$에 의해 추정된다.

### Forward Process
다음으로 reverse process를 학습하기 위하여 사용하는 posterior를 보자. 논문에서는 이 과정을 diffusion process 또는 forward process라 부른다. Posterior $q(\mathbf{x}_{1:T} | \mathbf{x}_0)$는 variance schedule $\beta\_1, ..., \beta\_T$에 따른 가우시안 분포를 계속 더해주는 고정된 Markov chain이다. 즉,
\begin{equation}
q(\mathbf{x}\_{1:T}|\mathbf{x}\_0) = \Pi\_{t=1}^{T}q(\mathbf{x}\_{t} | \mathbf{x}\_{t-1}).
\end{equation}
이 떄 각 조건부 분포는 다음과 같이 정의된다.
\begin{equation}
q(\mathbf{x}\_{t} | \mathbf{x}\_{t-1}) = \mathcal{N}(\mathbf{x}\_{t}; \sqrt(1-\beta\_{t})\mathbf{x}\_{t-1}, \beta\_{t}\mathbf{I})
\end{equation}.

### Training 
훈련은 VAE와 같이 ELBO 개념을 활용하여 negative log likelihood에서의 variational bound를 최적화 하는 방식으로 진행된다.
\begin{equation}
\mathbb{E}[-log p\_{\theta}(\mathbf{x}_0)] =\mathbb{E}\_q[-log\frac{p\_{\theta}(\mathbf{x}\_{0:T})}{q(\mathbf{x}\_{1:T}|\mathbf{x}\_{0})}] = \mathbb{E}\_[-log p(\mathbf{x}\_T) - \sum\_{t \geq 1}log\frac{p\_{\theta}(\mathbf{x}\_{t-1}\mathbf{x}\_{t})}{q(\mathbf{x}\_{t}|\mathbf{x}\_{t-1})}] = L
\end{equation}
이 식은 최종적으로 모델을 학습하기 위한 objective function으로 간소화 된다. 이 식을 풀어 쓰면 아래와 같다.
\begin{equation}
\mathbb{E}\_q[D\_{KL}(q(\mathbf{x}\_T | \mathbf{x}\_0) || p(\mathbf{x}\_T)) + \sum\_{t>1} D\_{KL}(q(\mathbf{x}\_{t-1} | \mathbf{x}\_t, \mathbf{x}\_0)) - log p\_{\theta}(\mathbf{x}\_0 | \mathbf{x}\_1)]
\end{equation}

증명은 아래와 같다.
![](/assets/images/2022-08-16_DDPM/8.jpg)

이제 위의 식을 해석해 보자. 
##### 첫번째 항 
먼저 가장 첫번째 항은 $L\_T$로 표현될 수 있고, 이 논문에서는 variance schedule $\beta$를 상수로 정의하였기 때문에 q에 대해서는 학습 파라미터가 없다. 또한 $p(\mathbf{x}\_T)$는 표준 가우시안 분포이므로 역시 학습할 것이 없다. 즉 $L\_T$ 상수이다.

##### 세번째 항 
$log p\_{\theta}(\mathbf{x}\_0 | \mathbf{x}\_1)$은 논문에서는 $\mathbf{x}\_1$에서 $\mathbf{x}\_0$으로 넘어가는 구간만 MSE를 사용했다고 말하는데 실제 구현상에서는 발견할 수가 없다. 다른 블로그에서도 아직은 이 항에 대하여 자세히 설명한 글도 찾을수 없어 일단은 넘어가자. 

##### 두번째 항 
두번쨰 항이 우리가 실질적으로 학습에 사용하는 항이다. KL divergence에 있는 두 항을 좁히기 위해서는 먼저 두 항을 알아야 한다. 먼저 $q(\mathbf{x}\_{t-1} | \mathbf{x}\_t, \mathbf{x}\_0)$는 아래와 같이 유도된다.
\begin{equation}
q(\mathbf{x}\_{t-1} | \mathbf{x}\_t, \mathbf{x}\_0) = \mathcal{N}(\mathbf{x}\_{t-1}; \tilde{\mu}\_t(\mathbf{x}\_t, \mathbf{x}\_0), \tilde{\beta\_t}I).
\end{equation}
$\tilde{\mu}\_t(\mathbf{x}\_t, \mathbf{x}\_0)=\frac{\sqrt{\bar{\alpha}\_{t-1}}\beta\_t}{1-\bar{\alpha}\_t}\mathbf{x}\_0 + \frac{\sqrt{\alpha\_t}(1 - \bar{\alpha}\_{t-1})}{1 - \bar{\alpha}\_t}\mathbf{x}\_t$, $\tilde{\beta}\_{t} = \frac{1 - \bar{\alpha}\_{t-1}}{1 - \bar{\alpha}\_t}\beta\_t$.

위의 식을 유도하기 위해서는 먼저 $q(\mathbf{x}\_t | \mathbf{x}\_0)=\mathcal{N}(\mathbf{x}\_t;\sqrt{\bar{\alpha}\_t}\mathbf{x}\_0, (1-\bar{\alpha}\_t)\mathbf{I})$를 사용한다. $\alpha\_t=1-\beta\_t$, $\bar{\alpha}\_t = \Pi\_{s=1}^{t}\alpha\_s$로 정의하고, 이러한 $\alpha$, $\beta$들은 모두 상수이며, 구현에서는 __init__함수에 모두 정의해 두고 시작한다. 이 분포의 증명은 아래와 같다.
![](/assets/images/2022-08-16_DDPM/9.jpg)



위의 $\tilde{\mu}\_t(\mathbf{x}\_t$와 $\tilde{\beta}\_{t}$에 대한 증명은 아래와 같다. 

![](/assets/images/2022-08-16_DDPM/10.jpg)

$\tilde{\beta}\_{t}$는 posterior variance라고 명명한다. 


# Diffusion Models and denoising autoencoders
지금까지 diffusion model에 대해서 기본적인 배경지식을 알아보았다. 이 장에서는 위에서 구한 objective function $L\_{t-1}$을 어떻게 딥러닝 학습에 적용하는지에 대하여 중점적으로 다룬다. 

위의 reverse process에서 다루었듯이, 우리가 학습해서 최종적으로 추론에 사용해야 하는 것은 $p\_{\theta}(\mathbf{x}\_{t-1} \| \mathbf{x}\_t)=\mathcal{N}(\mathbf{x}\_{t-1}; \mu\_{\theta}(\mathbf{x}\_t, t), \sum\_{\theta}(\mathbf{x}\_t, t))$이다. 주목할 것은 두개의 변수 $\mu\_{\theta}$와 $\sum\_{\theta}$이다. 

먼저 분산에 대해서는 2가지 선택지가 있다. 첫번째는 $\sigma\_t^2=\beta\_t$, 두번째는 $\sigma\_t^2=\tilde{\beta}\_t$이다. 논문에서는 이 두가지 설정이 비슷한 결과를 도출해낸다고 서술하였고, 실제 구현에서는 두번째 계수를 사용하였다. 

다음으로 이 논문에서 핵심적으로 다루는 $\mu\_{\theta}$이다. 식 (6)번에서 볼 수 있듯이, poterior $q(\mathbf{x}\_{t-1} \| \mathbf{x}\_t)$ 분포의 평균인 $\tilde{\mu}\_t(\mathbf{x}\_t, \mathbf{x}\_0)$를 따라가기 위해서 직관적으로 세울수 있는 손실함수는 아래와 같다. 

\begin{equation}
L\_{t-1} = \mathbb{E}\_q[\frac{1}{2\sigma\_t^2}\parallel\tilde{\mu}\_t(\mathbf{x}\_t, \mathbf{x}\_0) - \mu\_{\theta}(\mathbf{x}\_t, t)\parallel^2] + C
\end{equation}
C는 모델 파라미터 $\theta$에 독립인 상수항이다. 위의 식으로부터 forward proces posterior mean을 따라가는 모델을 설계할 수 있다.

**하지만, 본 논문에서는 직접적으로 평균을 따라가도록 학습하지 않는다.** $q(\mathbf{x}\_{t-1} \| \mathbf{x}\_0)=\mathcal{N}(\mathbf{x}\_t; \sqrt{\tilde{\alpha}\_t}\mathbf{x}\_0, (1-\tilde{\alpha}\_t)\mathbf{I})$를 통하여 $\mathbf{x}\_t$를 Reparametrize하여 $\mathbf{x}\_t(\mathbf{x}\_0, \epsilon)=\sqrt{\tilde{\alpha}\_t}\mathbf{x}\_0 + \sqrt{1-\tilde{\alpha}\_t}\epsilon$, $\epsilon \sim \mathcal{N}(0, \mathbf{I})$를 도출할 수 있다. 이를 대입하여 위의 평균을 맞춰주는 식은 아래와 같다.

\begin{equation}
L\_{t-1} - C = \mathbb{E}\_{\mathbf{x}\_0, \epsilon}[\frac{1}{2\sigma\_t^2}\tilde{\mu}\_t(\mathbf{x}\_t(\mathbf{x}\_0, \epsilon), \frac{1}{\sqrt{\tilde{\alpha}}\_t}\parallel(\mathbf{x}\_t(\mathbf{x}\_0, \epsilon) - \sqrt{1-\tilde{\alpha}\_t}\epsilon)) - \mu\_{\theta}(\mathbf{x}\_t(\mathbf{x}\_0, \epsilon), t)\parallel^2]
\end{equation}
\begin{equation}
= \mathbb{E}\_{\mathbf{x}\_0, \epsilon}[\frac{1}{2\sigma\_t^2}\parallel\frac{1}{\sqrt{\alpha\_t}}(\mathbf{x}\_t(\mathbf{x}\_0, \epsilon) - \frac{\beta\_t}{\sqrt{1-\tilde{\alpha}\_t}}\epsilon) - \mu\_{\theta}(\mathbf{x}\_t(\mathbf{x}\_0, \epsilon), t)\parallel^2]
\end{equation}

증명은 아래와 같다.
![](/assets/images/2022-08-16_DDPM/11.jpg)

식(9)는 모델이 출력한 $\mu\_{\theta}$가 $\frac{1}{\sqrt{\alpha\_t}}(\mathbf{x}\_t(\mathbf{x}\_0, \epsilon) - \frac{\beta\_t}{\sqrt{1-\tilde{\alpha}\_t}}\epsilon)$를 예측해야 한다는 것을 보여준다. 

분명 위의 식 (9)로 모델(모델이 평균을 예측하게 함)을 학습시켜도 학습이 된다. 즉, $\mu\_{\theta}(\mathbf{x}\_t, t) \approx \tilde{\mu}\_t(\mathbf{x}\_t, \mathbf{x}\_0)$을 만족하는데, 여기서 우리는 $\mathbf{x}\_0=\frac{1}{\sqrt{\tilde{\alpha}}\_t}(\mathbf{x}\_t, \sqrt{1-\tilde{\alpha}}\_t\epsilon\_{\theta}(\mathbf{x}\_t))$로 생각할 수 있다. 즉, $\mathbf{x}\_0$을 reparametrize하여 현재 알고 있는 $\mathbf{x}\_t$와, 관점을 바꾸어 모델이 어떠한 noise $\epsilon\_{\theta}$를 예측하도록 하고, 이 예측된 노이즈를 $\mathbf{x}\_t$와 결합하여 실제 이미지 $\mathbf{x}\_0$를 예측한다. 이를 $\mu\_{\theta}(\mathbf{x}\_t, t) \approx \tilde{\mu}\_t(\mathbf{x}\_t, \mathbf{x}\_0)$에 대입하면 아래와 같은 식을 도출할 수 있다.

\begin{equation}
\mu\_{\theta}(\mathbf{x}\_t, t) = \tilde{\mu}\_t(\mathbf{x}\_t, \frac{1}{\sqrt{\tilde{\alpha}\_t}}(\mathbf{x}\_t - \sqrt{1-\tilde{\alpha}\_t}\epsilon\_{\theta}(\mathbf{x}\_t))) = \frac{1}{\sqrt{\alpha\_t}}(\mathbf{x}\_t - \frac{\beta\_t}{\sqrt{1-\tilde{\alpha}\_t}}\epsilon\_{\theta}(\mathbf{x}\_t, t))
\end{equation}

위의 식 (10)번을 활용하여, 식 (9)번에 대입하면 아래와 같은 $\epsilon$으로 reparametrized 된 새로운 손실함수를 얻을 수 있다.
\begin{equation}
\mathbb{E}\_{\mathbf{x}\_0, \epsilon}[\frac{\beta\_t^2}{2\sigma\_t^2\alpha\_t(1 - \tilde{\alpha}\_t)} \parallel\epsilon - \epsilon\_{\theta}(\sqrt{\tilde{\alpha}\_t}\mathbf{x}\_0 + \sqrt{1 - \tilde{\alpha}\_t}\epsilon, t)\parallel]
\end{equation}
이는 단순히 대입하면 도출되므로 증명하지 않는다. 중요한 것은 최종 목표함수는 위와 같이 어떤 $\epsilon$과 비슷하도록 noise $\epsilon\_{\theta}$를 모델이 예측한다는 것이고, 이는 denoising score matching이라 할 수 있다. 

또한 모델이 $\epsilon\_{\theta}$를 예측한다면, 실제 추론(sampling)에서는 $\mathbf{x}\_{t-1} \sim p\_{\theta}(\mathbf{x}\_{t-1} \| \mathbf{x}\_t)$를 추출하게 되고,$p\_{\theta}(\mathbf{x}\_{t-1} \| \mathbf{x}\_t) = \mathcal{N}(\mathbf{x}\_{t-1}; \mu\_{\theta}(\mathbf{x}\_t, t), \sum\_{\theta}(\mathbf{x}\_t, t))$에 의하여 $\mathbf{x}\_{t-1}=\frac{1}{\sqrt{\alpha\_t}}(\mathbf{x}\_t - \frac{\beta\_t}{\sqrt{1-\tilde{\alpha}\_t}}\epsilon\_{\theta}(\mathbf{x}\_t, t)) + \sigma\_t z, z \sim \mathcal{N}(0, \mathbf{I})$로 구할 수 있다. 이 식들이 논문의 알고리즘 2에 for 문안에 있는것을 확인할 수 있다. 

![](/assets/images/2022-08-16_DDPM/2.PNG)

샘플링의 전체적인 흐름을 정리한 것은 포스팅 가장 위에서 3번째에 있는 도식이다. 구현상으로 간단히 정리하자면 아래와 같다. 

```python
    def q_posterior(self, x_0, x_t, t):  # mu, var, log_var 반환
        mu = extract(self.C.posterior_mean_coef1, t, x_t.shape) * x_0 + \
             extract(self.C.posterior_mean_coef2, t, x_t.shape) * x_t
        var = extract(self.C.posterior_variance, t, x_t.shape)
        log_var = extract(self.C.posterior_log_variance_clipped, t, x_t.shape)
        return mu, var, log_var
    def p_mean_variance(self, x_t, t, x_self_cond=None, clip_denoised=True):  # 샘플링에 필요한 mu, log_var와 self_condition을 위한 pred_x_0를 반환
        preds = self.model_prediction(x_t, t, x_self_cond)
        pred_x_0 = preds["pred_x_0"]
        if clip_denoised:
            pred_x_0.clip_(-1.0, 1.0)
        mu, var, log_var = self.q_posterior(pred_x_0, x_t, t)
        return mu, var, log_var, pred_x_0
    @torch.no_grad()
    def p_sample(self, x_t, t, x_self_cond=None, clip_denoised=True):  # 논문 algorithm2 for문 1 step, pred_x_0은 self_condition을 위함
        BS = x_t.shape[0]
        time = torch.full([BS, ], t, dtype=torch.long).cuda(x_t.get_device())
        mu, _, log_var, pred_x_0 = self.p_mean_variance(x_t, time, x_self_cond=x_self_cond, clip_denoised=clip_denoised)
        z = torch.randn_like(x_t).cuda(x_t.get_device()) if t > 0 else 0.0
        x_t_1 = mu + (0.5 * log_var).exp() * z  # 결과적으로는 sigma가 된다. 
        return x_t_1, pred_x_0        
    @torch.no_grad()
    def p_sample_loop(self, shape):  # 논문의 Algorithm 2
        x = torch.randn(shape).cuda(self.args.local_rank)  # x_T
        pred_x_0 = None
        for t in reversed(range(0, self.n_timesteps)):
            x_self_cond = pred_x_0 if self.args.self_condition else None
            x, pred_x_0 = self.p_sample(x_t=x, t=t, x_self_cond=x_self_cond)
        gene_img = x
        self.gene_img = gene_img
```

### Simplified Training Objective 
식 (11)번에서 우리는 reparametrize를 통하여 분포의 평균을 직접적으로 예측하지 않고, $\epsilon$값을 예측함으로써 denoising score matching으로 접근하였다. 여기서 눈여겨 볼 것은 식 앞에 있는 계수, $\frac{\beta\_t^2}{2\sigma\_t^2\alpha\_t(1 - \tilde{\alpha}\_t)}$인데, 저자들은 실험적으로 이 계수를 없애는 것이 더 높은 성능을 달성하는데 도움이 되었다고 한다. 추정되는 이유로는, t값이 커질수록 계수가 작기 때문에 절댓값 안의 에러값이 무시될 수 있지만, 직관적으로 생각해보면, t가 클 때, 즉 noise가 심할 때 더 많은 노이즈를 제거하는 것이 올바른 방향이다. 따라서 계수를 없앰으로써, t가 작을떄 보다 t가 클 때 상대적으로 더 큰 손실값을 만들어 내고, 우리의 직관대로 t가 클 떄 더 많은 노이즈를 제거하게 된다. 최종 손실 함수는 아래와 같다.
\begin{equation}
\mathbb{E}\_{t, \mathbf{x}\_0, \epsilon}[\parallel\epsilon - \epsilon\_{\theta}(\sqrt{\tilde{\alpha}\_t}\mathbf{x}\_0 + \sqrt{1-\tilde{\alpha}\_t}\epsilon, t)\parallel^2]
\end{equation}

위의 손실 함수를 기반으로 실제 구현에서 훈련 과정을 순서대로 설명하면 아래와 같다. 이는 논문의 알고리즘 1과도 같다.
1. real sample, t, noise를 샘플링한다.
2. $\mathbf{x}\_t=\sqrt{\tilde{\alpha}\_t}\mathbf{x}\_0 + \sqrt{1 - \tilde{\alpha}\_t}\epsilon$을 계산 (코드에서 self.q_sample 함수)
3. $\mathbf{x}\_t$, $t$를 Generator에 대입하여 1번에서 샘플링한 $\epsilon$과 L1 loss

코드는 아래 참조
```python
    def train(self):
        BS, C, H, W = self.x_0.shape
        t = torch.randint(0, self.args.n_timesteps, (BS,), device=self.x_0.get_device()).long()
        noise = torch.randn_like(self.x_0) if self.noise is None else self.noise
        x = self.q_sample(x_start=self.x_0, t=t, noise=noise)
        
        x_self_cond = None
        if self.args.self_condition and random.random() < 0.5:
            with torch.no_grad():
                x_self_cond = self.model_prediction(x, t)["pred_x_0"].detach()
        
        gene_x = self.G(x, t, x_self_cond)
        
        if self.args.model_objective == "pred_noise":
            target = noise
        elif self.args.model_objective == "pred_x0":
            target = self.x_0
        loss = self.criterion(gene_x, target)
        loss = torch.mean(loss, dim=list(range(loss.ndim))[1:])
        loss = (loss * extract(self.C.p2_loss_weight, t, loss.shape)).mean()

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
```

# Experiments
### Progressive generation
재밌는 실험중 하나는 reverse process를 진행하는데에서 t의 시작점(예를 들어 T=250 일 때, 시작 이미지가 표준 정규분포인지, 아니면 거의 완성된 이미지인지)에 따라 결과가 어떻게 달라지는지이다. 

![](/assets/images/2022-08-16_DDPM/3.PNG)
위의 그림을 볼 때, t가 큰 곳에서 노이즈에 가까운 이미지를 입력으로 한다면 완전히 다른 사람들이 생성되는 반면, 어느정도 구색이 잡혔다면 동일한 이미지가 생성되는 것을 볼 수 있다. 즉, 이미지의 전체적인 구조를 잡는 것은 노이즈에 가까운 t가 큰 구간에서 발생하고, 나머지는 fine-grained feature를 잡는데 도움을 준다. 

### Interpolation
GAN과 마찬가지로, $\mathbf{x}\_t$를 잡을때 두개의 상태 $\mathbf{x}\_1$, $\mathbf{x}\_2$를 선형 보간하여 중간의 이미지들을 만들수 있다. 아래 그림을 보자.
![](/assets/images/2022-08-16_DDPM/4.PNG)
보간하는 알고리즘은 아래와 같다.
1. 두 이미지 $\mathbf{x}\_1$, $\mathbf{x}\_2$를 선정.
2. forward pass를 일정스텝 t만큼 적용하여 $\mathbf{x}\_t^{\prime} \sim q(\mathbf{x}\_t \| \mathbf{x}\_0)$ 추출.
3. $\bar{\mathbf{x}\_t} = \lambda\mathbf{x}\_1^{\prime} + (1 - \lambda)\mathbf{x}\_2^{\prime}$
4. reverse process를 통하여 보간된 이미지 $\bar{\mathbf{x}\_t}$ 추출.
















