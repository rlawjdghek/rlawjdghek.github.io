---
title:  "평가 지표 모음"
excerpt: "평가 지표 모음"
categories:
  - Paper & Knowledge
  
tags:
  - Paper & Knowledge
  
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2021-07-11T15:48:00-05:00
---

|---|---|---|
||positive(1)_gt|negative(0)_gt|
|positive(1)_pred|TP|FP|
|negative(0)_pred|FN|TN|

precision = TP / (TP + FP)

recall = TP / (TP + FN)

specificity (특이도) = recall = TPR = TP / (TP + FN) 

sensitivity (민감도) = TN / (TN + FP)

FPR = FP / (FP + TN) = 1 - sensitivity => false 중에 false_pred의 비율

ROC curve는 pred를 정렬하고 하나씩 경계를 그으면서 y축은 TPR, x축은 FPR로 하는 것. 어떤건 FPR = 1 - sensitivity이므로 x을 1-sensitivity라고 하는 것도 있다.
roc를 그릴땐, x축을 sensitive라고 하고 y축을 TPR이라고 할 때, 그리기 편하려고 x축을 1-sensitivity라고 하는 경우도 있음. 이게 맞는 것 같다.

F1-score = 2 * (precision * recall) / (precision + recall)

### PSNR
![](/assets/images/2021-07-11_metrics/1.JPG)

### SSIM
![](/assets/images/2021-07-11_metrics/2.JPG)

코드는 아래와 같다. **이거는 공부용으로 보고 실제 구현에서는 pip install pytorch-msssim을 사용하자.**
```python
from math import exp
import torch
import torch.nn.functional as F
def _psnr(output, target):
    '''
    :param output: tensor with 0 ~ 1 float [B x C x H x W]
    :param target: tensor with 0 ~ 1 float [B x C x H x W]
    '''
    assert output.shape == target.shape, "input image shape must have same shape"
    output = torch.clip(output, 0, 1)
    target = torch.clip(target, 0, 1)
    mse = torch.mean((output - target) ** 2)
    return 20 * torch.log10(1.0 / torch.sqrt(mse))
def gaussian(window_size, sigma):
    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])
    return gauss/gauss.sum()
def create_window(window_size, channel=1):
    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)
    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()
    return window
def ssim(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):
    # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).
    if val_range is None:
        if torch.max(img1) > 128:
            max_val = 255
        else:
            max_val = 1

        if torch.min(img1) < -0.5:
            min_val = -1
        else:
            min_val = 0
        L = max_val - min_val
    else:
        L = val_range

    padd = 0
    (_, channel, height, width) = img1.size()
    if window is None:
        real_size = min(window_size, height, width)
        window = create_window(real_size, channel=channel).to(img1.device)

    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)
    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)

    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2

    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq
    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq
    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2

    C1 = (0.01 * L) ** 2
    C2 = (0.03 * L) ** 2

    v1 = 2.0 * sigma12 + C2
    v2 = sigma1_sq + sigma2_sq + C2
    cs = v1 / v2  # contrast sensitivity

    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)

    if size_average:
        cs = cs.mean()
        ret = ssim_map.mean()
    else:
        cs = cs.mean(1).mean(1).mean(1)
        ret = ssim_map.mean(1).mean(1).mean(1)

    if full:
        return ret, cs
    return ret
def msssim(img1, img2, window_size=11, size_average=True, val_range=None, normalize=None):
    device = img1.device
    weights = torch.FloatTensor([0.0448, 0.2856, 0.3001, 0.2363, 0.1333]).to(device)
    levels = weights.size()[0]
    ssims = []
    mcs = []
    for _ in range(levels):
        sim, cs = ssim(img1, img2, window_size=window_size, size_average=size_average, full=True, val_range=val_range)

        # Relu normalize (not compliant with original definition)
        if normalize == "relu":
            ssims.append(torch.relu(sim))
            mcs.append(torch.relu(cs))
        else:
            ssims.append(sim)
            mcs.append(cs)

        img1 = F.avg_pool2d(img1, (2, 2))
        img2 = F.avg_pool2d(img2, (2, 2))

    ssims = torch.stack(ssims)
    mcs = torch.stack(mcs)

    # Simple normalize (not compliant with original definition)
    # TODO: remove support for normalize == True (kept for backward support)
    if normalize == "simple" or normalize == True:
        ssims = (ssims + 1) / 2
        mcs = (mcs + 1) / 2

    pow1 = mcs ** weights
    pow2 = ssims ** weights

    # From Matlab implementation https://ece.uwaterloo.ca/~z70wang/research/iwssim/
    output = torch.prod(pow1[:-1]) * pow2[-1]
    return output
class PSNR(nn.Module):
    def __init__(self):
        super(PSNR, self).__init__()
        self.name = "PSNR"

    def forward(self, img1, img2):
        assert img1.shape == img2.shape, "input image shape must have same shape"
        return _psnr(img1, img2)
class SSIM(torch.nn.Module):
    def __init__(self, window_size=11, size_average=True, val_range=None):
        super(SSIM, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.val_range = val_range

        # Assume 1 channel for SSIM
        self.channel = 1
        self.window = create_window(window_size)

    def forward(self, img1, img2):
        (_, channel, _, _) = img1.size()

        if channel == self.channel and self.window.dtype == img1.dtype:
            window = self.window
        else:
            window = create_window(self.window_size, channel).to(img1.device).type(img1.dtype)
            self.window = window
            self.channel = channel

        return ssim(img1, img2, window=window, window_size=self.window_size, size_average=self.size_average)
class MSSSIM(torch.nn.Module):
    def __init__(self, window_size=11, size_average=True, channel=3):
        super(MSSSIM, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.channel = channel

    def forward(self, img1, img2):
        # TODO: store window between calls if possible
        return msssim(img1, img2, window_size=self.window_size, size_average=self.size_average)
```



### Inception Score
- IS Score는 다음에 같이 계산 가능하다.
- x_i를 ㅑ번째 영상데이터, y를 레이블, i번째 영상 데이터를 Inception 모델 입력해 얻을수 있는 레이블 y의 확률을 $P(y|x_i)$.
- 스코어 계산에 사용되는 전체 영상 데이터를 X로 정의하면 주변확률을 다음식과 같음.
![](/assets/images/2021-07-11_metrics/3.JPG)

```python
import torch
import torchvision
import torchvision.transforms as T
import matplotlib.pyplot as plt
from torchvision.models import inception_v3

transform = T.Compose([
    T.Resize((299,299)),
    T.ToTensor()
])
dataset = torchvision.datasets.CIFAR10(root="/data", download=False, transform=transform)
loader = torch.utils.data.DataLoader(dataset, batch_size=4)

img, label = next(iter(loader))

img = img.cuda()

def inception_score(image, model):
    model = model.cuda()
    p_yx= torch.softmax(model(image)[0], 1)  # 수식에서의 조건부 확률 p(y|x)에 해당된다. [batch_size, 1000]
    p_y = torch.mean(p_yx, axis=0)  # 수식에서의 p(y)에 해당된다. 조건부 확률을 모두 평균때린것을 수식적으로 보면 유도 할 수 있다. [1000,]
    eps = 1e-8
    D_KL = torch.sum(p_yx * torch.log(p_yx/(p_y+eps)), axis=1)  # sum까지 해주는 것을 잊지 말자. 원래 KL Divergence에도 시그마 들어있음.
    e = torch.mean(D_KL, axis=0)  # 마지막 E(기댓값) 계산.
    return torch.exp(e)


model = inception_v3(pretrained=True).cuda()
aa = inception_score(img, model)
print(aa)
```

### Frechet Inception Distance (FID)
- FID는 생성된 영상의 품질을 평가하는데 사용
- 이 지표는 영상 집합 사이의 거리를 나타낸다. 
- IS는 집합 그자체의 우수함을 표현하는 score이므로, 입력으로 한 가지 클래스만 입력한다.
- FID는 GAN을 사용해 생성된 영상의 집합과 실제 생성하고자 하는 클래스 데이터의 분포의 거리를 계산한다.
- 거리가 가까울수록 좋은 영상으로 판단한다.
- Inception 네트워크를 사용하여 중간 Layer에서 feature를 추출한다.
- 이후, 이 feature에서 평균 $\mu$와 공분한 $\Sigma$를 추출하여 계산.
- 실제 영상 x와 생성된 영상 g사이의 FID는 다음 식으로 계산
![](/assets/images/2021-07-11_metrics/4.JPG)

### Learned Perceptual Image Patch Similarity (LPIPS)
자세한 설명은 논문을 확인해야 한다. 간단히 정리된 글에 의하면 신경망 모델에서 추출되는 특성을 이용하여 학습에 의하여 사람의 인지적 특성에 맞도록 유사도를 평가한다고 한다.
코드는 아래와 같음. (돌아가지는 않는다. 사용시 풀 코드 참조.)<br/>
forward 함수를 보면
1. GT와 생성 이미지를 scaling_layer에 통과시킴.
2. 모델을 통과시킴.
3. 모델에서 feature를 뽑고 normalization 진행한 것의 차를 구해서 저장. 
4. 그 차를 다시 어떤 레이어를 통과시키고, 합을 구함.

```python
class LPIPS(nn.Module):
    def __init__(self, pretrained=True, net='alex', version='0.1', lpips=True, spatial=False, 
        pnet_rand=False, pnet_tune=False, use_dropout=True, model_path=None, eval_mode=True, verbose=True):
        """ Initializes a perceptual loss torch.nn.Module
        Parameters (default listed first)
        ---------------------------------
        lpips : bool
            [True] use linear layers on top of base/trunk network
            [False] means no linear layers; each layer is averaged together
        pretrained : bool
            This flag controls the linear layers, which are only in effect when lpips=True above
            [True] means linear layers are calibrated with human perceptual judgments
            [False] means linear layers are randomly initialized
        pnet_rand : bool
            [False] means trunk loaded with ImageNet classification weights
            [True] means randomly initialized trunk
        net : str
            ['alex','vgg','squeeze'] are the base/trunk networks available
        version : str
            ['v0.1'] is the default and latest
            ['v0.0'] contained a normalization bug; corresponds to old arxiv v1 (https://arxiv.org/abs/1801.03924v1)
        model_path : 'str'
            [None] is default and loads the pretrained weights from paper https://arxiv.org/abs/1801.03924v1
        The following parameters should only be changed if training the network
        eval_mode : bool
            [True] is for test mode (default)
            [False] is for training mode
        pnet_tune
            [False] keep base/trunk frozen
            [True] tune the base/trunk network
        use_dropout : bool
            [True] to use dropout when training linear layers
            [False] for no dropout when training linear layers
        """

        super(LPIPS, self).__init__()
        if(verbose):
            print('Setting up [%s] perceptual loss: trunk [%s], v[%s], spatial [%s]'%
                ('LPIPS' if lpips else 'baseline', net, version, 'on' if spatial else 'off'))

        self.pnet_type = net
        self.pnet_tune = pnet_tune
        self.pnet_rand = pnet_rand
        self.spatial = spatial
        self.lpips = lpips # false means baseline of just averaging all layers
        self.version = version
        self.scaling_layer = ScalingLayer()

        if(self.pnet_type in ['vgg','vgg16']):
            net_type = pn.vgg16
            self.chns = [64,128,256,512,512]
        elif(self.pnet_type=='alex'):
            net_type = pn.alexnet
            self.chns = [64,192,384,256,256]
        elif(self.pnet_type=='squeeze'):
            net_type = pn.squeezenet
            self.chns = [64,128,256,384,384,512,512]
        self.L = len(self.chns)

        self.net = net_type(pretrained=not self.pnet_rand, requires_grad=self.pnet_tune)

        if(lpips):
            self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)
            self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)
            self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)
            self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)
            self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)
            self.lins = [self.lin0,self.lin1,self.lin2,self.lin3,self.lin4]
            if(self.pnet_type=='squeeze'): # 7 layers for squeezenet
                self.lin5 = NetLinLayer(self.chns[5], use_dropout=use_dropout)
                self.lin6 = NetLinLayer(self.chns[6], use_dropout=use_dropout)
                self.lins+=[self.lin5,self.lin6]
            self.lins = nn.ModuleList(self.lins)

            if(pretrained):
                if(model_path is None):
                    import inspect
                    import os
                    model_path = os.path.abspath(os.path.join(inspect.getfile(self.__init__), '..', 'weights/v%s/%s.pth'%(version,net)))

                if(verbose):
                    print('Loading model from: %s'%model_path)
                self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)          

        if(eval_mode):
            self.eval()

    def forward(self, in0, in1, retPerLayer=False, normalize=False):
        if normalize: # turn on this flag if input is [0,1] so it can be adjusted to [-1, +1]
            in0 = 2 * in0  - 1
            in1 = 2 * in1  - 1

        # v0.0 - original release had a bug, where input was not scaled
        in0_input, in1_input = (self.scaling_layer(in0), self.scaling_layer(in1)) if self.version=='0.1' else (in0, in1)
        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)
        feats0, feats1, diffs = {}, {}, {}

        for kk in range(self.L):
            feats0[kk], feats1[kk] = lpips.normalize_tensor(outs0[kk]), lpips.normalize_tensor(outs1[kk])
            diffs[kk] = (feats0[kk]-feats1[kk])**2

        if(self.lpips):
            if(self.spatial):
                res = [upsample(self.lins[kk](diffs[kk]), out_HW=in0.shape[2:]) for kk in range(self.L)]
            else:
                res = [spatial_average(self.lins[kk](diffs[kk]), keepdim=True) for kk in range(self.L)]
        else:
            if(self.spatial):
                res = [upsample(diffs[kk].sum(dim=1,keepdim=True), out_HW=in0.shape[2:]) for kk in range(self.L)]
            else:
                res = [spatial_average(diffs[kk].sum(dim=1,keepdim=True), keepdim=True) for kk in range(self.L)]

        val = 0
        for l in range(self.L):
            val += res[l]
        
        if(retPerLayer):
            return (val, res)
        else:
            return val

```
### Natural Image Quality Evaluator (NIQE)
NIQE의 기본적 작동원리를 한문장으로 정리하자면 다음과 같다. "깨끗한 이미지들이 평균적으로 어떤 특성을 갖는지 살펴본 후, 그것을 테스트 이미지의 특성과 비교해서, 둘이 비슷할수록 테스트 이미지의 품질이 좋은 것으로 합시다."<br/>
우선 125장의 깨끗한 이미지를 mean subtraction and contrast normalization(MSCN) 처리를 해준다. MSCN처리는 7x7 가우시안 커널을 만들고, 이 커널로 이미지에 컨볼루션 연산을 한다. 그 다음 그 7x7 지역의 표준편차값을 모든 픽셀에 대하여 
찾고, 픽셀마다 (픽셀 값 - 픽셀 지역평균) / 픽셀 지역 표준편차 연산을 해준다. 그러면 그 이미지의 분포는 정규분포를 따른다.<br/>
MSCN처리를 끝낸 뒤에, 그 이미지에서 PxP사이즈의 패치로 분할하는데, 이 분할하는 과정에서 이미지의 중요한 객체만 패치화 해준다. 그 다음, 이 패치 안에서 BRISQUE를 찾는데, BRISQUE는 MSCN 처리된 이미지 히스토그램에 일반화된 
가우시안 분포 (Generalized Gaussian Distribution; GGD)를 매칭시켜서 형태에 대한 정보를 특성으로 활용한다. GGD의 식은 아래와 같다.
![](/assets/images/2021-07-11_metrics/5.JPG)
즉, 주어진 MSCN처리된 이미지의 히스토그램과 비슷한 분포를 가진 GGD 분포를 mean=$\alpha$, variance=$\sigma^2$ 값을 변경하면서 비슷한 값을 찾는것이다. 이 mean, variance를 찾고 이 값들로 SVM을 훈련시켜 이미지의 품질을 예측한다.<br/>
다시 돌아와 이렇게 찾은 특성으로 BRISQUE를 거치면 생성이미지에 대하여 패치갯수만큼의 특성벡터가 나오고, GT 이미지에 대하여 패치갯수만큼의 특성벡터가 나온다. 각각 평균 벡터와 공분산 행렬을 계산하고 아래 식으로 계산한다. 거리개념이므로 낮을 수록 
좋다. 
![](/assets/images/2021-07-11_metrics/6.JPG)
자연적 이미지 패치에서의 특성들의 분포 벡터로 만들고 평균과 공분산행렬을 구한다. 그 다음 생성이미지에서도 똑같은 연산을 진행한 뒤에 둘이 비교. 
