---
title: "[CVPR2021]The Spatially-Correlative Loss for Various Image Translation Tasks"
excerpt: "[CVPR2021]The Spatially-Correlative Loss for Various Image Translation Tasks"
categories:
  - Paper & Knowledge
  
tags:
  - GAN
 
published: True
toc: true
toc_sticky: true
toc_label: "On this page"
use_math: true
    
last_modified_at: 2023-01-19T15:04:00-05:00
---

![](/assets/images/2023-01-19-Sesim/1.jpg)
![](/assets/images/2023-01-19-Sesim/2.jpg)

i2i는 style과 content를 분리하여, content를 유지하고, style (=appearance)를 변경하는 task이다. 본 논문에서는 domain에 상관없이 하나의 query 패치에 대하여 다른 패치들과의 similarity를 구한 similarity map을 활용하여 content를 유지하는 훈련 방법을 소개한다. 중간에 CUT과 같이 contrastive loss가 등장하고, 얼핏보면 방법론이 CUT과 비슷하여 CUT의 확장으로 볼 수도 있으나, Sesim은 다양한 GAN 프레임워크에 content를 유지하는 loss를 추가하는 방법론을 제시하였다. 또한, content을 유지하는 loss를 소개하였기 때문에 기존의 i2i 모델을 효율적으로 변형도 가능하다는 측면이 i2i translation 프레임워크인 CUT과의 차이점이다. 본 논문에서는 기존과 달리 오직 content의 structure를 판단하는 similarity map을 활용하엿고, 이는 효과적으로 structural representation을 보존한다. 단, 코드는 CUT을 기반으로 작성되었다. 개인적으로는, 얼룩말 -> 말 과같은 appearance차이만 있는 i2i task에서 굉장히 효과적인듯 하다. 비슷한 structure을 가진 이미지 변환에서는 좋으나, 개->고양이와 같은 task에서는 잘 안될 듯 하다. 또한 실험 파트에서 제안하는 방법이 fidelity와 diversity trade-off를 보여주는 경향도 있다. 

# Abstract & Introduction
i2i에서 content를 유지하는 방법들중 여러개를 뽑아보자면,
1. pixel level에서 비교하는 cycleGAN
2. perceptual distance에서 비교하는 style transfer
3. feature map의 patch에서 contrastive learning으로 비교하는 CUT 등이 있다. 
본 논문의 analysis 섹션에서도 등장하지만, 위의 3가지 대표 content-preserving loss들은 모두 도메인이 다르지만, content가 일치하는 semantic segmentation등의 변환에서 여전히 큰 error를 보인다. 본 논문에서는 feature map에서 한 차원 더 나아가 패치끼리의 similarity map을 content preserving loss로 한다. 이러한 spatially-correlative loss는 도메인에 관계없이 이미지의 spatial relationship만을 인지하고, domain-specific appearance는 무시한다. 

# Method 
Sesim에서는 패치끼리의 similarity를 비교하므로, 패치를 뽑는것은 CUT과 같다. 메인 코드도 CUT을 기반으로 짜여져 있으나, PatchNCE는 사용되지 않았다는 사실에 유념하자. 본 논문에서는 pre-trained VGG를 사용하여 similarity를 뽑는 FSesim을 먼저 소개하고, VGG를 훈련 가능하게 만들고, Fsesim loss에 더하여 augmentation된 이미지의 target domain의 unpaired image를 contrastive loss로 학습하는 LSesim을 소개한다. FSesim의 알고리즘을 나열하면 아래와 같다.
1. VGG를 사용하기 때문에 imagenet으로 정규화한다.
2. 4,7,9번째 weight output을 feature로 사용한다. (Multi-scale)
3. feat src와 feat tgt을 평균과 C로 norm해준뒤, np개의 패치를 먼저 src에 대해서 랜덤으로 뽑는다.
4. query는 뽑힌 np개의 패치 각각이고, key는 각 query 주변의 $pw\times ph$개이다. 코드에서는 feature map을 $BS \times C \times W \times H$로 나타내었다. key를 뽑을때 feature map 내에 속하도록 하는 코드에 유념하자. 결과적으로 query = $[(BS\times np) \times 1 \times C]$, key = $[(BS \times np) \times C \times (pw \times ph)]$의 shape을 갖는다.
5. query와 key를 곱해서 similarity map을 갖는다. shape은 $[BS \times np \times (pw\times ph)]$이다.
6. tgt에 대해서도 src와 같은 위치에서 patch를 뽑고 similarity map을 계산한다.
7. tgt similarity map에 대해서 similiarity가 작은 순서대로 3/4은 0으로 지워버린다. 
8. src, tgt 두 similarity map의 cosine similarity를 구하고, 같은 map은 1, 다른 map은 0으로 L1 loss를 준다. (cosine distance와 같다.)

Fsesim은 pretrained를 사용하므로 유연성이 부족하다. 논문에서는 특정 task에서 structure representation을 학습할 수 없다고 한다. 결국 어느 방법론을 어느 task에든 적용하기 위해서 end-to-end학습을 도입해야 한다. 따라서 논문에서는 두번째로 LSesim을 제안한다. 

LSesim에서는 geneB외에도 실제 데이터를 augmentation시켜 negative를 만드는 방법을 제안한다. 

LSesim의 알고리즘을 나열하면 아래와 같다.
1. src와 target, target2를 설정한다. 코드에서 target2는 other과 같고, src는 [realA, realA], tgt는 [geneB, augA], other는 [realB, augB]이다. tgt은 content가 같고, other은 style과 content 모두 다르다. 
2. FSesim과 같이 similarity map을 구한다.
3. 정규화를 한번 거친다음, src, tgt, other에 대해서 각 쌍의 similarity를 구한다. 총 3개의 쌍이 나오는데, src와 tgt쌍의 같은 위치에 있는 패치가 positive이다.
4. 3개의 쌍에 대한 similarity 텐서는 $[2BS \times np \times 3np]$이다. 이것과 arange(0, np)와의 CE를 구한다.

따라서 최종 손실함수는 아래와 같다.
![](/assets/images/2023-01-19-Sesim/5.PNG)
![](/assets/images/2023-01-19-Sesim/3.PNG)
![](/assets/images/2023-01-19-Sesim/4.PNG) 

$L_G$를 보면 Discriminator는 결국 생성 이미지의 style을 변형하고, 그 뒤의 FSesim에서 제시한 simiarity loss가 content를 유지한다. 

### Analysis 
![](/assets/images/2023-01-19-Sesim/6.PNG) 
위의 그림은 input이 주어졌을 때, gt와 다른 이미지와의 차이를 비교한다. align이 맞으면 낮은에러를, unalign이면 높은 에러를 보여주어야한다. 단순 pixel loss는 당연히 다른 domain에서 높은 에러를 보이고, pretrained network를 사용한 직접적인 feature 비교, perceptual loss도 높은 에러를 보인다. CUT에서 제시한 patchNCE는 패치간 cosine distance를 활용하였으나, align에서 높은 에러를 보인다. 이는 서로 다른 domain에 대하여 패치에 style 정보가 섞여 있기 때문이다. 반면, similarity map에서는 서로 다른 domain의 이미지라도 더욱 효과적으로 semantic content 정보를 추출할 수 있다. 그 결과, semantic segmentation과 real image간의 similarity distance는 align에서 굉장히 낮고, unalign에서는 높다. 

### Discussion
Sesim에서 보여준 핵심 방법론은, feature map을 그대로 사용하는 것이 아닌, feature map에 있는 기준 패치와 다른 패치간의 similarity를 비교했다는 것이다. 즉, style-content를 나누기 위해서 content를 더욱 근본적으로 도출한 similarity map을 사용하였고, 이는 보다 low-level인 pixel-level 비교나 feature map (e.g. perceptual loss, PatchNCE loss) 비교에 비하여 domain에 관계없는 high-level의 접근을 가능하게 한다. 

# Experiments
Single Modal (말 -> 얼룩말, semantic segmentation -> real image), Multi Modal (Night -> Day), Single Image (painting -> photo)에서 수행함. 각각 CycleGAN, MUNIT, DRN을 베이스로 잡고 Sesim loss를 추가하였는데, 주목할점은 CycleGAN에서 auxiliary generator와 discriminator 없이 구현했다는 것이다. 

각 실험에서 공통점은, 말 -> 얼룩말과 같은 content가 거의 유사한 task에 대해서는 성능이 좋다. 이는 높은 FID로도 연결 되는데, 반면에, multi modal의 winter -> summer나 night->day에서는 LPIPS가 낮아진다. 또한 FSesim이 LSesim보다 낮은 diversity를 보이는데, content를 유지하는 성격이 강하다 보니, diversity에서 그만큼의 성능 하락이 있는 것으로 보인다. 

### Ablation Study
![](/assets/images/2023-01-19-Sesim/7.PNG) 

위의 fidelity vs diversity의 비슷한 맥락으로, 이는 ablation에서 l1 loss와 cosine loss를 비교한 결과에서도 보인다. 논문에서는 cosine loss를 사용했을 때 더 높은 FID를 보이는데, cosine이 content를 유지하는데에 조금 더 느슨한 특성을 지니기 때문이라 추측된다. 

위의 표에서 또한 주목할 점은, config. B의 night->day에서 굉장히 낮은 FID를 보이는 것이다. global attention을 사용하였을 경우, local 보다 많은 영역의 similarity를 계산하게 되는데, 이 과정에서 아래와 같이 similarity map이 object에 몰리지 않고 노이즈한 경향을 보인다. 
![](/assets/images/2023-01-19-Sesim/8.PNG) 

Sesim도 결과적으로는 content 정보를 포함하는 representation 거리를 최소화하는 것이기 때문에 이 영역을 넓힌다면, 더욱 많은 객체를 담고 있는 이미지에서는 낮은 FID를 보인다. 