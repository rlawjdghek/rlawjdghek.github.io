<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>StableVITON</title>
<link href="./StableVITON_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./StableVITON_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./StableVITON_files/jquery.js"></script>

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
</head>

<body>

<div class="content">
  <h1><strong>StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On</strong></h1>
  <p id="authors"><a href="https://scholar.google.co.kr/citations?user=ucoiLHQAAAAJ&hl=ko">Jeongho Kim</a><a href="https://www.linkedin.com/in/gyojung-gu-29033118b/">Gyojung Gu</a><a href="https://pmh9960.github.io/">Minho Park</a><a href="https://psh01087.github.io/">Sunghyun Park</a><a href="https://sites.google.com/site/jaegulchoo/">Jaegul Choo</a><br>
    <br>
  <span style="font-size: 24px">KAIST
  </span></p>
  <br>
  <img src="./StableVITON_files/teaser-teaser_v2.drawio.png" class="teaser-gif" style="width:100%;"><br>
    <font size="+2">
          <p style="text-align: center;font-size: 25px;">
            <a href="https://arxiv.org/abs/2312.01725" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
	        <a href="https://github.com/rlawjdghek/StableVITON" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="StableVITON_files/bibtex.txt" target="_blank">[BibTeX]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://youtu.be/x3k4u7F4ReA" target="_blank">[Demo Video]</a>            
          </p>
    </font>
</div>


<div class="content">
  <h2>Abstract</h2>
  <p>Given a clothing image and a person image, an image-based virtual try-on aims to generate a customized image that appears natural and accurately reflects the characteristics of the clothing image. In this work, we aim to expand the applicability of the pre-trained diffusion model so that it can be utilized independently for the virtual try-on task. The main challenge is to preserve the clothing details while effectively utilizing the robust generative capability of the pre-trained model. In order to tackle these issues, we propose StableVITON, learning the semantic correspondence between the clothing and the human body within the latent space of the pre-trained diffusion model in an end-to-end manner. Our proposed zero cross-attention blocks not only preserve the clothing details by learning the semantic correspondence but also generate high-fidelity images by utilizing the inherent knowledge of the pre-trained model in the warping process. Through our proposed novel attention total variation loss and applying augmentation, we achieve the sharp attention map, resulting in a more precise representation of clothing details. StableVITON outperforms the baselines in qualitative and quantitative evaluation, showing promising quality in arbitrary person images..</p>
</div>

<!-- <div class="content">
  <h2>Background</h2>
  <p> Given a particular subject such as clock (shown in the real images on the left), it is very challenging to generate it in different contexts with state-of-the-art text-to-image models, while maintaining high fidelity to its key visual features. Even with dozens of iterations over a text prompt that contains a detailed description of the appearance of the clock (<em>"retro style yellow alarm clock with a white clock face and a yellow number three on the right part of the clock face in the jungle"</em>), the Imagen model [Saharia et al., 2022] can't reconstruct its key visual features (third column). Furthermore, even models whose text embedding lies in a shared language-vision space and can create semantic variations of the image, such as DALL-E2 [Ramesh et al., 2022], can neither reconstruct the appearance of the given subject nor modify the context (second column). In contrast, our approach (right) can synthesize the clock with high fidelity and in new contexts (<em>"a [V] clock in the jungle"</em>).</p>
  <br>
  <img class="summary-img" src="./StableVITON_files/background.png" style="width:100%;"> <br>
</div> -->

<div class="content">
  <h2>Method</h2>
  <p> For the virtual try-on task, StableVITON additionally takes three conditions: agnostic map, agnostic mask, and dense pose, as the input of the pre-trained U-Net, which serves as the query (Q) for the cross-attention. The feature map of the clothing is used as the key (K) and value (V) for the cross-attention and is conditioned on the UNet, as depicted in (b).</p>
  <br>
  <img class="summary-img" src="./StableVITON_files/method_overview2.png" style="width:100%;"> <br>

  <p> The attention mechanism in the latent space performs patch-wise warping by activating each token corresponding to clothing alignment within the generation region. Moreover, to further sharpen attention maps, we propose a novel attention total variation loss and apply the augmentation, which yields improved preservation of clothing details. By not impairing the pre-trained diffusion model, this architecture generates high-quality images even when images with complex backgrounds are provided, only using an existing virtual try-on dataset.</p>
  <br>
  <img class="summary-img" src="./StableVITON_files/noaug_aug_tvloss.png" style="width:100%;"> <br>
</div>


<div class="content">
    <h2>Results</h2> 
    <p>Generated results for VITON-HD, DressCode, SHHQ-1.0, and web-crawled images. All generated outputs were produced using StableVITON, which was trained on the VITON-HD training dataset.</p>

    <div id="myCarousel2" class="carousel slide" data-ride="carousel">
      <!-- Indicators -->
      <ol class="carousel-indicators">
        <li data-target="#myCarousel2" data-slide-to="0" class="active"></li>
        <li data-target="#myCarousel2" data-slide-to="1"></li>
        <li data-target="#myCarousel2" data-slide-to="2"></li>
        <li data-target="#myCarousel2" data-slide-to="3"></li>
      </ol>
  
      <!-- Wrapper for slides -->
      <div class="carousel-inner">
        <div class="item active">
            <div class="carousel-caption-top">
              <h1>VITON-HD</h1>
            </div>
            <img src="./StableVITON_files/VITON.png" alt="VITON-HD" style="width:100%;">
        </div>
        

        <div class="item">
            <div class="carousel-caption-top">
                <h1>DressCode</h1>
            </div>
            <img src="./StableVITON_files/DressCode.png" alt="DressCode" style="width:100%;">
        </div>
  
        <div class="item">
            <div class="carousel-caption-top">
              <h1>SHHQ-1.0</h1>
            </div>
            <img src="./StableVITON_files/SHHQ.png" alt="SHHQ" style="width:100%;">
        </div>
      
        <div class="item">
            <div class="carousel-caption-top">
                <h1>Web-crawled</h1>    
            </div>
            <img src="./StableVITON_files/web.png" alt="Web-crawled" style="width:100%;">
        </div>
      </div>
  
      <!-- Left and right controls -->
      <a class="left carousel-control" href="#myCarousel2" data-slide="prev">
        <span class="glyphicon glyphicon-chevron-left"></span>
        <span class="sr-only">Previous</span>
      </a>
      <a class="right carousel-control" href="#myCarousel2" data-slide="next">
        <span class="glyphicon glyphicon-chevron-right"></span>
        <span class="sr-only">Next</span>
      </a>
    </div>

    <br>
    <br>
    <br>
    <br>
    
    <h2>In-the-wild Results</h2>  
    <center> 
    <div id="myCarousel" class="carousel slide" data-ride="carousel">
        <!-- Indicators -->
        <ol class="carousel-indicators">
          <li data-target="#myCarousel" data-slide-to="0" class="active"></li>
          <li data-target="#myCarousel" data-slide-to="1"></li>
          <li data-target="#myCarousel" data-slide-to="2"></li>
          <li data-target="#myCarousel" data-slide-to="3"></li>
        </ol>
    
        <!-- Wrapper for slides -->
        <div class="carousel-inner">
          <div class="item active">
              <img src="./StableVITON_files/single0.png" alt="VITON-HD" style="width:70%;">
          </div>
          
          <div class="item">
              <img src="./StableVITON_files/single1.png" alt="DressCode" style="width:70%;">
          </div>
    
          <div class="item">
              <img src="./StableVITON_files/single2.png" alt="SHHQ" style="width:70%;">
          </div>
        
          <div class="item">
              <img src="./StableVITON_files/single3.png" alt="Web-crawled" style="width:70%;">
          </div>

          <div class="item">
              <img src="./StableVITON_files/single4.png" alt="Web-crawled" style="width:70%;">
          </div>

          <div class="item">
              <img src="./StableVITON_files/single5.png" alt="Web-crawled" style="width:70%;">
          </div>

          <div class="item">
              <img src="./StableVITON_files/single6.png" alt="Web-crawled" style="width:70%;">
          </div>
        </div>
    
        <!-- Left and right controls -->
        <a class="left carousel-control" href="#myCarousel" data-slide="prev">
          <span class="glyphicon glyphicon-chevron-left"></span>
          <span class="sr-only">Previous</span>
        </a> 
        <a class="right carousel-control" href="#myCarousel" data-slide="next">
          <span class="glyphicon glyphicon-chevron-right"></span>
          <span class="sr-only">Next</span>
        </a>
    </div>
    </center>     
    
    <br>
    <br>
    <br>

    <h2>Demo Video</h2>  
    <center>
    <iframe width="800" height="450" src="https://www.youtube.com/embed/x3k4u7F4ReA?si=be41wCZuYsUhyvwY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
    </center> 
  </div>

  <div class="content">
    <h2>Model Weights</h2>
    You can download it from <a href="https://kaistackr-my.sharepoint.com/personal/rlawjdghek_kaist_ac_kr/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Frlawjdghek%5Fkaist%5Fac%5Fkr%2FDocuments%2FStableVITON&ga=1">Link</a>.
  </div>

<div class="content">
  <h2>BibTex</h2>
  <code> @article{kim2023stableviton,<br>
  &nbsp;&nbsp;title={StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On},<br>
  &nbsp;&nbsp;author={Kim, Jeongho and Gu, Gyojung and Park, Minho and Park, Sunghyun and Choo, Jaegul},<br>
  &nbsp;&nbsp;journal={arXiv preprint arXiv:2312.01725},<br>
  &nbsp;&nbsp;year={2023}<br>
  } </code> 
</div>

<!-- <div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    Sunghyun Park is the corresponding author.
  </p>
</div> -->

<!-- <div class="content">
    <h2>Acknowledgements</h2>
    <p>Sunghyun Park is the corresponding author.</p>
  </div> -->

<div class="content">
    <p class="serif">
      Project page template is borrowed from <a href="https://dreambooth.github.io/">DreamBooth</a>.<br>
      <strong>Acknowledgements.</strong>  Sunghyun Park is the corresponding author.
    </p>
</div>

</body>
</html>
